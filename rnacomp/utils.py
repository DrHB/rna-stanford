# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_utils.ipynb.

# %% auto 0
__all__ = ['seed_everything', 'loss', 'loss_unc', 'MAE', 'loss_laplace', 'MAEDict', 'loss_W', 'loss_Wex', 'LossDict']

# %% ../nbs/02_utils.ipynb 1
import torch
import random
import os
import numpy as np
from fastai.vision.all import Metric
import torch.nn.functional as F
from torch import nn

# %% ../nbs/02_utils.ipynb 2
def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True
    
    
def loss(pred,target):
    p = pred[target['mask'][:,:pred.shape[1]]]
    y = target['react'][target['mask']].clip(0,1)
    loss = F.l1_loss(p, y, reduction='none')
    loss = loss[~torch.isnan(loss)].mean()
    
    return loss

def loss_unc(pred,target):
    p = pred[target['mask'][:,:pred.shape[1]]]
    y = target['react'][target['mask']]#.clip(0,1)
    loss = F.l1_loss(p, y, reduction='none')
    loss = loss[~torch.isnan(loss)].mean()
    
    return loss

class MAE(Metric):
    def __init__(self): 
        self.reset()
        
    def reset(self): 
        self.x,self.y = [],[]
        
    def accumulate(self, learn):
        x = learn.pred[learn.y['mask'][:,:learn.pred.shape[1]]].clip(0,1)
        y = learn.y['react'][learn.y['mask']].clip(0,1)
        self.x.append(x)
        self.y.append(y)

    @property
    def value(self):
        x,y = torch.cat(self.x,0),torch.cat(self.y,0)
        loss = F.l1_loss(x, y, reduction='none')
        loss = loss[~torch.isnan(loss)].mean()
        return loss
    
def loss_laplace(pred,target):
    p = pred[target['mask'][:,:pred.shape[1]]].float()
    y = target['react'][target['mask']].clip(0,1).float()
    y_err = target['react_err'][target['mask']].float()
    
    loss = F.l1_loss(p, y, reduction='none')
    m = ~torch.isnan(loss)
    loss = (loss[m]/torch.sqrt(1.0+torch.nan_to_num(y_err[m],100.0).clip(0,100.0))).mean()
    
    return loss


class MAEDict(Metric):
    def __init__(self): 
        self.reset()
        
    def reset(self): 
        self.x,self.y = [],[]
        
    def accumulate(self, learn):
        x = learn.pred['pred'][learn.y['mask'][:,:learn.pred['pred'].shape[1]]].clip(0,1)
        y = learn.y['react'][learn.y['mask']].clip(0,1)
        self.x.append(x)
        self.y.append(y)

    @property
    def value(self):
        x,y = torch.cat(self.x,0),torch.cat(self.y,0)
        loss = F.l1_loss(x, y, reduction='none')
        loss = loss[~torch.isnan(loss)].mean()
        return loss
    
def loss_W(pred,target):
    p = pred['pred'][target['mask'][:,:pred['pred'].shape[1]]].float()
    y = target['react'][target['mask']].clip(0,1).float()
    y_err = target['react_err'][target['mask']].float()
    
    loss = F.l1_loss(p, y, reduction='none')
    m = ~torch.isnan(loss)
    if m.sum() > 0:
        w = 1.0/torch.sqrt(1.0/6.0+torch.nan_to_num(y_err[m],nan=100).clip(0,100))
        loss = (loss[m]*w).mean()/w.mean()
    else: loss = 0
    
    return loss

def loss_Wex(pred,target):
    p = pred['pred_ex'][target['mask'][:,:pred['pred_ex'].shape[1]]].float()
    y = target['react_ex'][target['mask']].clip(0,1).float()
    y_err = target['react_ex_err'][target['mask']].float()
    
    loss = F.l1_loss(p, y, reduction='none')
    m = ~torch.isnan(loss)
    if m.sum() > 0:
        w = 1.0/torch.sqrt(1.0/6.0+torch.nan_to_num(y_err[m],nan=100).clip(0,100))
        loss = (loss[m]*w).mean()/w.mean()
    else: loss = 0
    
    return loss



class LossDict(nn.Module):
    def __init__(self):
        super().__init__()
        
    def forward(self, pred,target):
        return loss_W(pred,target) + loss_Wex(pred,target)
