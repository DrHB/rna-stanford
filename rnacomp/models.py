# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_models.ipynb.

# %% auto 0
__all__ = ['List', 'exists', 'default', 'PreNorm', 'Residual', 'GatedResidual', 'Attention', 'FeedForward',
           'GraphTransformerAdjacent', 'full_attention_conv', 'gcn_conv', 'DIFFormerConv', 'DIFFormer',
           'to_graph_batch', 'DifformerCustomV0', 'DropPath', 'Mlp', 'RotaryEmbedding', 'rotate_half',
           'apply_rotary_pos_emb', 'Conv1D', 'ResBlock', 'Extractor', 'ExtractorV0', 'Block', 'Block_conv',
           'RNA_ModelV2', 'RNA_ModelV2SS', 'RNA_ModelV2SSV1', 'CustomTransformerV0', 'CustomTransformerV1', 'GAT',
           'to_graph_batchv1', 'PytorchBatchWrapper', 'RNA_ModelV3', 'RNA_ModelV3SS', 'GCN', 'LayerNorm', 'GEGLU',
           'FeedForwardV0', 'RNA_ModelV4', 'RNA_ModelV5', 'RNA_ModelV6', 'RNA_ModelV7', 'ConvFeedForward',
           'FlashAttention2d', 'Attention2d', 'TriangleAttention', 'RNAformerBlock', 'PosEmbedding',
           'EmbedSequence2Matrix', 'RNAformerStack', 'load_matching_weights', 'CONFIGRNAFORMER', 'CustomRnaFormer',
           'CustomRnaFormerV0']

# %% ../nbs/01_models.ipynb 1
import torch
from torch import nn, einsum
from einops import rearrange, repeat
from rotary_embedding_torch import RotaryEmbedding, apply_rotary_emb

import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
import math
from timm.models.layers import drop_path, to_2tuple, trunc_normal_
from torch_sparse import SparseTensor, matmul
from torch_geometric.utils import degree
from torch_geometric.data import Data, Batch
import numpy as np
from torch_geometric.utils import to_dense_batch
from x_transformers import ContinuousTransformerWrapper, Encoder, TransformerWrapper
from torch_geometric.nn import GATConv, GCNConv

# %% ../nbs/01_models.ipynb 2
def exists(val):
    return val is not None

def default(val, d):
    return val if exists(val) else d

List = nn.ModuleList

# normalizations

class PreNorm(nn.Module):
    def __init__(
        self,
        dim,
        fn
    ):
        super().__init__()
        self.fn = fn
        self.norm = nn.LayerNorm(dim)

    def forward(self, x, *args, **kwargs):
        x = self.norm(x)
        return self.fn(x, *args,**kwargs)

# gated residual

class Residual(nn.Module):
    def forward(self, x, res):
        return x + res

class GatedResidual(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.proj = nn.Sequential(
            nn.Linear(dim * 3, 1, bias = False),
            nn.Sigmoid()
        )

    def forward(self, x, res):
        gate_input = torch.cat((x, res, x - res), dim = -1)
        gate = self.proj(gate_input)
        return x * gate + res * (1 - gate)

# attention

class Attention(nn.Module):
    def __init__(
        self,
        dim,
        pos_emb = None,
        dim_head = 64,
        heads = 8,
        edge_dim = None
    ):
        super().__init__()
        edge_dim = default(edge_dim, dim)

        inner_dim = dim_head * heads
        self.heads = heads
        self.scale = dim_head ** -0.5

        self.pos_emb = pos_emb

        self.to_q = nn.Linear(dim, inner_dim)
        self.to_kv = nn.Linear(dim, inner_dim * 2)
        self.edges_to_kv = nn.Linear(edge_dim, inner_dim)

        self.to_out = nn.Linear(inner_dim, dim)

    def forward(self, nodes, edges, mask = None):
        h = self.heads

        q = self.to_q(nodes)
        k, v = self.to_kv(nodes).chunk(2, dim = -1)

        e_kv = self.edges_to_kv(edges)

        q, k, v, e_kv = map(lambda t: rearrange(t, 'b ... (h d) -> (b h) ... d', h = h), (q, k, v, e_kv))

        if exists(self.pos_emb):
            freqs = self.pos_emb(torch.arange(nodes.shape[1], device = nodes.device))
            freqs = rearrange(freqs, 'n d -> () n d')
            q = apply_rotary_emb(freqs, q)
            k = apply_rotary_emb(freqs, k)

        ek, ev = e_kv, e_kv

        k, v = map(lambda t: rearrange(t, 'b j d -> b () j d '), (k, v))
        k = k + ek
        v = v + ev

        sim = einsum('b i d, b i j d -> b i j', q, k) * self.scale

        if exists(mask):
            mask = rearrange(mask, 'b i -> b i ()') & rearrange(mask, 'b j -> b () j')
            mask = repeat(mask, 'b i j -> (b h) i j', h = h)
            max_neg_value = -torch.finfo(sim.dtype).max
            sim.masked_fill_(~mask, max_neg_value)

        attn = sim.softmax(dim = -1)
        out = einsum('b i j, b i j d -> b i d', attn, v)
        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)
        return self.to_out(out)

# optional feedforward

def FeedForward(dim, ff_mult = 4):
    return nn.Sequential(
        nn.Linear(dim, dim * ff_mult),
        nn.GELU(),
        nn.Linear(dim * ff_mult, dim)
    )


# classes

class GraphTransformerAdjacent(nn.Module):
    def __init__(
        self,
        dim,
        depth,
        edge_dim = 16,
        dim_head = 32,
        heads = 8,
        with_feedforwards = False,
        rel_pos_emb = False,
        accept_adjacency_matrix = True,
    ):
        super().__init__()
        self.layers = List([])
        edge_dim = default(edge_dim, dim)


        pos_emb = RotaryEmbedding(dim_head) if rel_pos_emb else None

        for _ in range(depth):
            self.layers.append(List([
                List([
                    PreNorm(dim, Attention(dim, pos_emb = pos_emb, edge_dim = edge_dim, dim_head = dim_head, heads = heads)),
                    GatedResidual(dim)
                ]),
                List([
                    PreNorm(dim, FeedForward(dim)),
                    GatedResidual(dim)
                ]) if with_feedforwards else None
            ]))
            

    def forward(
        self,
        nodes,
        mask, 
        adj_mat,
    ):  
        
        edges = None
        batch, seq, _ = nodes.shape
        all_edges = default(edges, 0) + default(adj_mat, 0)

        for attn_block, ff_block in self.layers:
            attn, attn_residual = attn_block
            nodes = attn_residual(attn(nodes, all_edges, mask = mask), nodes)

            if exists(ff_block):
                ff, ff_residual = ff_block
                nodes = ff_residual(ff(nodes), nodes)
        return nodes
    
    




def full_attention_conv(qs, ks, vs, kernel, output_attn=False):
    '''
    qs: query tensor [N, H, M]
    ks: key tensor [L, H, M]
    vs: value tensor [L, H, D]

    return output [N, H, D]
    '''
    if kernel == 'simple':
        # normalize input
        qs = qs / torch.norm(qs, p=2) # [N, H, M]
        ks = ks / torch.norm(ks, p=2) # [L, H, M]
        N = qs.shape[0]

        # numerator
        kvs = torch.einsum("lhm,lhd->hmd", ks, vs)
        attention_num = torch.einsum("nhm,hmd->nhd", qs, kvs) # [N, H, D]
        all_ones = torch.ones([vs.shape[0]]).to(vs.device)
        vs_sum = torch.einsum("l,lhd->hd", all_ones, vs) # [H, D]
        attention_num += vs_sum.unsqueeze(0).repeat(vs.shape[0], 1, 1) # [N, H, D]

        # denominator
        all_ones = torch.ones([ks.shape[0]]).to(ks.device)
        ks_sum = torch.einsum("lhm,l->hm", ks, all_ones)
        attention_normalizer = torch.einsum("nhm,hm->nh", qs, ks_sum)  # [N, H]

        # attentive aggregated results
        attention_normalizer = torch.unsqueeze(attention_normalizer, len(attention_normalizer.shape))  # [N, H, 1]
        attention_normalizer += torch.ones_like(attention_normalizer) * N
        attn_output = attention_num / attention_normalizer # [N, H, D]

        # compute attention for visualization if needed
        if output_attn:
            attention = torch.einsum("nhm,lhm->nlh", qs, ks) / attention_normalizer # [N, L, H]

    elif kernel == 'sigmoid':
        # numerator
        attention_num = torch.sigmoid(torch.einsum("nhm,lhm->nlh", qs, ks))  # [N, L, H]

        # denominator
        all_ones = torch.ones([ks.shape[0]]).to(ks.device)
        attention_normalizer = torch.einsum("nlh,l->nh", attention_num, all_ones)
        attention_normalizer = attention_normalizer.unsqueeze(1).repeat(1, ks.shape[0], 1)  # [N, L, H]

        # compute attention and attentive aggregated results
        attention = attention_num / attention_normalizer
        attn_output = torch.einsum("nlh,lhd->nhd", attention, vs)  # [N, H, D]

    if output_attn:
        return attn_output, attention
    else:
        return attn_output

def gcn_conv(x, edge_index, edge_weight):
    N, H = x.shape[0], x.shape[1]
    row, col = edge_index
    d = degree(col, N).float()
    d_norm_in = (1. / d[col]).sqrt()
    d_norm_out = (1. / d[row]).sqrt()
    gcn_conv_output = []
    if edge_weight is None:
        value = torch.ones_like(row) * d_norm_in * d_norm_out
    else:
        value = edge_weight * d_norm_in * d_norm_out
    value = torch.nan_to_num(value, nan=0.0, posinf=0.0, neginf=0.0)
    adj = SparseTensor(row=col, col=row, value=value, sparse_sizes=(N, N))
    for i in range(x.shape[1]):
        gcn_conv_output.append( matmul(adj, x[:, i]) )  # [N, D]
    gcn_conv_output = torch.stack(gcn_conv_output, dim=1) # [N, H, D]
    return gcn_conv_output

class DIFFormerConv(nn.Module):
    '''
    one DIFFormer layer
    '''
    def __init__(self, in_channels,
               out_channels,
               num_heads,
               kernel='simple',
               use_graph=True,
               use_weight=True):
        super(DIFFormerConv, self).__init__()
        self.Wk = nn.Linear(in_channels, out_channels * num_heads)
        self.Wq = nn.Linear(in_channels, out_channels * num_heads)
        if use_weight:
            self.Wv = nn.Linear(in_channels, out_channels * num_heads)

        self.out_channels = out_channels
        self.num_heads = num_heads
        self.kernel = kernel
        self.use_graph = use_graph
        self.use_weight = use_weight

    def reset_parameters(self):
        self.Wk.reset_parameters()
        self.Wq.reset_parameters()
        if self.use_weight:
            self.Wv.reset_parameters()

    def forward(self, query_input, source_input, edge_index=None, edge_weight=None, output_attn=False):
        # feature transformation
        query = self.Wq(query_input).reshape(-1, self.num_heads, self.out_channels)
        key = self.Wk(source_input).reshape(-1, self.num_heads, self.out_channels)
        if self.use_weight:
            value = self.Wv(source_input).reshape(-1, self.num_heads, self.out_channels)
        else:
            value = source_input.reshape(-1, 1, self.out_channels)

        # compute full attentive aggregation
        if output_attn:
            attention_output, attn = full_attention_conv(query, key, value, self.kernel, output_attn)  # [N, H, D]
        else:
            attention_output = full_attention_conv(query,key,value,self.kernel) # [N, H, D]

        # use input graph for gcn conv
        if self.use_graph:
            final_output = attention_output + gcn_conv(value, edge_index, edge_weight)
        else:
            final_output = attention_output
        final_output = final_output.mean(dim=1)

        if output_attn:
            return final_output, attn
        else:
            return final_output

class DIFFormer(nn.Module):
    '''
    DIFFormer model class
    x: input node features [N, D]
    edge_index: 2-dim indices of edges [2, E]
    return y_hat predicted logits [N, C]
    '''
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1, kernel='simple',
                 alpha=0.5, dropout=0.5, use_bn=True, use_residual=True, use_weight=True, use_graph=True):
        super(DIFFormer, self).__init__()

        self.convs = nn.ModuleList()
        self.fcs = nn.ModuleList()
        self.fcs.append(nn.Linear(in_channels, hidden_channels))
        self.bns = nn.ModuleList()
        self.bns.append(nn.LayerNorm(hidden_channels))
        for i in range(num_layers):
            self.convs.append(
                DIFFormerConv(hidden_channels, hidden_channels, num_heads=num_heads, kernel=kernel, use_graph=use_graph, use_weight=use_weight))
            self.bns.append(nn.LayerNorm(hidden_channels))

        self.fcs.append(nn.Linear(hidden_channels, out_channels))

        self.dropout = dropout
        self.activation = F.relu
        self.use_bn = use_bn
        self.residual = use_residual
        self.alpha = alpha

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()
        for bn in self.bns:
            bn.reset_parameters()
        for fc in self.fcs:
            fc.reset_parameters()

    def forward(self, x, edge_index, edge_weight=None):
        layer_ = []

        # input MLP layer
        x = self.fcs[0](x)
        if self.use_bn:
            x = self.bns[0](x)
        x = self.activation(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

        # store as residual link
        layer_.append(x)

        for i, conv in enumerate(self.convs):
            # graph convolution with DIFFormer layer
            x = conv(x, x, edge_index, edge_weight)
            if self.residual:
                x = self.alpha * x + (1-self.alpha) * layer_[i]
            if self.use_bn:
                x = self.bns[i+1](x)
            x = F.dropout(x, p=self.dropout, training=self.training)
            layer_.append(x)

        # output MLP layer
        x_out = self.fcs[-1](x)
        return x_out

    def get_attentions(self, x):
        layer_, attentions = [], []
        x = self.fcs[0](x)
        if self.use_bn:
            x = self.bns[0](x)
        x = self.activation(x)
        layer_.append(x)
        for i, conv in enumerate(self.convs):
            x, attn = conv(x, x, output_attn=True)
            attentions.append(attn)
            if self.residual:
                x = self.alpha * x + (1 - self.alpha) * layer_[i]
            if self.use_bn:
                x = self.bns[i + 1](x)
            layer_.append(x)
        return torch.stack(attentions, dim=0) # [layer num, N, N]

def to_graph_batch(batch):
    res = []
    seq = batch["seq"]
    mask = batch["mask"]
    adj_matrix = batch["adj_matrix"]
    for i in range(len(seq)):
        res.append(Data(x=seq[i][mask[i]], edge_index=adj_matrix[i].nonzero().T))
    return Batch.from_data_list(res)

class DifformerCustomV0(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1):
        super().__init__()
        self.encoder = DIFFormer(in_channels, hidden_channels, out_channels, num_layers=num_layers, num_heads=num_heads)
        
    def forward(self, x):
        x  = to_graph_batch(x)
        x = self.encoder(x.x, x.edge_index)
        x, _ = to_dense_batch(x, x.batch)
        return x
        
        
        
        

# %% ../nbs/01_models.ipynb 3
class DropPath(nn.Module):
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)

    def extra_repr(self) -> str:
        return "p={}".format(self.drop_prob)


class Mlp(nn.Module):
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=nn.GELU,
        drop=0.0,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class RotaryEmbedding(nn.Module):
    def __init__(self, dim, scale_base=512, use_xpos=True):
        super().__init__()
        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq)

        self.use_xpos = use_xpos
        self.scale_base = scale_base
        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)
        self.register_buffer("scale", scale)

    def forward(self, seq_len, device="cuda"):
        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)
        freqs = torch.einsum("i , j -> i j", t, self.inv_freq)
        freqs = torch.cat((freqs, freqs), dim=-1)

        if not self.use_xpos:
            return freqs, torch.ones(1, device=device)

        power = (t - (seq_len // 2)) / self.scale_base
        scale = self.scale ** rearrange(power, "n -> n 1")
        scale = torch.cat((scale, scale), dim=-1)

        return freqs, scale


def rotate_half(x):
    x1, x2 = x.chunk(2, dim=-1)
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(pos, t, scale=1.0):
    return (t * pos.cos() * scale) + (rotate_half(t) * pos.sin() * scale)


class Conv1D(nn.Conv1d):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.src_key_padding_mask = None

    def forward(self, x, src_key_padding_mask=None):
        if src_key_padding_mask is not None:
            self.src_key_padding_mask = src_key_padding_mask
        if self.src_key_padding_mask is not None:
            x = torch.where(
                self.src_key_padding_mask.unsqueeze(-1)
                .expand(-1, -1, x.shape[-1])
                .bool(),
                torch.zeros_like(x),
                x,
            )
        return super().forward(x.permute(0, 2, 1)).permute(0, 2, 1)


class ResBlock(nn.Sequential):
    def __init__(self, d_model):
        super().__init__(
            nn.LayerNorm(d_model), nn.GELU(), Conv1D(d_model, d_model, 3, padding=1)
        )
        self.src_key_padding_mask = None

    def forward(self, x, src_key_padding_mask=None):
        self[-1].src_key_padding_mask = (
            src_key_padding_mask
            if src_key_padding_mask is not None
            else self.src_key_padding_mask
        )
        return x + super().forward(x)


class Extractor(nn.Sequential):
    def __init__(self, d_model, in_ch=4):
        super().__init__(
            nn.Embedding(in_ch, d_model // 4),
            Conv1D(d_model // 4, d_model, 7, padding=3),
            ResBlock(d_model),
        )

    def forward(self, x, src_key_padding_mask=None):
        for i in [1, 2]:
            self[i].src_key_padding_mask = src_key_padding_mask
        return super().forward(x)
    
    
class ExtractorV0(nn.Sequential):
    def __init__(self, d_model, in_ch=4):
        super().__init__()
        self.seq_embed  = nn.Sequential(nn.Embedding(in_ch, d_model // 4),
                                        Conv1D(d_model // 4, d_model//2, 7, padding=3))
        self.ss_embed  = nn.Sequential(nn.Embedding(3, d_model // 4),
                                        Conv1D(d_model // 4, d_model//2, 7, padding=3))
        
        self.out = ResBlock(d_model)

    def forward(self, seq, ss, src_key_padding_mask=None):
        for i in [1, 2]:
            self[i].src_key_padding_mask = src_key_padding_mask
    
        seq = self.seq_embed(seq)
        ss = self.ss_embed(ss)
        x = torch.concat([seq, ss], dim=-1)
        out = self.out(x)
        return out


# BEiTv2 block
class Block(nn.Module):
    def __init__(
        self,
        dim,
        num_heads,
        mlp_ratio=4.0,
        qkv_bias=False,
        qk_scale=None,
        drop=0.0,
        attn_drop=0.0,
        drop_path=0.0,
        init_values=None,
        act_layer=nn.GELU,
        norm_layer=nn.LayerNorm,
        window_size=None,
        attn_head_dim=None,
        **kwargs
    ):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = nn.MultiheadAttention(
            dim, num_heads, dropout=drop, batch_first=True
        )
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(
            in_features=dim,
            hidden_features=mlp_hidden_dim,
            act_layer=act_layer,
            drop=drop,
        )

        if init_values is not None:
            self.gamma_1 = nn.Parameter(
                init_values * torch.ones((dim)), requires_grad=True
            )
            self.gamma_2 = nn.Parameter(
                init_values * torch.ones((dim)), requires_grad=True
            )
        else:
            self.gamma_1, self.gamma_2 = None, None

        self.emb = RotaryEmbedding(dim)

    def forward(self, x, attn_mask=None, key_padding_mask=None):
        q = k = v = self.norm1(x)
        positions, scale = self.emb(x.shape[1], x.device)
        q = apply_rotary_pos_emb(positions, q, scale)
        k = apply_rotary_pos_emb(positions, k, scale**-1)

        if self.gamma_1 is None:
            x = x + self.drop_path(
                self.attn(
                    q,
                    k,
                    v,
                    attn_mask=attn_mask,
                    key_padding_mask=key_padding_mask,
                    need_weights=False,
                )[0]
            )
            x = x + self.drop_path(self.mlp(self.norm2(x)))
        else:
            x = x + self.drop_path(
                self.gamma_1
                * self.attn(
                    q,
                    k,
                    v,
                    attn_mask=attn_mask,
                    key_padding_mask=key_padding_mask,
                    need_weights=False,
                )[0]
            )
            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))
        return x


class Block_conv(Block):
    def __init__(self, dim, mlp_ratio, *args, **kwargs):
        super().__init__(dim, *args, **kwargs)
        self.mlp.fc1 = Conv1D(dim, dim, 3, padding=1)
        self.mlp.fc2 = Conv1D(dim, dim, 3, padding=1)

    def forward(self, *args, key_padding_mask=None, **kwargs):
        self.mlp.fc1.src_key_padding_mask = key_padding_mask
        self.mlp.fc2.src_key_padding_mask = key_padding_mask
        return super().forward(*args, **kwargs)


class RNA_ModelV2(nn.Module):
    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):
        super().__init__()
        # self.emb = nn.Sequential(nn.Embedding(4,dim//4), Conv1D(dim//4,dim,7,padding=3),
        #                        nn.LayerNorm(dim), nn.GELU(), Conv1D(dim,dim,3,padding=1))
        self.extractor = Extractor(dim)
        # self.pos_enc = SinusoidalPosEmb(dim)

        self.blocks = nn.ModuleList(
            [
                Block_conv(
                    dim=dim,
                    num_heads=dim // head_size,
                    mlp_ratio=4,
                    drop_path=0.2 * (i / (depth - 1)),
                    init_values=1,
                    drop=0.1,
                )
                for i in range(depth)
            ]
        )

        # self.transformer = nn.TransformerEncoder(
        #    TransformerEncoderLayer_conv(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,
        #        dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)
        self.proj_out = nn.Linear(dim, 2)

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max()
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        x = self.extractor(x, src_key_padding_mask=~mask)
        for blk in self.blocks:
            x = blk(x, key_padding_mask=~mask)
        x = self.proj_out(x)
        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))
        return x
    
    
    
class RNA_ModelV2SS(nn.Module):
    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):
        super().__init__()
        # self.emb = nn.Sequential(nn.Embedding(4,dim//4), Conv1D(dim//4,dim,7,padding=3),
        #                        nn.LayerNorm(dim), nn.GELU(), Conv1D(dim,dim,3,padding=1))
        self.seq_extractor = Extractor(dim//2)
        self.ss_extractor = Extractor(dim//2, 3)
        # self.pos_enc = SinusoidalPosEmb(dim)

        self.blocks = nn.ModuleList(
            [
                Block_conv(
                    dim=dim,
                    num_heads=dim // head_size,
                    mlp_ratio=4,
                    drop_path=0.2 * (i / (depth - 1)),
                    init_values=1,
                    drop=0.1,
                )
                for i in range(depth)
            ]
        )

        # self.transformer = nn.TransformerEncoder(
        #    TransformerEncoderLayer_conv(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,
        #        dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)
        self.proj_out = nn.Linear(dim, 2)

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max()
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        ss = x0["ss_seq"][:, :Lmax]
        seq = self.seq_extractor(x, src_key_padding_mask=~mask)
        ss = self.ss_extractor(ss, src_key_padding_mask=~mask)
        x = torch.concat([seq, ss], dim=-1)
        for blk in self.blocks:
            x = blk(x, key_padding_mask=~mask)
        x = self.proj_out(x)
        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))
        return x
    
    
class RNA_ModelV2SSV1(nn.Module):
    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):
        super().__init__()
        # self.emb = nn.Sequential(nn.Embedding(4,dim//4), Conv1D(dim//4,dim,7,padding=3),
        #                        nn.LayerNorm(dim), nn.GELU(), Conv1D(dim,dim,3,padding=1))
        self.extractor = ExtractorV0(dim)
        # self.pos_enc = SinusoidalPosEmb(dim)

        self.blocks = nn.ModuleList(
            [
                Block_conv(
                    dim=dim,
                    num_heads=dim // head_size,
                    mlp_ratio=4,
                    drop_path=0.2 * (i / (depth - 1)),
                    init_values=1,
                    drop=0.1,
                )
                for i in range(depth)
            ]
        )

        # self.transformer = nn.TransformerEncoder(
        #    TransformerEncoderLayer_conv(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,
        #        dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)
        self.proj_out = nn.Linear(dim, 2)

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max()
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        ss = x0["ss_seq"][:, :Lmax]
        x = self.extractor(x, ss, src_key_padding_mask=~mask)
        for blk in self.blocks:
            x = blk(x, key_padding_mask=~mask)
        x = self.proj_out(x)
        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))
        return x



class CustomTransformerV0(nn.Module):
    def __init__(self, dim=192, depth=12, attb_heads=8, out=2):
        super().__init__()
        self.emb = nn.Embedding(4, dim)
        self.dec = ContinuousTransformerWrapper(
            dim_in=dim,
            dim_out=out,
            max_seq_len=512,
            attn_layers=Encoder(
                dim=dim,
                depth=depth,
                heads=attb_heads,
                attn_flash=True,
                rotary_pos_emb=True,
            ),
        )

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max()
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        x = self.emb(x)
        out = self.dec(x, mask=mask)
        return out


class CustomTransformerV1(nn.Module):
    def __init__(self, dim=192, depth=12, attn_heads=8, head_size=32):
        super().__init__()
        self.dec = TransformerWrapper(
            num_tokens=4,
            logits_dim=2,
            max_seq_len=512,
            attn_layers=Encoder(
                dim=dim,
                depth=depth,
                heads=attn_heads,
                attn_flash=True,
                rotary_pos_emb=True,
            ),
        )

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max()
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        out = self.dec(x, mask=mask)
        return out


class GAT(nn.Module):
    def __init__(
        self,
        in_channels,
        hidden_channels,
        out_channels,
        num_layers=2,
        dropout=0.5,
        use_bn=False,
        heads=2,
        out_heads=1,
    ):
        super(GAT, self).__init__()

        self.convs = nn.ModuleList()
        self.convs.append(
            GATConv(
                in_channels, hidden_channels, dropout=dropout, heads=heads, concat=True
            )
        )

        self.bns = nn.ModuleList()
        self.bns.append(nn.LayerNorm(hidden_channels * heads))
        for _ in range(num_layers - 2):
            self.convs.append(
                GATConv(
                    hidden_channels * heads,
                    hidden_channels,
                    dropout=dropout,
                    heads=heads,
                    concat=True,
                )
            )
            self.bns.append(nn.LayerNorm(hidden_channels * heads))

        self.convs.append(
            GATConv(
                hidden_channels * heads,
                out_channels,
                dropout=dropout,
                heads=out_heads,
                concat=False,
            )
        )

        self.dropout = dropout
        self.activation = F.elu
        self.use_bn = use_bn

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()
        for bn in self.bns:
            bn.reset_parameters()

    def forward(self, x, edge_index):
        res = x
        x = F.dropout(x, p=self.dropout, training=self.training)
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            if self.use_bn:
                x = self.bns[i](x)
            x = self.activation(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.convs[-1](x, edge_index)
        x = res + x
        return x


def to_graph_batchv1(seq, mask, adj_matrix):
    res = []
    for i in range(len(seq)):
        res.append(Data(x=seq[i][mask[i]], edge_index=adj_matrix[i].nonzero().T))
    return Batch.from_data_list(res)


class PytorchBatchWrapper(nn.Module):
    def __init__(self, md):
        super().__init__()
        self.md = md

    def forward(self, seq, mask, adj_matrix):
        batch = to_graph_batchv1(seq, mask, adj_matrix)
        out = self.md(batch.x, batch.edge_index)
        out, _ = to_dense_batch(out, batch.batch)
        return out


class RNA_ModelV3(nn.Module):
    def __init__(self, dim=192, depth=12, head_size=32, graph_layers_every=4, **kwargs):
        super().__init__()

        self.extractor = Extractor(dim)

        self.blocks = nn.ModuleList(
            [
                Block_conv(
                    dim=dim,
                    num_heads=dim // head_size,
                    mlp_ratio=4,
                    drop_path=0.2 * (i / (depth - 1)),
                    init_values=1,
                    drop=0.1,
                )
                for i in range(depth)
            ]
        )

        self.graph_layers_every = graph_layers_every
        self.graph_layers = nn.ModuleList(
            [
                PytorchBatchWrapper(
                    GAT(
                        in_channels=dim,
                        hidden_channels=dim // 2,
                        out_channels=dim,
                        num_layers=2,
                        dropout=0.1,
                        use_bn=True,
                        heads=4,
                        out_heads=1,
                    )
                )
                for i in range(depth)
                if i % self.graph_layers_every == 0
            ]
        )

        self.proj_out = nn.Linear(dim, 2)

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max()
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        x = self.extractor(x, src_key_padding_mask=~mask)

        graph_layer_index = 0
        for i, blk in enumerate(self.blocks):
            x = blk(x, key_padding_mask=~mask)
            if i % self.graph_layers_every == 0:
                x = self.graph_layers[graph_layer_index](x, mask, x0["adj_matrix"])
                graph_layer_index += 1

        x = self.proj_out(x)
        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))
        return x

class RNA_ModelV3SS(nn.Module):
    def __init__(self, dim=192, depth=12, head_size=32, graph_layers_every=4, **kwargs):
        super().__init__()

        self.extractor = Extractor(dim)

        self.blocks = nn.ModuleList(
            [
                Block_conv(
                    dim=dim,
                    num_heads=dim // head_size,
                    mlp_ratio=4,
                    drop_path=0.2 * (i / (depth - 1)),
                    init_values=1,
                    drop=0.1,
                )
                for i in range(depth)
            ]
        )

        self.graph_layers_every = graph_layers_every
        self.graph_layers = nn.ModuleList(
            [
                PytorchBatchWrapper(
                    GAT(
                        in_channels=dim,
                        hidden_channels=dim // 2,
                        out_channels=dim,
                        num_layers=2,
                        dropout=0.1,
                        use_bn=True,
                        heads=4,
                        out_heads=1,
                    )
                )
                for i in range(depth)
                if i % self.graph_layers_every == 0
            ]
        )

        self.proj_out = nn.Linear(dim, 2)

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max()
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        x = self.extractor(x, src_key_padding_mask=~mask)

        graph_layer_index = 0
        for i, blk in enumerate(self.blocks):
            x = blk(x, key_padding_mask=~mask)
            if i % self.graph_layers_every == 0:
                x = self.graph_layers[graph_layer_index](x, mask, x0["ss_adj"])
                graph_layer_index += 1

        x = self.proj_out(x)
        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))
        return x


class GCN(nn.Module):
    def __init__(
        self,
        in_channels,
        hidden_channels,
        out_channels,
        num_layers=2,
        dropout=0.5,
        save_mem=True,
        use_bn=True,
    ):
        super(GCN, self).__init__()

        self.convs = nn.ModuleList()
        # self.convs.append(
        #     GCNConv(in_channels, hidden_channels, cached=not save_mem, normalize=not save_mem))
        self.convs.append(GCNConv(in_channels, hidden_channels, cached=not save_mem))

        self.bns = nn.ModuleList()
        self.bns.append(nn.LayerNorm(hidden_channels))
        for _ in range(num_layers - 2):
            self.convs.append(
                GCNConv(hidden_channels, hidden_channels, cached=not save_mem)
            )
            self.bns.append(nn.LayerNorm(hidden_channels))

        # self.convs.append(
        #     GCNConv(hidden_channels, out_channels, cached=not save_mem, normalize=not save_mem))
        self.convs.append(GCNConv(hidden_channels, out_channels, cached=not save_mem))

        self.dropout = dropout
        self.activation = F.gelu
        self.use_bn = use_bn

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()
        for bn in self.bns:
            bn.reset_parameters()

    def forward(self, x, edge_index):
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            if self.use_bn:
                x = self.bns[i](x)
            x = self.activation(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.convs[-1](x, edge_index)
        return x


class LayerNorm(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.g = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        eps = 1e-5 if x.dtype == torch.float32 else 1e-3
        var = torch.var(x, dim=-1, unbiased=False, keepdim=True)
        mean = torch.mean(x, dim=-1, keepdim=True)
        return (x - mean) * (var + eps).rsqrt() * self.g


class GEGLU(nn.Module):
    def forward(self, x):
        x, gate = x.chunk(2, dim=-1)
        return x * F.gelu(gate)


class FeedForwardV0(nn.Module):
    def __init__(self, dim, out=2, mult=4, dropout=0.1):
        super().__init__()
        inner_dim = int(dim * mult)

        self.net = nn.Sequential(
            nn.Linear(dim, inner_dim * 2, bias=False),
            GEGLU(),
            LayerNorm(inner_dim),
            nn.Dropout(dropout),
            nn.Linear(inner_dim, out, bias=False),
        )

    def forward(self, x):
        return self.net(x)


class RNA_ModelV4(nn.Module):
    def __init__(self, dim=192, depth=12, head_size=32, graph_layers_every=3, **kwargs):
        super().__init__()

        self.extractor = Extractor(dim)

        self.blocks = nn.ModuleList(
            [
                Block_conv(
                    dim=dim,
                    num_heads=dim // head_size,
                    mlp_ratio=4,
                    drop_path=0.2 * (i / (depth - 1)),
                    init_values=1,
                    drop=0.1,
                )
                for i in range(depth)
            ]
        )

        self.graph_layers_every = graph_layers_every
        self.graph_layers = PytorchBatchWrapper(
            GCN(
                dim * (depth // graph_layers_every),
                dim,
                dim,
                num_layers=depth // graph_layers_every,
                dropout=0.2,
                use_bn=True,
            )
        )

        self.proj_out = FeedForwardV0(dim * 2, 2)

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max()
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        x = self.extractor(x, src_key_padding_mask=~mask)

        intermediates = []
        for i, blk in enumerate(self.blocks):
            x = blk(x, key_padding_mask=~mask)
            if i % self.graph_layers_every == 0:
                intermediates.append(x)
        graph = self.graph_layers(
            torch.concat(intermediates, dim=-1), mask, x0["adj_matrix"]
        )

        x = torch.concat([x, graph], dim=-1)
        x = self.proj_out(x)
        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))
        return x


class RNA_ModelV5(nn.Module):
    def __init__(
        self,
        dim=192,
        depth=12,
        head_size=32,
        graph_layers_every=4,
        edge_dim=8,
        **kwargs
    ):
        super().__init__()

        self.extractor = Extractor(dim)

        self.blocks = nn.ModuleList(
            [
                Block_conv(
                    dim=dim,
                    num_heads=dim // head_size,
                    mlp_ratio=4,
                    drop_path=0.2 * (i / (depth - 1)),
                    init_values=1,
                    drop=0.1,
                )
                for i in range(depth)
            ]
        )
        self.adj_emb = nn.Embedding(2, edge_dim)
        self.graph_layers_every = graph_layers_every
        self.graph_layers = nn.ModuleList(
            [
                GraphTransformerAdjacent(
                    dim=dim,
                    depth=1,
                    heads=4,
                    edge_dim=edge_dim,
                )
                for i in range(depth)
                if i % self.graph_layers_every == 0
            ]
        )

        self.proj_out = nn.Linear(dim, 2)

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max()
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        adj_matrix = x0["adj_matrix"][:, :Lmax, :Lmax]
        adj_matrix = self.adj_emb(adj_matrix.long())

        x = self.extractor(x, src_key_padding_mask=~mask)

        graph_layer_index = 0
        for i, blk in enumerate(self.blocks):
            x = blk(x, key_padding_mask=~mask)
            if i % self.graph_layers_every == 0:
                x = self.graph_layers[graph_layer_index](x, mask, adj_matrix)
                graph_layer_index += 1

        x = self.proj_out(x)
        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))
        return x


class RNA_ModelV6(nn.Module):
    def __init__(self, dim=192, depth=12, head_size=32, graph_layers_every=4, **kwargs):
        super().__init__()

        self.extractor = Extractor(dim)

        self.blocks = nn.ModuleList(
            [
                Block_conv(
                    dim=dim,
                    num_heads=dim // head_size,
                    mlp_ratio=4,
                    drop_path=0.2 * (i / (depth - 1)),
                    init_values=1,
                    drop=0.1,
                )
                for i in range(depth)
            ]
        )

        self.graph_layers_every = graph_layers_every
        self.graph_layers = nn.ModuleList(
            [
                PytorchBatchWrapper(
                    GCN(
                        dim,
                        dim//2,
                        dim,
                        num_layers=depth // graph_layers_every,
                        dropout=0.2,
                        use_bn=True,
                    )
                )
                for i in range(depth)
                if i % self.graph_layers_every == 0
            ]
        )

        self.proj_out = nn.Linear(dim, 2)

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max()
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        x = self.extractor(x, src_key_padding_mask=~mask)

        graph_layer_index = 0
        for i, blk in enumerate(self.blocks):
            x = blk(x, key_padding_mask=~mask)
            if i % self.graph_layers_every == 0:
                x = self.graph_layers[graph_layer_index](x, mask, x0["adj_matrix"])
                graph_layer_index += 1

        x = self.proj_out(x)
        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))
        return x



class RNA_ModelV7(nn.Module):
    def __init__(self, dim=192, depth=12, head_size=32, graph_layers=4, **kwargs):
        super().__init__()

        self.extractor = Extractor(dim//2)

        self.blocks = nn.ModuleList(
            [
                Block_conv(
                    dim=dim,
                    num_heads=dim // head_size,
                    mlp_ratio=4,
                    drop_path=0.2 * (i / (depth - 1)),
                    init_values=1,
                    drop=0.1,
                )
                for i in range(depth)
            ]
        )

        self.graph_layers =PytorchBatchWrapper(
                    GAT(
                        in_channels=dim//2,
                        hidden_channels=dim // 2,
                        out_channels=dim//2,
                        num_layers=graph_layers,
                        dropout=0.1,
                        use_bn=True,
                        heads=4,
                        out_heads=1,
                    )
        )

        self.proj_out = nn.Linear(dim, 2)

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max()
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        x = self.extractor(x, src_key_padding_mask=~mask)
        x = torch.concat([x, self.graph_layers(x, mask, x0["adj_matrix"])],  -1)
    
        for i, blk in enumerate(self.blocks):
            x = blk(x, key_padding_mask=~mask)

        x = self.proj_out(x)
        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))
        return x
    


# %% ../nbs/01_models.ipynb 4
from torch.utils.checkpoint import checkpoint
import warnings
from termcolor import colored
import torch.nn.functional as F
try:
    from flash_attn.flash_attn_interface import (
        flash_attn_unpadded_qkvpacked_func,
        flash_attn_unpadded_kvpacked_func,
    )
    from flash_attn.bert_padding import unpad_input, pad_input
    from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func
except:
    warnings.warn(colored("Could not import flash_attn", "magenta"))
    load_flash_attn_check = False


class FeedForward(nn.Module):
    def __init__(
        self, model_dim, ff_dim, use_bias, glu, initializer_range, zero_init, n_layers
    ):
        super(FeedForward, self).__init__()

        self.glu = glu

        if self.glu:
            ff_dim_2 = np.exp2(np.ceil(np.log2(256 * 4 / 3))).astype(int)
            ff_dim_1 = ff_dim_2 * 2
        else:
            ff_dim_1, ff_dim_2 = ff_dim, ff_dim

        self.input_norm = nn.LayerNorm(model_dim, eps=1e-6)

        self.linear_1 = nn.Linear(model_dim, ff_dim_1, bias=use_bias)
        self.linear_2 = nn.Linear(ff_dim_2, model_dim, bias=use_bias)
        self.act = nn.SiLU()

        self.initialize(zero_init, use_bias, initializer_range, n_layers)

    def initialize(self, zero_init, use_bias, initializer_range, n_layers):
        nn.init.normal_(self.linear_1.weight, mean=0.0, std=initializer_range)

        if use_bias:
            nn.init.constant_(self.linear_1.bias, 0.0)
            nn.init.constant_(self.linear_2.bias, 0.0)

        if zero_init:
            nn.init.constant_(self.linear_2.weight, 0.0)
        else:
            nn.init.normal_(
                self.linear_2.weight,
                mean=0.0,
                std=initializer_range / math.sqrt(2 * n_layers),
            )

    def forward(self, x):
        x = self.input_norm(x)

        if self.glu:
            x = self.linear_1(x)
            x, gate = x.chunk(2, dim=-1)
            x = self.act(gate) * x
        else:
            x = self.act(self.linear_1(x))

        return self.linear_2(x)


class ConvFeedForward(nn.Module):
    def __init__(
        self,
        model_dim,
        ff_dim,
        use_bias,
        initializer_range,
        n_layers,
        kernel,
        zero_init=True,
    ):
        super(ConvFeedForward, self).__init__()

        self.zero_init = zero_init

        self.input_norm = nn.GroupNorm(1, model_dim)

        if kernel == 1:
            self.conv1 = nn.Conv2d(model_dim, ff_dim, kernel_size=1, bias=use_bias)
            self.conv2 = nn.Conv2d(ff_dim, model_dim, kernel_size=1, bias=use_bias)
        else:
            self.conv1 = nn.Conv2d(
                model_dim,
                ff_dim,
                bias=use_bias,
                kernel_size=kernel,
                padding=(kernel - 1) // 2,
            )
            self.conv2 = nn.Conv2d(
                ff_dim,
                model_dim,
                bias=use_bias,
                kernel_size=kernel,
                padding=(kernel - 1) // 2,
            )

        self.act = nn.SiLU()

        self.initialize(zero_init, use_bias, initializer_range, n_layers)

    def initialize(self, zero_init, use_bias, initializer_range, n_layers):
        nn.init.normal_(self.conv1.weight, mean=0.0, std=initializer_range)

        if use_bias:
            nn.init.constant_(self.conv1.bias, 0.0)
            nn.init.constant_(self.conv2.bias, 0.0)

        if zero_init:
            nn.init.constant_(self.conv2.weight, 0.0)
        else:
            nn.init.normal_(
                self.conv2.weight,
                mean=0.0,
                std=initializer_range / math.sqrt(2 * n_layers),
            )

    def forward(self, x):
        x = x.permute(0, 3, 1, 2)

        x = self.input_norm(x)
        x = self.act(self.conv1(x))
        x = self.conv2(x)
        x = x.permute(0, 2, 3, 1)
        return x


class FlashAttention2d(nn.Module):
    def __init__(
        self,
        model_dim,
        num_head,
        softmax_scale,
        zero_init,
        use_bias,
        initializer_range,
        n_layers,
    ):
        super().__init__()
        assert model_dim % num_head == 0
        assert model_dim % num_head == 0
        self.key_dim = model_dim // num_head
        self.value_dim = model_dim // num_head

        self.causal = False
        self.checkpointing = False

        if softmax_scale:
            self.softmax_scale = self.key_dim ** (-0.5)
        else:
            self.softmax_scale = None

        self.num_head = num_head

        self.Wqkv = nn.Linear(model_dim, 3 * model_dim, bias=use_bias)

        self.out_proj = nn.Linear(model_dim, model_dim, bias=use_bias)

        self.initialize(zero_init, use_bias, initializer_range, n_layers)

    def initialize(self, zero_init, use_bias, initializer_range, n_layers):
        nn.init.normal_(self.Wqkv.weight, mean=0.0, std=initializer_range)

        if use_bias:
            nn.init.constant_(self.Wqkv.bias, 0.0)
            nn.init.constant_(self.out_proj.bias, 0.0)

        if zero_init:
            nn.init.constant_(self.out_proj.weight, 0.0)
        else:
            nn.init.normal_(
                self.out_proj.weight,
                mean=0.0,
                std=initializer_range / math.sqrt(2 * n_layers),
            )

    def forward(self, pair_act, attention_mask):
        batch_size = pair_act.shape[0]
        seqlen = pair_act.shape[1]
        extended_batch_size = batch_size * seqlen

        qkv = self.Wqkv(pair_act)
        not_attention_mask = torch.logical_not(attention_mask)

        x_qkv = rearrange(
            qkv, "b s f ... -> (b s) f ...", b=batch_size, f=seqlen, s=seqlen
        )
        key_padding_mask = rearrange(
            not_attention_mask,
            "b s f ... -> (b s) f ...",
            b=batch_size,
            f=seqlen,
            s=seqlen,
        )

        x_unpad, indices, cu_seqlens, max_s = unpad_input(x_qkv, key_padding_mask)
        x_unpad = rearrange(
            x_unpad, "nnz (three h d) -> nnz three h d", three=3, h=self.num_head
        )

        if self.training and self.checkpointing:
            output_unpad = torch.utils.checkpoint.checkpoint(
                flash_attn_unpadded_qkvpacked_func,
                x_unpad,
                cu_seqlens,
                max_s,
                0.0,
                self.softmax_scale,
                self.causal,
                False,
            )
        else:
            output_unpad = flash_attn_unpadded_qkvpacked_func(
                x_unpad,
                cu_seqlens,
                max_s,
                0.0,
                softmax_scale=self.softmax_scale,
                causal=self.causal,
            )

        pre_pad_latent = rearrange(output_unpad, "nnz h d -> nnz (h d)")
        padded_latent = pad_input(pre_pad_latent, indices, extended_batch_size, seqlen)
        output = rearrange(padded_latent, "b f (h d) -> b f h d", h=self.num_head)

        output = rearrange(
            output, "(b s) f h d -> b s f (h d)", b=batch_size, f=seqlen, s=seqlen
        )

        return self.out_proj(output)


class Attention2d(nn.Module):
    def __init__(
        self,
        model_dim,
        num_head,
        softmax_scale,
        precision,
        zero_init,
        use_bias,
        initializer_range,
        n_layers,
    ):
        super().__init__()
        assert model_dim % num_head == 0
        assert model_dim % num_head == 0
        self.key_dim = model_dim // num_head
        self.value_dim = model_dim // num_head

        if softmax_scale:
            self.softmax_scale = torch.sqrt(torch.FloatTensor([self.key_dim]))
        else:
            self.softmax_scale = False

        self.num_head = num_head
        self.model_dim = model_dim

        if precision == "fp32" or precision == 32 or precision == "bf16":
            self.mask_bias = -1e9
        elif precision == "fp16" or precision == 16:
            self.mask_bias = -1e4
        else:
            raise UserWarning(
                f"unknown precision: {precision} . Please us fp16, fp32 or bf16"
            )

        self.Wqkv = nn.Linear(model_dim, 3 * model_dim, bias=use_bias)
        self.out_proj = nn.Linear(model_dim, model_dim, bias=use_bias)

        self.initialize(zero_init, use_bias, initializer_range, n_layers)

    def initialize(self, zero_init, use_bias, initializer_range, n_layers):
        nn.init.normal_(self.Wqkv.weight, mean=0.0, std=initializer_range)

        if use_bias:
            nn.init.constant_(self.Wqkv.bias, 0.0)
            nn.init.constant_(self.out_proj.bias, 0.0)

        if zero_init:
            nn.init.constant_(self.out_proj.weight, 0.0)
        else:
            nn.init.normal_(
                self.out_proj.weight,
                mean=0.0,
                std=initializer_range / math.sqrt(2 * n_layers),
            )

    def forward(self, pair_act, attention_mask):
        batch_size = pair_act.size(0)
        N_seq = pair_act.size(1)
        N_res = pair_act.size(2)

        query, key, value = self.Wqkv(pair_act).split(self.model_dim, dim=3)

        query = query.view(
            batch_size, N_seq, N_res, self.num_head, self.key_dim
        ).permute(0, 1, 3, 2, 4)
        key = key.view(batch_size, N_seq, N_res, self.num_head, self.value_dim).permute(
            0, 1, 3, 4, 2
        )
        value = value.view(
            batch_size, N_seq, N_res, self.num_head, self.value_dim
        ).permute(0, 1, 3, 2, 4)

        attn_weights = torch.matmul(query, key)

        if self.softmax_scale:
            attn_weights = attn_weights / self.softmax_scale.to(pair_act.device)

        if attention_mask is not None:
            attention_mask = attention_mask[:, :, None, None, :]
            attn_weights.masked_fill_(attention_mask, self.mask_bias)
        attn_weights = F.softmax(attn_weights, dim=-1)

        weighted_avg = torch.matmul(attn_weights, value).permute(0, 1, 3, 2, 4)

        output = self.out_proj(
            weighted_avg.reshape(
                batch_size, N_seq, N_res, self.num_head * self.value_dim
            )
        )
        return output


class TriangleAttention(nn.Module):
    def __init__(
        self,
        model_dim,
        num_head,
        orientation,
        softmax_scale,
        precision,
        zero_init,
        use_bias,
        flash_attn,
        initializer_range,
        n_layers,
    ):
        super().__init__()

        self.model_dim = model_dim
        self.num_head = num_head

        assert orientation in ["per_row", "per_column"]
        self.orientation = orientation

        self.input_norm = nn.LayerNorm(model_dim, eps=1e-6)

        if flash_attn:
            self.attn = FlashAttention2d(
                model_dim,
                num_head,
                softmax_scale,
                zero_init,
                use_bias,
                initializer_range,
                n_layers,
            )
        else:
            self.attn = Attention2d(
                model_dim,
                num_head,
                softmax_scale,
                precision,
                zero_init,
                use_bias,
                initializer_range,
                n_layers,
            )

    def forward(self, pair_act, pair_mask, cycle_infer=False):
        assert len(pair_act.shape) == 4

        if self.orientation == "per_column":
            pair_act = torch.swapaxes(pair_act, -2, -3)
            if pair_mask is not None:
                pair_mask = torch.swapaxes(pair_mask, -1, -2)

        pair_act = self.input_norm(pair_act)

        if self.training and not cycle_infer:
            pair_act = checkpoint(self.attn, pair_act, pair_mask, use_reentrant=True)
        else:
            pair_act = self.attn(pair_act, pair_mask)

        if self.orientation == "per_column":
            pair_act = torch.swapaxes(pair_act, -2, -3)

        return pair_act


class RNAformerBlock(nn.Module):
    def __init__(self, config):
        super().__init__()

        ff_dim = int(config.ff_factor * config.model_dim)

        self.attn_pair_row = TriangleAttention(
            config.model_dim,
            config.num_head,
            "per_row",
            config.softmax_scale,
            config.precision,
            config.zero_init,
            config.use_bias,
            config.flash_attn,
            config.initializer_range,
            config.n_layers,
        )
        self.attn_pair_col = TriangleAttention(
            config.model_dim,
            config.num_head,
            "per_column",
            config.softmax_scale,
            config.precision,
            config.zero_init,
            config.use_bias,
            config.flash_attn,
            config.initializer_range,
            config.n_layers,
        )

        self.pair_dropout_row = nn.Dropout(p=config.resi_dropout / 2)
        self.pair_dropout_col = nn.Dropout(p=config.resi_dropout / 2)

        if config.ff_kernel:
            self.pair_transition = ConvFeedForward(
                config.model_dim,
                ff_dim,
                use_bias=config.use_bias,
                kernel=config.ff_kernel,
                initializer_range=config.initializer_range,
                zero_init=config.zero_init,
                n_layers=config.n_layers,
            )
        else:
            self.pair_transition = FeedForward(
                config.model_dim,
                ff_dim,
                use_bias=config.use_bias,
                glu=config.use_glu,
                initializer_range=config.initializer_range,
                zero_init=config.zero_init,
                n_layers=config.n_layers,
            )

        self.res_dropout = nn.Dropout(p=config.resi_dropout)

    def forward(self, pair_act, pair_mask, cycle_infer=False):
        pair_act = pair_act + self.pair_dropout_row(
            self.attn_pair_row(pair_act, pair_mask, cycle_infer)
        )
        pair_act = pair_act + self.pair_dropout_col(
            self.attn_pair_col(pair_act, pair_mask, cycle_infer)
        )
        pair_act = pair_act + self.res_dropout(self.pair_transition(pair_act))

        return pair_act


class ConvFeedForward(nn.Module):
    def __init__(
        self,
        model_dim,
        ff_dim,
        use_bias,
        initializer_range,
        n_layers,
        kernel,
        zero_init=True,
    ):
        super(ConvFeedForward, self).__init__()

        self.zero_init = zero_init

        self.input_norm = nn.GroupNorm(1, model_dim)

        if kernel == 1:
            self.conv1 = nn.Conv2d(model_dim, ff_dim, kernel_size=1, bias=use_bias)
            self.conv2 = nn.Conv2d(ff_dim, model_dim, kernel_size=1, bias=use_bias)
        else:
            self.conv1 = nn.Conv2d(
                model_dim,
                ff_dim,
                bias=use_bias,
                kernel_size=kernel,
                padding=(kernel - 1) // 2,
            )
            self.conv2 = nn.Conv2d(
                ff_dim,
                model_dim,
                bias=use_bias,
                kernel_size=kernel,
                padding=(kernel - 1) // 2,
            )

        self.act = nn.SiLU()

        self.initialize(zero_init, use_bias, initializer_range, n_layers)

    def initialize(self, zero_init, use_bias, initializer_range, n_layers):
        nn.init.normal_(self.conv1.weight, mean=0.0, std=initializer_range)

        if use_bias:
            nn.init.constant_(self.conv1.bias, 0.0)
            nn.init.constant_(self.conv2.bias, 0.0)

        if zero_init:
            nn.init.constant_(self.conv2.weight, 0.0)
        else:
            nn.init.normal_(
                self.conv2.weight,
                mean=0.0,
                std=initializer_range / math.sqrt(2 * n_layers),
            )

    def forward(self, x):
        x = x.permute(0, 3, 1, 2)

        x = self.input_norm(x)
        x = self.act(self.conv1(x))
        x = self.conv2(x)
        x = x.permute(0, 2, 3, 1)
        return x


class PosEmbedding(nn.Module):
    def __init__(self, vocab, model_dim, max_len, rel_pos_enc, initializer_range):
        super().__init__()

        self.rel_pos_enc = rel_pos_enc
        self.max_len = max_len

        self.embed_seq = nn.Embedding(vocab, model_dim)

        self.scale = nn.Parameter(
            torch.sqrt(torch.FloatTensor([model_dim // 2])), requires_grad=False
        )

        if rel_pos_enc:
            self.embed_pair_pos = nn.Linear(max_len, model_dim, bias=False)
        else:
            self.embed_pair_pos = nn.Linear(model_dim, model_dim, bias=False)

            pe = torch.zeros(max_len, model_dim)
            position = torch.arange(0, max_len).unsqueeze(1).type(torch.FloatTensor)
            div_term = torch.exp(
                torch.arange(0, model_dim, 2).type(torch.FloatTensor)
                * -(math.log(10000.0) / model_dim)
            )
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            pe = pe.unsqueeze(0)
            pe = torch.nn.Parameter(pe, requires_grad=False)
            self.register_buffer("pe", pe)

        self.initialize(initializer_range)  #

    def initialize(self, initializer_range):
        nn.init.normal_(self.embed_seq.weight, mean=0.0, std=initializer_range)
        nn.init.normal_(self.embed_pair_pos.weight, mean=0.0, std=initializer_range)

    def relative_position_encoding(self, src_seq):
        residue_index = torch.arange(src_seq.size()[1], device=src_seq.device).expand(
            src_seq.size()
        )
        rel_pos = F.one_hot(
            torch.clip(residue_index, min=0, max=self.max_len - 1), self.max_len
        )

        if isinstance(self.embed_pair_pos.weight, torch.cuda.BFloat16Tensor):
            rel_pos = rel_pos.type(torch.bfloat16)
        elif isinstance(self.embed_pair_pos.weight, torch.cuda.HalfTensor):
            rel_pos = rel_pos.half()
        else:
            rel_pos = rel_pos.type(torch.float32)

        pos_encoding = self.embed_pair_pos(rel_pos)
        return pos_encoding

    def forward(self, src_seq):
        seq_embed = self.embed_seq(src_seq) * self.scale

        if self.rel_pos_enc:
            seq_embed = seq_embed + self.relative_position_encoding(src_seq)
        else:
            seq_embed = seq_embed + self.embed_pair_pos(self.pe[:, : src_seq.size(1)])

        return seq_embed


class EmbedSequence2Matrix(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.src_embed_1 = PosEmbedding(
            config.seq_vocab_size,
            config.model_dim,
            config.max_len,
            config.rel_pos_enc,
            config.initializer_range,
        )
        self.src_embed_2 = PosEmbedding(
            config.seq_vocab_size,
            config.model_dim,
            config.max_len,
            config.rel_pos_enc,
            config.initializer_range,
        )

        self.norm = nn.LayerNorm(config.model_dim)

    def forward(self, src_seq):
        seq_1_embed = self.src_embed_1(src_seq)
        seq_2_embed = self.src_embed_2(src_seq)

        pair_latent = seq_1_embed.unsqueeze(1) + seq_2_embed.unsqueeze(2)

        pair_latent = self.norm(pair_latent)

        return pair_latent


class RNAformerStack(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.output_ln = nn.LayerNorm(config.model_dim)

        module_list = []
        for idx in range(config.n_layers):
            layer = RNAformerBlock(config=config)
            module_list.append(layer)
        self.layers = nn.ModuleList(module_list)

    def forward(self, pair_act, pair_mask, cycle_infer=False):
        for idx, layer in enumerate(self.layers):
            pair_act = layer(pair_act, pair_mask, cycle_infer=cycle_infer)

        pair_act = self.output_ln(pair_act)

        return pair_act
    



def load_matching_weights(model, weights_path):
    """
    Load model weights from a given path if they match, otherwise skip.
    Prints the number of matched and unmatched weights.

    :param model: The PyTorch model for which weights should be loaded.
    :param weights_path: The path to the saved weights file (.pth or .pt).
    """
    # Load the saved state dictionary
    saved_state_dict = torch.load(weights_path, map_location=torch.device('cpu'))

    # Get the model's state dictionary
    model_state_dict = model.state_dict()

    # Create a new state dictionary to store matching weights
    matching_state_dict = {}

    # Initialize counters for matched and unmatched weights
    matched_weights = 0
    unmatched_weights = 0

    # Iterate through the saved state dictionary
    for name, saved_weight in saved_state_dict.items():
        # Check if the name exists in the model's state dictionary and the shapes match
        if name in model_state_dict and model_state_dict[name].shape == saved_weight.shape:
            # If it matches, add it to the matching state dictionary
            matching_state_dict[name] = saved_weight
            matched_weights += 1
        else:
            #print(f"Skipping weight: {name} - Shape mismatch or not found in model")
            unmatched_weights += 1

    # Update the model's state dictionary with the matching state dictionary
    model_state_dict.update(matching_state_dict)

    # Load the updated state dictionary into the model
    model.load_state_dict(model_state_dict)

    print(f"Matched weights: {matched_weights}")
    print(f"Unmatched weights: {unmatched_weights}")

class CONFIGRNAFORMER:
    model_dim = 128  # hidden dimension of transformer
    n_layers = 6  # number of transformer layers
    num_head = 4  # number of heads per layer
    ff_factor = 4  # hidden dim * ff_factor = size of feed-forward layer
    ff_kernel = 3
    cycling = False
    resi_dropout = 0.1
    embed_dropout = 0.1
    rel_pos_enc = True  # relative position encoding
    head_bias = False
    ln_eps = 1e-5
    softmax_scale = True
    key_dim_scaler = True
    gating = False
    use_glu = False
    use_bias = True
    flash_attn = False
    initializer_range = 0.02
    zero_init = False
    precision = 16
    seq_vocab_size = 5
    max_len = 500
    
class CustomRnaFormer(nn.Module):
    def __init__(self, config = CONFIGRNAFORMER):
        super().__init__()
        self.seq2mat_embed = EmbedSequence2Matrix(config)
        self.RNAformer = RNAformerStack(config)
        self.proj_out = nn.Sequential(
            nn.LayerNorm(config.model_dim),  # Layer Normalization
            nn.Linear(config.model_dim, config.model_dim // 2),  # Linear Layer
            nn.GELU(),  # GeLU activation
            nn.Linear(config.model_dim // 2, 2),  # Final Linear Layer with 2 outputs
        )

    def make_pair_mask(self, src, src_len):
        encode_mask = torch.arange(src.shape[1], device=src.device).expand(
            src.shape[:2]
        ) < src_len.unsqueeze(1)

        pair_mask = encode_mask[:, None, :] * encode_mask[:, :, None]

        assert isinstance(pair_mask, torch.BoolTensor) or isinstance(
            pair_mask, torch.cuda.BoolTensor
        )
        return torch.bitwise_not(pair_mask)

    def extract_features(self, x, mask_L):
        n_n_mask = self.make_pair_mask(x, mask_L)
        n_n_latent = self.seq2mat_embed(x)
        n_n_latent.masked_fill_(n_n_mask[:, :, :, None], 0.0)
        latent = self.RNAformer(
            pair_act=n_n_latent, pair_mask=n_n_mask, cycle_infer=False
        )
        latent.masked_fill_(n_n_mask.unsqueeze(-1), 0.0)
        return latent.max(2)[0]

    def forward(self, x0):
        mask = x0["mask"]
        L0 = mask.shape[1]
        Lmax = mask.sum(-1).max() + 5
        mask = mask[:, :Lmax]
        x = x0["seq"][:, :Lmax]
        mask_L = x0["mask"].sum(-1)
        out = self.extract_features(x, mask_L)
        out = self.proj_out(out)
        out = F.pad(out, (0, 0, 0, L0 - Lmax, 0, 0))
    
        return out

def CustomRnaFormerV0():
    md = CustomRnaFormer()
    load_matching_weights(md, '/opt/slh/rna/eda/rna_former.pth')
    return md
