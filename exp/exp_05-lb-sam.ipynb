{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ef213b-1e08-48b4-bd35-622c9e69969b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f461f7de-9a31-40da-95b8-91a29e0e966c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from rnacomp.fastai_fit import *\n",
    "from rnacomp.dataset import RNA_DatasetV1, LenMatchBatchSampler, DeviceDataLoader\n",
    "from rnacomp.models import GraphTransformer\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from x_transformers import TransformerWrapper, Decoder, Encoder\n",
    "\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self,dim=192, depth=12,  head_size=32):\n",
    "        super().__init__()\n",
    "        self.dec = TransformerWrapper(\n",
    "            num_tokens = 4,\n",
    "            logits_dim = 2, \n",
    "            max_seq_len = 512,\n",
    "            attn_layers = Decoder(\n",
    "                dim = dim,\n",
    "                depth = depth,\n",
    "                heads = dim//head_size,\n",
    "                attn_flash = True, \n",
    "                rotary_pos_emb = True\n",
    "            )\n",
    "         )\n",
    "        \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:,:Lmax]\n",
    "        x = x0['seq'][:,:Lmax]\n",
    "        x = self.dec(x, mask = mask)\n",
    "        return x\n",
    "    \n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class RNA_Model(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(4,dim)\n",
    "        self.pos_enc = SinusoidalPosEmb(dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "                dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)\n",
    "        self.proj_out = nn.Linear(dim,2)\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:,:Lmax]\n",
    "        x = x0['seq'][:,:Lmax]\n",
    "        \n",
    "        pos = torch.arange(Lmax, device=x.device).unsqueeze(0)\n",
    "        pos = self.pos_enc(pos)\n",
    "        x = self.emb(x)\n",
    "        x = x + pos\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=~mask)\n",
    "        x = self.proj_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a3246f-8f9c-4adb-a7bc-2a034a233f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "def loss(pred,target):\n",
    "    p = pred[target['mask'][:,:pred.shape[1]]]\n",
    "    y = target['react'][target['mask']].clip(0,1)\n",
    "    loss = F.l1_loss(p, y, reduction='none')\n",
    "    loss = loss[~torch.isnan(loss)].mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def loss_unc(pred,target):\n",
    "    p = pred[target['mask'][:,:pred.shape[1]]]\n",
    "    y = target['react'][target['mask']]#.clip(0,1)\n",
    "    loss = F.l1_loss(p, y, reduction='none')\n",
    "    loss = loss[~torch.isnan(loss)].mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class MAE(Metric):\n",
    "    def __init__(self): \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self): \n",
    "        self.x,self.y = [],[]\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        x = learn.pred[learn.y['mask'][:,:learn.pred.shape[1]]].clip(0,1)\n",
    "        y = learn.y['react'][learn.y['mask']].clip(0,1)\n",
    "        self.x.append(x)\n",
    "        self.y.append(y)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        x,y = torch.cat(self.x,0),torch.cat(self.y,0)\n",
    "        loss = F.l1_loss(x, y, reduction='none')\n",
    "        loss = loss[~torch.isnan(loss)].mean()\n",
    "        return loss\n",
    "    \n",
    "def loss_laplace(pred,target):\n",
    "    p = pred[target['mask'][:,:pred.shape[1]]].float()\n",
    "    y = target['react'][target['mask']].clip(0,1).float()\n",
    "    y_err = target['react_err'][target['mask']].float()\n",
    "    \n",
    "    loss = F.l1_loss(p, y, reduction='none')\n",
    "    m = ~torch.isnan(loss)\n",
    "    loss = (loss[m]/torch.sqrt(1.0+torch.nan_to_num(y_err[m],100.0).clip(0,100.0))).mean()\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4fc4778-26fa-4b5f-8a5d-d283d700fad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    path = Path(\"../data/split\")\n",
    "    pathbb = Path(\"../data/Ribonanza_bpp_files\")\n",
    "    split_id = 'v0'\n",
    "    bs = 1024\n",
    "    num_workers = 12\n",
    "    device = 'cuda'\n",
    "    adjnact_prob = 0.5\n",
    "    seed = 2023\n",
    "    out = 'exp_05_lb_sam'\n",
    "    \n",
    "    dim = 192\n",
    "    depth = 12\n",
    "    dim_head = 32\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "os.makedirs(CFG.out, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1b50d7e-15b1-4477-b89a-0015ff991ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fns = list(CFG.pathbb.rglob(\"*.txt\"))\n",
    "bpp_df = pd.DataFrame({\"bpp\": fns})\n",
    "bpp_df['sequence_id'] = bpp_df['bpp'].apply(lambda x: x.stem)\n",
    "df_train = pd.read_parquet(CFG.path / f\"train_data_{CFG.split_id}.parquet\")\n",
    "df_valid = pd.read_parquet(CFG.path / f\"valid_data_{CFG.split_id}.parquet\")\n",
    "df_train = df_train.merge(bpp_df, on=\"sequence_id\", how=\"left\").reset_index(drop=True)\n",
    "df_valid = df_valid.merge(bpp_df, on=\"sequence_id\", how=\"left\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "ds_train = RNA_DatasetV1(df_train)\n",
    "ds_train_len = RNA_DatasetV1(df_train, mask_only=True)\n",
    "sampler_train = torch.utils.data.RandomSampler(ds_train_len)\n",
    "len_sampler_train = LenMatchBatchSampler(sampler_train, batch_size=CFG.bs,\n",
    "            drop_last=True)\n",
    "dl_train = DeviceDataLoader(torch.utils.data.DataLoader(ds_train, \n",
    "            batch_sampler=len_sampler_train, num_workers=CFG.num_workers,\n",
    "            persistent_workers=True), CFG.device)\n",
    "\n",
    "\n",
    "ds_val = RNA_DatasetV1(df_valid)\n",
    "ds_val_len = RNA_DatasetV1(df_valid,mask_only=True)\n",
    "sampler_val = torch.utils.data.SequentialSampler(ds_val_len)\n",
    "len_sampler_val = LenMatchBatchSampler(sampler_val, batch_size=CFG.bs, \n",
    "               drop_last=True)\n",
    "dl_val= DeviceDataLoader(torch.utils.data.DataLoader(ds_val, \n",
    "               batch_sampler=len_sampler_val, num_workers=CFG.num_workers), CFG.device)\n",
    "gc.collect()\n",
    "data = DataLoaders(dl_train,dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a69b026-0d03-41ce-a0b9-e70cc655bf84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#x, y  = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6647b555-c4f7-44b7-8d4f-c90b9d5bf2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "from einops import rearrange, repeat, reduce, pack, unpack\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from typing import Union, Tuple\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, scale_base = 512, use_xpos = True):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        self.use_xpos = use_xpos\n",
    "        self.scale_base = scale_base\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "        self.register_buffer('scale', scale)\n",
    "\n",
    "    def forward(self, seq_len, device='cuda'):\n",
    "        t = torch.arange(seq_len, device = device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum('i , j -> i j', t, self.inv_freq)\n",
    "        freqs = torch.cat((freqs, freqs), dim = -1)\n",
    "\n",
    "        if not self.use_xpos:\n",
    "            return freqs, torch.ones(1, device = device)\n",
    "\n",
    "        power = (t - (seq_len // 2)) / self.scale_base\n",
    "        scale = self.scale ** rearrange(power, 'n -> n 1')\n",
    "        scale = torch.cat((scale, scale), dim = -1)\n",
    "\n",
    "        return freqs, scale\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(pos, t, scale = 1.):\n",
    "    return (t * pos.cos() * scale) + (rotate_half(t) * pos.sin() * scale)\n",
    "\n",
    "class Conv1D(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        if src_key_padding_mask is not None: \n",
    "            self.src_key_padding_mask = src_key_padding_mask\n",
    "        if self.src_key_padding_mask is not None:\n",
    "            x = torch.where(self.src_key_padding_mask.unsqueeze(-1\n",
    "                    ).expand(-1,-1,x.shape[-1]).bool(), torch.zeros_like(x), x)\n",
    "        return super().forward(x.permute(0,2,1)).permute(0,2,1)\n",
    "    \n",
    "class ResBlock(nn.Sequential):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__(nn.LayerNorm(d_model), nn.GELU(), \n",
    "                         Conv1D(d_model,d_model,3,padding=1))\n",
    "        self.src_key_padding_mask = None\n",
    "        \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        self[-1].src_key_padding_mask = src_key_padding_mask if \\\n",
    "            src_key_padding_mask is not None else self.src_key_padding_mask\n",
    "        return x + super().forward(x)\n",
    "    \n",
    "class Extractor(nn.Sequential):\n",
    "    def __init__(self, d_model, in_ch=4):\n",
    "        super().__init__(nn.Embedding(in_ch,d_model//4), \n",
    "                Conv1D(d_model//4,d_model,7,padding=3), \n",
    "                ResBlock(d_model))\n",
    "        \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        for i in [1,2]:\n",
    "            self[i].src_key_padding_mask = src_key_padding_mask\n",
    "        return super().forward(x)\n",
    "\n",
    "#BEiTv2 block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "            \n",
    "        self.emb = RotaryEmbedding(dim)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        q = k = v = self.norm1(x)\n",
    "        positions, scale = self.emb(x.shape[1],x.device)\n",
    "        q = apply_rotary_pos_emb(positions, q, scale)\n",
    "        k = apply_rotary_pos_emb(positions, k, scale ** -1)\n",
    "        \n",
    "        if self.gamma_1 is None:\n",
    "            x = x + self.drop_path(self.attn(q,k,v,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(q,k,v,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "    \n",
    "class Block_conv(Block):\n",
    "    def __init__(self, dim, mlp_ratio, *args, **kwargs):\n",
    "        super().__init__(dim, *args, **kwargs)\n",
    "        self.mlp.fc1= Conv1D(dim,dim,3,padding=1)\n",
    "        self.mlp.fc2 = Conv1D(dim,dim,3,padding=1)\n",
    "\n",
    "    def forward(self, *args, key_padding_mask=None, **kwargs):\n",
    "        self.mlp.fc1.src_key_padding_mask = key_padding_mask\n",
    "        self.mlp.fc2.src_key_padding_mask = key_padding_mask\n",
    "        return super().forward(*args, **kwargs)\n",
    "    \n",
    "class RNA_Model(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        #self.emb = nn.Sequential(nn.Embedding(4,dim//4), Conv1D(dim//4,dim,7,padding=3),\n",
    "        #                        nn.LayerNorm(dim), nn.GELU(), Conv1D(dim,dim,3,padding=1))\n",
    "        self.extractor = Extractor(dim)\n",
    "        #self.pos_enc = SinusoidalPosEmb(dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([ \n",
    "            Block_conv(\n",
    "                dim=dim, num_heads=dim//head_size, mlp_ratio=4, drop_path=0.2*(i/(depth-1)), init_values=1,\n",
    "                drop=0.1)\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        #self.transformer = nn.TransformerEncoder(\n",
    "        #    TransformerEncoderLayer_conv(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "        #        dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)\n",
    "        self.proj_out = nn.Linear(dim,2)\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:,:Lmax]\n",
    "        x = x0['seq'][:,:Lmax]\n",
    "        \n",
    "        #pos = torch.arange(Lmax, device=x.device).unsqueeze(0)\n",
    "        #pos = self.pos_enc(pos)\n",
    "        #self.emb[1].src_key_padding_mask=~mask\n",
    "        #self.emb[4].src_key_padding_mask=~mask\n",
    "        #x = self.emb(x)\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        #x = x + pos\n",
    "        \n",
    "        #x = self.transformer(x, src_key_padding_mask=~mask)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x,key_padding_mask=~mask)   \n",
    "        x = self.proj_out(x)\n",
    "        \n",
    "        x = F.pad(x,(0,0,0,L0-Lmax,0,0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf28f1a3-48f3-4a3e-8347-68d0a1449ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.180769</td>\n",
       "      <td>1.330352</td>\n",
       "      <td>0.508791</td>\n",
       "      <td>26:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.024785</td>\n",
       "      <td>1.145629</td>\n",
       "      <td>0.508791</td>\n",
       "      <td>26:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.909499</td>\n",
       "      <td>1.004368</td>\n",
       "      <td>0.508790</td>\n",
       "      <td>26:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.820862</td>\n",
       "      <td>0.907868</td>\n",
       "      <td>0.508432</td>\n",
       "      <td>26:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.752007</td>\n",
       "      <td>0.832408</td>\n",
       "      <td>0.481439</td>\n",
       "      <td>26:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.699856</td>\n",
       "      <td>0.772221</td>\n",
       "      <td>0.447463</td>\n",
       "      <td>27:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.660005</td>\n",
       "      <td>0.731462</td>\n",
       "      <td>0.415771</td>\n",
       "      <td>26:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.629822</td>\n",
       "      <td>0.689449</td>\n",
       "      <td>0.380177</td>\n",
       "      <td>26:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.607132</td>\n",
       "      <td>0.666272</td>\n",
       "      <td>0.353707</td>\n",
       "      <td>26:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.591858</td>\n",
       "      <td>0.651014</td>\n",
       "      <td>0.341738</td>\n",
       "      <td>26:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.579325</td>\n",
       "      <td>0.638581</td>\n",
       "      <td>0.334288</td>\n",
       "      <td>26:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.572407</td>\n",
       "      <td>0.630484</td>\n",
       "      <td>0.328278</td>\n",
       "      <td>26:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.568336</td>\n",
       "      <td>0.625931</td>\n",
       "      <td>0.326441</td>\n",
       "      <td>26:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.565469</td>\n",
       "      <td>0.622983</td>\n",
       "      <td>0.324656</td>\n",
       "      <td>26:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.565197</td>\n",
       "      <td>0.622518</td>\n",
       "      <td>0.324185</td>\n",
       "      <td>26:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.565130</td>\n",
       "      <td>0.622424</td>\n",
       "      <td>0.323785</td>\n",
       "      <td>26:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(data,\n",
    "                RNA_Model().cuda(), \n",
    "                path = CFG.out, \n",
    "                loss_func=loss_laplace,\n",
    "                cbs=[GradientClip(3.0),\n",
    "                       CSVLogger(),\n",
    "                     SAM(rho=.01),\n",
    "                    SaveModelCallback(monitor='mae',comp=np.less,every_epoch=True)],\n",
    "                metrics=[MAE()]).to_fp16() \n",
    "learn.fit_one_cycle(16, lr_max=5e-4, wd=0.05, pct_start=0.02)\n",
    "#learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a112b-ff31-4b7a-96b0-575c94d2ecd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
