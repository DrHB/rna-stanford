{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ef213b-1e08-48b4-bd35-622c9e69969b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f461f7de-9a31-40da-95b8-91a29e0e966c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from rnacomp.fastai_fit import *\n",
    "from rnacomp.dataset import RNA_DatasetV1, LenMatchBatchSampler, DeviceDataLoader\n",
    "from rnacomp.models import GraphTransformer\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from x_transformers import TransformerWrapper, Decoder, Encoder\n",
    "from transformers.optimization import (\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a3246f-8f9c-4adb-a7bc-2a034a233f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "def loss(pred,target):\n",
    "    p = pred[target['mask'][:,:pred.shape[1]]]\n",
    "    y = target['react'][target['mask']].clip(0,1)\n",
    "    loss = F.l1_loss(p, y, reduction='none')\n",
    "    loss = loss[~torch.isnan(loss)].mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def loss_unc(pred,target):\n",
    "    p = pred[target['mask'][:,:pred.shape[1]]]\n",
    "    y = target['react'][target['mask']]#.clip(0,1)\n",
    "    loss = F.l1_loss(p, y, reduction='none')\n",
    "    loss = loss[~torch.isnan(loss)].mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class MAE(Metric):\n",
    "    def __init__(self): \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self): \n",
    "        self.x,self.y = [],[]\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        x = learn.pred[learn.y['mask'][:,:learn.pred.shape[1]]].clip(0,1)\n",
    "        y = learn.y['react'][learn.y['mask']].clip(0,1)\n",
    "        self.x.append(x)\n",
    "        self.y.append(y)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        x,y = torch.cat(self.x,0),torch.cat(self.y,0)\n",
    "        loss = F.l1_loss(x, y, reduction='none')\n",
    "        loss = loss[~torch.isnan(loss)].mean()\n",
    "        return loss\n",
    "    \n",
    "def loss_laplace(pred,target):\n",
    "    p = pred[target['mask'][:,:pred.shape[1]]].float()\n",
    "    y = target['react'][target['mask']].clip(0,1).float()\n",
    "    y_err = target['react_err'][target['mask']].float()\n",
    "    \n",
    "    loss = F.l1_loss(p, y, reduction='none')\n",
    "    m = ~torch.isnan(loss)\n",
    "    loss = (loss[m]/torch.sqrt(1.0+torch.nan_to_num(y_err[m],100.0).clip(0,100.0))).mean()\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59517a57-89e6-45c7-ae4d-1c9f6d52634e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "from einops import rearrange, repeat, reduce, pack, unpack\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from typing import Union, Tuple\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, scale_base = 512, use_xpos = True):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        self.use_xpos = use_xpos\n",
    "        self.scale_base = scale_base\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "        self.register_buffer('scale', scale)\n",
    "\n",
    "    def forward(self, seq_len, device='cuda'):\n",
    "        t = torch.arange(seq_len, device = device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum('i , j -> i j', t, self.inv_freq)\n",
    "        freqs = torch.cat((freqs, freqs), dim = -1)\n",
    "\n",
    "        if not self.use_xpos:\n",
    "            return freqs, torch.ones(1, device = device)\n",
    "\n",
    "        power = (t - (seq_len // 2)) / self.scale_base\n",
    "        scale = self.scale ** rearrange(power, 'n -> n 1')\n",
    "        scale = torch.cat((scale, scale), dim = -1)\n",
    "\n",
    "        return freqs, scale\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(pos, t, scale = 1.):\n",
    "    return (t * pos.cos() * scale) + (rotate_half(t) * pos.sin() * scale)\n",
    "\n",
    "class Conv1D(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        if src_key_padding_mask is not None: \n",
    "            self.src_key_padding_mask = src_key_padding_mask\n",
    "        if self.src_key_padding_mask is not None:\n",
    "            x = torch.where(self.src_key_padding_mask.unsqueeze(-1\n",
    "                    ).expand(-1,-1,x.shape[-1]).bool(), torch.zeros_like(x), x)\n",
    "        return super().forward(x.permute(0,2,1)).permute(0,2,1)\n",
    "    \n",
    "class ResBlock(nn.Sequential):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__(nn.LayerNorm(d_model), nn.GELU(), \n",
    "                         Conv1D(d_model,d_model,3,padding=1))\n",
    "        self.src_key_padding_mask = None\n",
    "        \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        self[-1].src_key_padding_mask = src_key_padding_mask if \\\n",
    "            src_key_padding_mask is not None else self.src_key_padding_mask\n",
    "        return x + super().forward(x)\n",
    "    \n",
    "class Extractor(nn.Sequential):\n",
    "    def __init__(self, d_model, in_ch=4):\n",
    "        super().__init__(nn.Embedding(in_ch,d_model//4), \n",
    "                Conv1D(d_model//4,d_model,7,padding=3), \n",
    "                ResBlock(d_model))\n",
    "        \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        for i in [1,2]:\n",
    "            self[i].src_key_padding_mask = src_key_padding_mask\n",
    "        return super().forward(x)\n",
    "\n",
    "#BEiTv2 block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "            \n",
    "        self.emb = RotaryEmbedding(dim)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        q = k = v = self.norm1(x)\n",
    "        positions, scale = self.emb(x.shape[1],x.device)\n",
    "        q = apply_rotary_pos_emb(positions, q, scale)\n",
    "        k = apply_rotary_pos_emb(positions, k, scale ** -1)\n",
    "        \n",
    "        if self.gamma_1 is None:\n",
    "            x = x + self.drop_path(self.attn(q,k,v,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(q,k,v,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "    \n",
    "class Block_conv(Block):\n",
    "    def __init__(self, dim, mlp_ratio, *args, **kwargs):\n",
    "        super().__init__(dim, *args, **kwargs)\n",
    "        self.mlp.fc1= Conv1D(dim,dim,3,padding=1)\n",
    "        self.mlp.fc2 = Conv1D(dim,dim,3,padding=1)\n",
    "\n",
    "    def forward(self, *args, key_padding_mask=None, **kwargs):\n",
    "        self.mlp.fc1.src_key_padding_mask = key_padding_mask\n",
    "        self.mlp.fc2.src_key_padding_mask = key_padding_mask\n",
    "        return super().forward(*args, **kwargs)\n",
    "    \n",
    "class RNA_Model(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        #self.emb = nn.Sequential(nn.Embedding(4,dim//4), Conv1D(dim//4,dim,7,padding=3),\n",
    "        #                        nn.LayerNorm(dim), nn.GELU(), Conv1D(dim,dim,3,padding=1))\n",
    "        self.extractor = Extractor(dim)\n",
    "        #self.pos_enc = SinusoidalPosEmb(dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([ \n",
    "            Block_conv(\n",
    "                dim=dim, num_heads=dim//head_size, mlp_ratio=4, drop_path=0.2*(i/(depth-1)), init_values=1,\n",
    "                drop=0.1)\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        #self.transformer = nn.TransformerEncoder(\n",
    "        #    TransformerEncoderLayer_conv(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "        #        dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)\n",
    "        self.proj_out = nn.Linear(dim,2)\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:,:Lmax]\n",
    "        x = x0['seq'][:,:Lmax]\n",
    "        \n",
    "        #pos = torch.arange(Lmax, device=x.device).unsqueeze(0)\n",
    "        #pos = self.pos_enc(pos)\n",
    "        #self.emb[1].src_key_padding_mask=~mask\n",
    "        #self.emb[4].src_key_padding_mask=~mask\n",
    "        #x = self.emb(x)\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        #x = x + pos\n",
    "        \n",
    "        #x = self.transformer(x, src_key_padding_mask=~mask)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x,key_padding_mask=~mask)   \n",
    "        x = self.proj_out(x)\n",
    "        \n",
    "        x = F.pad(x,(0,0,0,L0-Lmax,0,0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4fc4778-26fa-4b5f-8a5d-d283d700fad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    path = Path(\"../data/split\")\n",
    "    pathbb = Path(\"../data/Ribonanza_bpp_files\")\n",
    "    split_id = 'v0'\n",
    "    bs = 1024\n",
    "    num_workers = 8\n",
    "    device = 'cuda'\n",
    "    adjnact_prob = 0.5\n",
    "    seed = 2023\n",
    "    folder = Path('exp_test')\n",
    "    out = 'exp_05_lbs_asam'\n",
    "    \n",
    "    dim = 192\n",
    "    depth = 12\n",
    "    dim_head = 32\n",
    "    \n",
    "    \n",
    "    lr=5e-4\n",
    "    wd =5e-4\n",
    "    warm_up_pct = 0.02\n",
    "    epochs = 16\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "os.makedirs(CFG.out, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b50d7e-15b1-4477-b89a-0015ff991ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fns = list(CFG.pathbb.rglob(\"*.txt\"))\n",
    "bpp_df = pd.DataFrame({\"bpp\": fns})\n",
    "bpp_df['sequence_id'] = bpp_df['bpp'].apply(lambda x: x.stem)\n",
    "df_train = pd.read_parquet(CFG.path / f\"train_data_{CFG.split_id}.parquet\")\n",
    "df_valid = pd.read_parquet(CFG.path / f\"valid_data_{CFG.split_id}.parquet\")\n",
    "df_train = df_train.merge(bpp_df, on=\"sequence_id\", how=\"left\").reset_index(drop=True)\n",
    "df_valid = df_valid.merge(bpp_df, on=\"sequence_id\", how=\"left\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "ds_train = RNA_DatasetV1(df_train)\n",
    "ds_train_len = RNA_DatasetV1(df_train, mask_only=True)\n",
    "sampler_train = torch.utils.data.RandomSampler(ds_train_len)\n",
    "len_sampler_train = LenMatchBatchSampler(sampler_train, batch_size=CFG.bs,\n",
    "            drop_last=True)\n",
    "dl_train = DeviceDataLoader(torch.utils.data.DataLoader(ds_train, \n",
    "            batch_sampler=len_sampler_train, num_workers=CFG.num_workers,\n",
    "            persistent_workers=True), CFG.device)\n",
    "\n",
    "\n",
    "ds_val = RNA_DatasetV1(df_valid)\n",
    "ds_val_len = RNA_DatasetV1(df_valid,mask_only=True)\n",
    "sampler_val = torch.utils.data.SequentialSampler(ds_val_len)\n",
    "len_sampler_val = LenMatchBatchSampler(sampler_val, batch_size=CFG.bs, \n",
    "               drop_last=True)\n",
    "dl_val= DeviceDataLoader(torch.utils.data.DataLoader(ds_val, \n",
    "               batch_sampler=len_sampler_val, num_workers=CFG.num_workers), CFG.device)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad34580-add2-4683-b368-54daf3767322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function fastai.optimizer.RAdam(params: 'Tensor | Iterable', lr: 'float | slice', mom: 'float' = 0.9, sqr_mom: 'float' = 0.99, eps: 'float' = 1e-05, wd: 'Real' = 0.0, beta: 'float' = 0.0, decouple_wd: 'bool' = True) -> 'Optimizer'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a69b026-0d03-41ce-a0b9-e70cc655bf84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "custom_model = RNA_Model()\n",
    "opt = torch.optim.SGD(custom_model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n",
    "loss_func = loss_laplace\n",
    "warmup_steps = int(len(dl_train) * CFG.warm_up_pct * CFG.epochs)\n",
    "total_steps = int(len(dl_train) * CFG.epochs)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    opt,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59878d8c-4b31-4978-b8a4-71ab0b03b7a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SaveModelEpoch:\n",
    "    def __init__(self, folder, exp_name, best=-np.inf):\n",
    "        self.best = best\n",
    "        self.folder = Path(folder)\n",
    "        self.exp_name = exp_name\n",
    "\n",
    "    def __call__(self, score, model, epoch):\n",
    "        self.best = score\n",
    "        print(f\"Better model found at epoch {epoch} with value: {self.best}.\")\n",
    "        torch.save(model.state_dict(), f\"{self.folder/self.exp_name}_{epoch}.pth\")\n",
    "\n",
    "        \n",
    "def custom_metric(x, y):\n",
    "        loss = F.l1_loss(x, y, reduction='none')\n",
    "        loss = loss[~torch.isnan(loss)].mean()\n",
    "        return loss.item()\n",
    "        \n",
    "\n",
    "def fit(\n",
    "    epochs,\n",
    "    model,\n",
    "    train_dl,\n",
    "    valid_dl,\n",
    "    loss_fn,\n",
    "    opt,\n",
    "    metric,\n",
    "    folder=\"models\",\n",
    "    exp_name=\"exp_00\",\n",
    "    device=None,\n",
    "    sched=None,\n",
    "    save_md=SaveModelEpoch,\n",
    "):\n",
    "    if device is None:\n",
    "        device = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    mb = master_bar(range(epochs))\n",
    "    mb.write([\"epoch\", \"train_loss\", \"valid_loss\", \"val_metric\"], table=True)\n",
    "    model.to(device)  # we have to put our model on gpu\n",
    "    scaler = torch.cuda.amp.GradScaler()  # this for half precision training\n",
    "    save_md = save_md(folder, exp_name)\n",
    "\n",
    "    for i in mb:  # iterating  epoch\n",
    "        trn_loss, val_loss = 0.0, 0.0\n",
    "        model.train()  # set model for training\n",
    "        for batch in progress_bar(train_dl, parent=mb):\n",
    "            # putting batches to device\n",
    "            x, y = batch\n",
    "            with torch.cuda.amp.autocast():  # half precision\n",
    "                out = model(x)  # forward pass\n",
    "                loss = loss_fn(out, y)  # calulation loss\n",
    "\n",
    "            trn_loss += loss.item()\n",
    "\n",
    "            scaler.scale(loss).backward()  # backward\n",
    "            scaler.step(opt)  # optimzers step\n",
    "            scaler.update()  # for half precision\n",
    "            opt.zero_grad()  # zeroing optimizer\n",
    "            if sched is not None:\n",
    "                sched.step()  # scuedular step\n",
    "\n",
    "        trn_loss /= mb.child.total\n",
    "\n",
    "        # putting model in eval mode\n",
    "        model.eval()\n",
    "        gt = []\n",
    "        pred = []\n",
    "        # after epooch is done we can run a validation dataloder and see how are doing\n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar(valid_dl, parent=mb):\n",
    "                x, y = batch\n",
    "                with torch.cuda.amp.autocast():  # half precision\n",
    "                    out = model(x)  # forward pass\n",
    "                    loss = loss_fn(out, y)  # calulation loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                gt.append(y['react'][y['mask']].clip(0,1).detach())\n",
    "                pred.append(out[y['mask'][:,:out.shape[1]]].clip(0,1).detach())\n",
    "        # calculating metric\n",
    "        metric_ = metric(torch.cat(pred), torch.cat(gt))\n",
    "        # saving model if necessary\n",
    "        save_md(metric_, model, i)\n",
    "        val_loss /= mb.child.total\n",
    "        res = pd.DataFrame(\n",
    "            {\n",
    "                \"epoch\": [i],\n",
    "                \"train_loss\": [trn_loss],\n",
    "                \"valid_loss\": [val_loss],\n",
    "                \"metric\": [metric_],\n",
    "            }\n",
    "        )\n",
    "        res.to_csv(f\"{Path(folder)/exp_name}_{i}.csv\", index=False)\n",
    "        mb.write(\n",
    "            [\n",
    "                i,\n",
    "                f\"{trn_loss:.6f}\",\n",
    "                f\"{val_loss:.6f}\",\n",
    "                f\"{metric_:.6f}\",\n",
    "            ],\n",
    "            table=True,\n",
    "        )\n",
    "        gc.collect()\n",
    "    print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e184e170-763c-4756-915a-1fe9325e261c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>val_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.254781</td>\n",
       "      <td>0.249498</td>\n",
       "      <td>0.325098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.212609</td>\n",
       "      <td>0.237302</td>\n",
       "      <td>0.308779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.205578</td>\n",
       "      <td>0.228639</td>\n",
       "      <td>0.296958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.200865</td>\n",
       "      <td>0.221793</td>\n",
       "      <td>0.287387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.197540</td>\n",
       "      <td>0.217176</td>\n",
       "      <td>0.280869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.195017</td>\n",
       "      <td>0.211003</td>\n",
       "      <td>0.271894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.192988</td>\n",
       "      <td>0.207470</td>\n",
       "      <td>0.266651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.191508</td>\n",
       "      <td>0.203579</td>\n",
       "      <td>0.260649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.190316</td>\n",
       "      <td>0.201192</td>\n",
       "      <td>0.256840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.189457</td>\n",
       "      <td>0.198993</td>\n",
       "      <td>0.253089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.188830</td>\n",
       "      <td>0.197375</td>\n",
       "      <td>0.250067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.188387</td>\n",
       "      <td>0.196487</td>\n",
       "      <td>0.248170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.188105</td>\n",
       "      <td>0.195840</td>\n",
       "      <td>0.246501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.187919</td>\n",
       "      <td>0.195636</td>\n",
       "      <td>0.245843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.187828</td>\n",
       "      <td>0.195520</td>\n",
       "      <td>0.245387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.187798</td>\n",
       "      <td>0.195510</td>\n",
       "      <td>0.245336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with value: 0.3250979466443659.\n",
      "Better model found at epoch 1 with value: 0.30877905826308033.\n",
      "Better model found at epoch 2 with value: 0.29695820473827766.\n",
      "Better model found at epoch 3 with value: 0.2873868415822642.\n",
      "Better model found at epoch 4 with value: 0.2808691671122807.\n",
      "Better model found at epoch 5 with value: 0.2718935343029247.\n",
      "Better model found at epoch 6 with value: 0.2666510134021157.\n",
      "Better model found at epoch 7 with value: 0.2606492465488646.\n",
      "Better model found at epoch 8 with value: 0.2568397928802657.\n",
      "Better model found at epoch 9 with value: 0.25308930475239655.\n",
      "Better model found at epoch 10 with value: 0.2500666057521908.\n",
      "Better model found at epoch 11 with value: 0.24816982319985417.\n",
      "Better model found at epoch 12 with value: 0.2465007240385073.\n",
      "Better model found at epoch 13 with value: 0.24584259637350458.\n",
      "Better model found at epoch 14 with value: 0.24538674807148417.\n",
      "Better model found at epoch 15 with value: 0.2453356349595801.\n",
      "Training done\n"
     ]
    }
   ],
   "source": [
    "fit(    train_dl = dl_train,\n",
    "        valid_dl = dl_val,\n",
    "        epochs=CFG.epochs,\n",
    "        model=custom_model,\n",
    "        loss_fn=loss_laplace,\n",
    "        opt=opt,\n",
    "        metric=custom_metric,\n",
    "        folder=CFG.folder/CFG.out,\n",
    "        exp_name=f\"{CFG.out}\",\n",
    "        device=CFG.device,\n",
    "        sched=scheduler,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae1a9a-8215-4e8f-8ab0-30f34839403c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a112b-ff31-4b7a-96b0-575c94d2ecd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
