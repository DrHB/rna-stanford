{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1529148/999603196.py:16: UserWarning: \u001b[35mCould not import flash_attn\u001b[0m\n",
      "  warnings.warn(colored('Could not import flash_attn', 'magenta'))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import warnings\n",
    "from termcolor import colored\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "try:\n",
    "    from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func, flash_attn_unpadded_kvpacked_func\n",
    "    from flash_attn.bert_padding import unpad_input, pad_input\n",
    "    from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\n",
    "except:\n",
    "    warnings.warn(colored('Could not import flash_attn', 'magenta'))\n",
    "    load_flash_attn_check = False\n",
    "\n",
    "\n",
    "class FlashAttention2d(nn.Module):\n",
    "    def __init__(self, model_dim, num_head, softmax_scale,\n",
    "                 zero_init, use_bias, initializer_range, n_layers):\n",
    "        super().__init__()\n",
    "        assert model_dim % num_head == 0\n",
    "        assert model_dim % num_head == 0\n",
    "        self.key_dim = model_dim // num_head\n",
    "        self.value_dim = model_dim // num_head\n",
    "\n",
    "        self.causal = False\n",
    "        self.checkpointing = False\n",
    "\n",
    "        if softmax_scale:\n",
    "            self.softmax_scale = self.key_dim ** (-0.5)\n",
    "        else:\n",
    "            self.softmax_scale = None\n",
    "\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.Wqkv = nn.Linear(model_dim, 3 * model_dim, bias=use_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(model_dim, model_dim, bias=use_bias)\n",
    "\n",
    "        self.initialize(zero_init, use_bias, initializer_range, n_layers)\n",
    "\n",
    "    def initialize(self, zero_init, use_bias, initializer_range, n_layers):\n",
    "\n",
    "        nn.init.normal_(self.Wqkv.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "        if use_bias:\n",
    "            nn.init.constant_(self.Wqkv.bias, 0.0)\n",
    "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
    "\n",
    "        if zero_init:\n",
    "            nn.init.constant_(self.out_proj.weight, 0.0)\n",
    "        else:\n",
    "            nn.init.normal_(self.out_proj.weight, mean=0.0, std=initializer_range / math.sqrt(2 * n_layers))\n",
    "\n",
    "    def forward(self, pair_act, attention_mask):\n",
    "\n",
    "        batch_size = pair_act.shape[0]\n",
    "        seqlen = pair_act.shape[1]\n",
    "        extended_batch_size = batch_size * seqlen\n",
    "\n",
    "        qkv = self.Wqkv(pair_act)\n",
    "        not_attention_mask = torch.logical_not(attention_mask)\n",
    "\n",
    "        x_qkv = rearrange(qkv, 'b s f ... -> (b s) f ...', b=batch_size, f=seqlen, s=seqlen)\n",
    "        key_padding_mask = rearrange(not_attention_mask, 'b s f ... -> (b s) f ...', b=batch_size, f=seqlen, s=seqlen)\n",
    "\n",
    "        x_unpad, indices, cu_seqlens, max_s = unpad_input(x_qkv, key_padding_mask)\n",
    "        x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=self.num_head)\n",
    "\n",
    "        if self.training and self.checkpointing:\n",
    "            output_unpad = torch.utils.checkpoint.checkpoint(flash_attn_unpadded_qkvpacked_func,\n",
    "                                                             x_unpad, cu_seqlens, max_s, 0.0, self.softmax_scale,\n",
    "                                                             self.causal, False\n",
    "                                                             )\n",
    "        else:\n",
    "            output_unpad = flash_attn_unpadded_qkvpacked_func(\n",
    "                x_unpad, cu_seqlens, max_s, 0.0,\n",
    "                softmax_scale=self.softmax_scale, causal=self.causal\n",
    "            )\n",
    "\n",
    "        pre_pad_latent = rearrange(output_unpad, 'nnz h d -> nnz (h d)')\n",
    "        padded_latent = pad_input(pre_pad_latent, indices, extended_batch_size, seqlen)\n",
    "        output = rearrange(padded_latent, 'b f (h d) -> b f h d', h=self.num_head)\n",
    "\n",
    "        output = rearrange(output, '(b s) f h d -> b s f (h d)', b=batch_size, f=seqlen, s=seqlen)\n",
    "\n",
    "        return self.out_proj(output)\n",
    "\n",
    "\n",
    "class Attention2d(nn.Module):\n",
    "    def __init__(self, model_dim, num_head, softmax_scale,\n",
    "                 precision, zero_init, use_bias,\n",
    "                 initializer_range, n_layers):\n",
    "        super().__init__()\n",
    "        assert model_dim % num_head == 0\n",
    "        assert model_dim % num_head == 0\n",
    "        self.key_dim = model_dim // num_head\n",
    "        self.value_dim = model_dim // num_head\n",
    "\n",
    "        if softmax_scale:\n",
    "            self.softmax_scale = torch.sqrt(torch.FloatTensor([self.key_dim]))\n",
    "        else:\n",
    "            self.softmax_scale = False\n",
    "\n",
    "        self.num_head = num_head\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        if precision == \"fp32\" or precision == 32 or precision == \"bf16\":\n",
    "            self.mask_bias = -1e9\n",
    "        elif precision == \"fp16\" or precision == 16:\n",
    "            self.mask_bias = -1e4\n",
    "        else:\n",
    "            raise UserWarning(f\"unknown precision: {precision} . Please us fp16, fp32 or bf16\")\n",
    "\n",
    "        self.Wqkv = nn.Linear(model_dim, 3 * model_dim, bias=use_bias)\n",
    "        self.out_proj = nn.Linear(model_dim, model_dim, bias=use_bias)\n",
    "\n",
    "        self.initialize(zero_init, use_bias, initializer_range, n_layers)\n",
    "\n",
    "    def initialize(self, zero_init, use_bias, initializer_range, n_layers):\n",
    "\n",
    "        nn.init.normal_(self.Wqkv.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "        if use_bias:\n",
    "            nn.init.constant_(self.Wqkv.bias, 0.0)\n",
    "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
    "\n",
    "        if zero_init:\n",
    "            nn.init.constant_(self.out_proj.weight, 0.0)\n",
    "        else:\n",
    "            nn.init.normal_(self.out_proj.weight, mean=0.0, std=initializer_range / math.sqrt(2 * n_layers))\n",
    "\n",
    "    def forward(self, pair_act, attention_mask):\n",
    "\n",
    "        batch_size = pair_act.size(0)\n",
    "        N_seq = pair_act.size(1)\n",
    "        N_res = pair_act.size(2)\n",
    "\n",
    "        query, key, value = self.Wqkv(pair_act).split(self.model_dim, dim=3)\n",
    "\n",
    "        query = query.view(batch_size, N_seq, N_res, self.num_head, self.key_dim).permute(0, 1, 3, 2, 4)\n",
    "        key = key.view(batch_size, N_seq, N_res, self.num_head, self.value_dim).permute(0, 1, 3, 4, 2)\n",
    "        value = value.view(batch_size, N_seq, N_res, self.num_head, self.value_dim).permute(0, 1, 3, 2, 4)\n",
    "\n",
    "        attn_weights = torch.matmul(query, key)\n",
    "\n",
    "        if self.softmax_scale:\n",
    "            attn_weights = attn_weights / self.softmax_scale.to(pair_act.device)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask[:, :, None, None, :]\n",
    "            attn_weights.masked_fill_(attention_mask, self.mask_bias)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        weighted_avg = torch.matmul(attn_weights, value).permute(0, 1, 3, 2, 4)\n",
    "\n",
    "        output = self.out_proj(weighted_avg.reshape(batch_size, N_seq, N_res, self.num_head * self.value_dim))\n",
    "        return output\n",
    "\n",
    "\n",
    "class TriangleAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_head, orientation, softmax_scale,\n",
    "                 precision, zero_init, use_bias, flash_attn,\n",
    "                 initializer_range, n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.num_head = num_head\n",
    "\n",
    "        assert orientation in ['per_row', 'per_column']\n",
    "        self.orientation = orientation\n",
    "\n",
    "        self.input_norm = nn.LayerNorm(model_dim, eps=1e-6)\n",
    "\n",
    "        if flash_attn:\n",
    "            self.attn = FlashAttention2d(model_dim, num_head, softmax_scale, zero_init, use_bias,\n",
    "                                         initializer_range, n_layers)\n",
    "        else:\n",
    "            self.attn = Attention2d(model_dim, num_head, softmax_scale,\n",
    "                                    precision, zero_init, use_bias, initializer_range, n_layers)\n",
    "\n",
    "    def forward(self, pair_act, pair_mask, cycle_infer=False):\n",
    "\n",
    "        assert len(pair_act.shape) == 4\n",
    "\n",
    "        if self.orientation == 'per_column':\n",
    "            pair_act = torch.swapaxes(pair_act, -2, -3)\n",
    "            if pair_mask is not None:\n",
    "                pair_mask = torch.swapaxes(pair_mask, -1, -2)\n",
    "\n",
    "        pair_act = self.input_norm(pair_act)\n",
    "\n",
    "        if self.training and not cycle_infer:\n",
    "            pair_act = checkpoint(self.attn, pair_act, pair_mask, use_reentrant=True)\n",
    "        else:\n",
    "            pair_act = self.attn(pair_act, pair_mask)\n",
    "\n",
    "        if self.orientation == 'per_column':\n",
    "            pair_act = torch.swapaxes(pair_act, -2, -3)\n",
    "\n",
    "        return pair_act\n",
    "    \n",
    "class RNAformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        ff_dim = int(config.ff_factor * config.model_dim)\n",
    "\n",
    "        self.attn_pair_row = TriangleAttention(config.model_dim, config.num_head, 'per_row', config.softmax_scale,\n",
    "                                               config.precision, config.zero_init, config.use_bias, config.flash_attn,\n",
    "                                               config.initializer_range, config.n_layers)\n",
    "        self.attn_pair_col = TriangleAttention(config.model_dim, config.num_head, 'per_column', config.softmax_scale,\n",
    "                                               config.precision, config.zero_init, config.use_bias, config.flash_attn,\n",
    "                                               config.initializer_range, config.n_layers)\n",
    "\n",
    "        self.pair_dropout_row = nn.Dropout(p=config.resi_dropout / 2)\n",
    "        self.pair_dropout_col = nn.Dropout(p=config.resi_dropout / 2)\n",
    "\n",
    "        if config.ff_kernel:\n",
    "            self.pair_transition = ConvFeedForward(config.model_dim, ff_dim, use_bias=config.use_bias,\n",
    "                                                   kernel=config.ff_kernel,\n",
    "                                                   initializer_range=config.initializer_range,\n",
    "                                                   zero_init=config.zero_init,\n",
    "                                                   n_layers=config.n_layers)\n",
    "        else:\n",
    "            self.pair_transition = FeedForward(config.model_dim, ff_dim, use_bias=config.use_bias, glu=config.use_glu,\n",
    "                                               initializer_range=config.initializer_range, zero_init=config.zero_init,\n",
    "                                               n_layers=config.n_layers)\n",
    "\n",
    "        self.res_dropout = nn.Dropout(p=config.resi_dropout)\n",
    "\n",
    "    def forward(self, pair_act, pair_mask, cycle_infer=False):\n",
    "\n",
    "        pair_act = pair_act + self.pair_dropout_row(self.attn_pair_row(pair_act, pair_mask, cycle_infer))\n",
    "        pair_act = pair_act + self.pair_dropout_col(self.attn_pair_col(pair_act, pair_mask, cycle_infer))\n",
    "        pair_act = pair_act + self.res_dropout(self.pair_transition(pair_act))\n",
    "\n",
    "        return pair_act\n",
    "    \n",
    "class ConvFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim, ff_dim, use_bias, initializer_range, n_layers, kernel, zero_init=True):\n",
    "        super(ConvFeedForward, self).__init__()\n",
    "\n",
    "        self.zero_init = zero_init\n",
    "\n",
    "        self.input_norm = nn.GroupNorm(1, model_dim)\n",
    "\n",
    "        if kernel == 1:\n",
    "            self.conv1 = nn.Conv2d(model_dim, ff_dim, kernel_size=1, bias=use_bias)\n",
    "            self.conv2 = nn.Conv2d(ff_dim, model_dim, kernel_size=1, bias=use_bias)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(model_dim, ff_dim, bias=use_bias, kernel_size=kernel, padding=(kernel - 1) // 2)\n",
    "            self.conv2 = nn.Conv2d(ff_dim, model_dim, bias=use_bias, kernel_size=kernel, padding=(kernel - 1) // 2)\n",
    "\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.initialize(zero_init, use_bias, initializer_range, n_layers)\n",
    "\n",
    "    def initialize(self, zero_init, use_bias, initializer_range, n_layers):\n",
    "\n",
    "        nn.init.normal_(self.conv1.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "        if use_bias:\n",
    "            nn.init.constant_(self.conv1.bias, 0.0)\n",
    "            nn.init.constant_(self.conv2.bias, 0.0)\n",
    "\n",
    "        if zero_init:\n",
    "            nn.init.constant_(self.conv2.weight, 0.0)\n",
    "        else:\n",
    "            nn.init.normal_(self.conv2.weight, mean=0.0, std=initializer_range / math.sqrt(2 * n_layers))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.input_norm(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "    \n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PosEmbedding(nn.Module):\n",
    "    def __init__(self, vocab, model_dim, max_len, rel_pos_enc, initializer_range):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.rel_pos_enc = rel_pos_enc\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.embed_seq = nn.Embedding(vocab, model_dim)\n",
    "\n",
    "        self.scale = nn.Parameter(torch.sqrt(torch.FloatTensor([model_dim // 2])), requires_grad=False)\n",
    "\n",
    "        if rel_pos_enc:\n",
    "            self.embed_pair_pos = nn.Linear(max_len, model_dim, bias=False)\n",
    "        else:\n",
    "            self.embed_pair_pos = nn.Linear(model_dim, model_dim, bias=False)\n",
    "\n",
    "            pe = torch.zeros(max_len, model_dim)\n",
    "            position = torch.arange(0, max_len).unsqueeze(1).type(torch.FloatTensor)\n",
    "            div_term = torch.exp(\n",
    "                torch.arange(0, model_dim, 2).type(torch.FloatTensor) * -(math.log(10000.0) / model_dim))\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "            pe = pe.unsqueeze(0)\n",
    "            pe = torch.nn.Parameter(pe, requires_grad=False)\n",
    "            self.register_buffer('pe', pe)\n",
    "\n",
    "        self.initialize(initializer_range)  #\n",
    "\n",
    "    def initialize(self, initializer_range):\n",
    "\n",
    "        nn.init.normal_(self.embed_seq.weight, mean=0.0, std=initializer_range)\n",
    "        nn.init.normal_(self.embed_pair_pos.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "    def relative_position_encoding(self, src_seq):\n",
    "\n",
    "        residue_index = torch.arange(src_seq.size()[1], device=src_seq.device).expand(src_seq.size())\n",
    "        rel_pos = F.one_hot(torch.clip(residue_index, min=0, max=self.max_len - 1), self.max_len)\n",
    "\n",
    "        if isinstance(self.embed_pair_pos.weight, torch.cuda.BFloat16Tensor):\n",
    "            rel_pos = rel_pos.type(torch.bfloat16)\n",
    "        elif isinstance(self.embed_pair_pos.weight, torch.cuda.HalfTensor):\n",
    "            rel_pos = rel_pos.half()\n",
    "        else:\n",
    "            rel_pos = rel_pos.type(torch.float32)\n",
    "\n",
    "        pos_encoding = self.embed_pair_pos(rel_pos)\n",
    "        return pos_encoding\n",
    "\n",
    "    def forward(self, src_seq):\n",
    "\n",
    "        seq_embed = self.embed_seq(src_seq) * self.scale\n",
    "\n",
    "        if self.rel_pos_enc:\n",
    "            seq_embed = seq_embed + self.relative_position_encoding(src_seq)\n",
    "        else:\n",
    "            seq_embed = seq_embed + self.embed_pair_pos(self.pe[:, :src_seq.size(1)])\n",
    "\n",
    "        return seq_embed\n",
    "\n",
    "\n",
    "class EmbedSequence2Matrix(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_embed_1 = PosEmbedding(config.seq_vocab_size, config.model_dim, config.max_len,\n",
    "                                        config.rel_pos_enc, config.initializer_range)\n",
    "        self.src_embed_2 = PosEmbedding(config.seq_vocab_size, config.model_dim, config.max_len,\n",
    "                                        config.rel_pos_enc, config.initializer_range)\n",
    "\n",
    "        self.norm = nn.LayerNorm(config.model_dim)\n",
    "\n",
    "    def forward(self, src_seq):\n",
    "        seq_1_embed = self.src_embed_1(src_seq)\n",
    "        seq_2_embed = self.src_embed_2(src_seq)\n",
    "\n",
    "        pair_latent = seq_1_embed.unsqueeze(1) + seq_2_embed.unsqueeze(2)\n",
    "\n",
    "        pair_latent = self.norm(pair_latent)\n",
    "\n",
    "        return pair_latent\n",
    "    \n",
    "class RNAformerStack(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_ln = nn.LayerNorm(config.model_dim)\n",
    "\n",
    "        module_list = []\n",
    "        for idx in range(config.n_layers):\n",
    "            layer = RNAformerBlock(config=config)\n",
    "            module_list.append(layer)\n",
    "        self.layers = nn.ModuleList(module_list)\n",
    "\n",
    "    def forward(self, pair_act, pair_mask, cycle_infer=False):\n",
    "\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            pair_act = layer(pair_act, pair_mask, cycle_infer=cycle_infer)\n",
    "\n",
    "        pair_act = self.output_ln(pair_act)\n",
    "\n",
    "        return pair_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    model_dim= 256 # hidden dimension of transformer\n",
    "    n_layers= 6  # number of transformer layers\n",
    "    num_head= 4  # number of heads per layer    \n",
    "    ff_factor= 4  # hidden dim * ff_factor = size of feed-forward layer\n",
    "    ff_kernel= 3    \n",
    "    cycling = False \n",
    "    resi_dropout = 0.1\n",
    "    embed_dropout = 0.1  \n",
    "    rel_pos_enc =True  # relative position encoding\n",
    "    head_bias= False\n",
    "    ln_eps = 1e-5    \n",
    "    softmax_scale= True\n",
    "    key_dim_scaler= True\n",
    "    gating = False\n",
    "    use_glu = False\n",
    "    use_bias = True  \n",
    "    flash_attn =  False   \n",
    "    initializer_range = 0.02\n",
    "    zero_init =  False\n",
    "    precision = 16\n",
    "    seq_vocab_size = 5\n",
    "    max_len = 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "file = \"../data/all_test_data.npy\"\n",
    "df = pd.DataFrame(np.load(file, allow_pickle=True).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class IndexDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_indices):\n",
    "        self.dataset_indices = dataset_indices\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset_indices[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_indices)\n",
    "\n",
    "\n",
    "class TokenBasedRandomSampler:\n",
    "\n",
    "    def __init__(self, dataset, token_key_fn, batch_token_size, batching, repeat, sort_samples, shuffle,\n",
    "                 shuffle_pool_size, drop_last, seed=1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.token_length = [token_key_fn(s) for s in dataset]\n",
    "\n",
    "        self.batch_token_size = batch_token_size\n",
    "\n",
    "        self.repeat = repeat\n",
    "        self.batching = batching\n",
    "        self.sort_samples = sort_samples\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.shuffle_pool_size = shuffle_pool_size\n",
    "\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "        self.reverse = False\n",
    "\n",
    "        if not self.repeat:\n",
    "            self.minibatches = self.precompute_minibatches()\n",
    "\n",
    "    def __len__(self):\n",
    "        if not self.batching:\n",
    "            return len(self.token_length)\n",
    "        else:\n",
    "            return len(self.minibatches)\n",
    "\n",
    "    def get_index_list(self):\n",
    "        index_list = [i for i in range(len(self.token_length))]\n",
    "        if self.shuffle:\n",
    "            self.rng.shuffle(index_list)\n",
    "        return index_list\n",
    "\n",
    "    def get_index_iter(self):\n",
    "        while True:\n",
    "            index_list = self.get_index_list()\n",
    "            for i in index_list:\n",
    "                yield i\n",
    "\n",
    "    def pool_and_sort(self, sample_iter):\n",
    "        pool = []\n",
    "        for sample in sample_iter:\n",
    "            if not self.sort_samples:\n",
    "                yield sample\n",
    "            else:\n",
    "                pool.append(sample)\n",
    "                if len(pool) >= self.shuffle_pool_size:\n",
    "                    pool.sort(key=lambda x: self.token_length[x], reverse=self.reverse)\n",
    "                    self.reverse = not self.reverse\n",
    "                    while len(pool) > 0:\n",
    "                        yield pool.pop()\n",
    "        if len(pool) > 0:\n",
    "            pool.sort(key=lambda x: self.token_length[x], reverse=self.reverse)\n",
    "            self.reverse = not self.reverse\n",
    "            while len(pool) > 0:\n",
    "                yield pool.pop()\n",
    "\n",
    "    def get_minibatches(self, index_iter):\n",
    "\n",
    "        minibatch, max_size_in_batch = [], 0\n",
    "\n",
    "        if self.batching and self.shuffle and self.shuffle_pool_size and self.sort_samples:\n",
    "            index_iter = self.pool_and_sort(index_iter)\n",
    "\n",
    "        for sample in index_iter:\n",
    "\n",
    "            if self.batching:\n",
    "                minibatch.append(sample)\n",
    "                max_size_in_batch = max(max_size_in_batch, self.token_length[sample])\n",
    "                size_so_far = len(minibatch) * max(max_size_in_batch, self.token_length[sample])\n",
    "                if size_so_far == self.batch_token_size:\n",
    "                    yield minibatch\n",
    "                    minibatch, max_size_in_batch = [], 0\n",
    "                if size_so_far > self.batch_token_size:\n",
    "                    yield minibatch[:-1]\n",
    "                    minibatch = minibatch[-1:]\n",
    "                    max_size_in_batch = self.token_length[minibatch[0]]\n",
    "            else:\n",
    "                yield [sample]\n",
    "\n",
    "        if (not self.drop_last) and len(minibatch) > 0:\n",
    "            yield minibatch\n",
    "\n",
    "    def precompute_minibatches(self):\n",
    "        index_iter = self.get_index_list()\n",
    "\n",
    "        minibatches = [m for m in self.get_minibatches(index_iter) if len(m) > 0]\n",
    "        if self.shuffle:\n",
    "            self.rng.shuffle(minibatches)\n",
    "        return minibatches\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.repeat:\n",
    "            index_iter = self.get_index_iter()\n",
    "            for batch in self.get_minibatches(index_iter):\n",
    "                yield batch\n",
    "        else:\n",
    "\n",
    "            for m in self.minibatches:\n",
    "                yield m\n",
    "\n",
    "\n",
    "class CollatorRNA:\n",
    "\n",
    "    def __init__(self, pad_index, ignore_index):\n",
    "        self.ignore_index = ignore_index\n",
    "        self.pad_index = pad_index\n",
    "\n",
    "    def __call__(self, samples, neg_samples=False) -> List[List[int]]:\n",
    "        # tokenize the input text samples\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_dict = {k: [dic[k] for dic in samples] for k in samples[0] if k in ['length', 'pdb_sample', 'pos1id']}\n",
    "\n",
    "            batch_dict['length'] = torch.stack(batch_dict['length'])\n",
    "\n",
    "            if 'pdb_sample' in batch_dict:\n",
    "                batch_dict['pdb_sample'] = torch.stack(batch_dict['pdb_sample'])\n",
    "\n",
    "            max_len = batch_dict['length'].max()\n",
    "            batch_size = len(samples)\n",
    "\n",
    "            src_seq = torch.full((batch_size, max_len), self.pad_index)\n",
    "            src_struct = torch.full((batch_size, max_len), self.pad_index)\n",
    "\n",
    "            if 'pos1id' in batch_dict:\n",
    "                max_pos = max(pos.shape[0] for pos in batch_dict['pos1id'])\n",
    "                pos1id = torch.full((batch_size, max_pos), self.pad_index)\n",
    "                pos2id = torch.full((batch_size, max_pos), self.pad_index)\n",
    "\n",
    "                trg_seq = torch.full((batch_size, max_len), self.ignore_index)\n",
    "                trg_struct = torch.full((batch_size, max_len), self.ignore_index)\n",
    "\n",
    "            src_mat = torch.LongTensor(batch_size, max_len, max_len).fill_(self.pad_index)\n",
    "            trg_mat = torch.LongTensor(batch_size, max_len, max_len).fill_(self.ignore_index)\n",
    "\n",
    "            for b_id, sample in enumerate(samples):\n",
    "                src_seq[b_id, :sample['src_seq'].size(0)] = sample['src_seq']\n",
    "\n",
    "                if 'src_struct' in batch_dict:\n",
    "                    src_struct[b_id, :sample['src_struct'].size(0)] = sample['src_struct']\n",
    "\n",
    "                if 'pos1id' in batch_dict:\n",
    "                    pos1id[b_id, :sample['pos1id'].size(0)] = sample['pos1id']\n",
    "                    pos2id[b_id, :sample['pos2id'].size(0)] = sample['pos2id']\n",
    "                    trg_seq[b_id, :sample['trg_seq'].size(0)] = sample['trg_seq']\n",
    "                    trg_struct[b_id, :sample['trg_struct'].size(0)] = sample['trg_struct']\n",
    "\n",
    "                    src_mat[b_id, :batch_dict['length'][b_id], :batch_dict['length'][b_id]] = 0\n",
    "                    trg_mat[b_id, :batch_dict['length'][b_id], :batch_dict['length'][b_id]] = 0\n",
    "\n",
    "                    src_mat[b_id, sample['pos1id'], sample['pos2id']] = 1\n",
    "                    src_mat[b_id, sample['pos2id'], sample['pos1id']] = 1\n",
    "                    trg_mat[b_id, sample['pos1id'], sample['pos2id']] = 1\n",
    "                    trg_mat[b_id, sample['pos2id'], sample['pos1id']] = 1\n",
    "\n",
    "            batch_dict['src_seq'] = src_seq\n",
    "\n",
    "            if 'src_struct' in batch_dict:\n",
    "                batch_dict['src_struct'] = src_struct\n",
    "\n",
    "            if 'pos1id' in batch_dict:\n",
    "                batch_dict['pos1id'] = pos1id\n",
    "                batch_dict['pos2id'] = pos2id\n",
    "                batch_dict['trg_seq'] = trg_seq\n",
    "                batch_dict['trg_struct'] = trg_struct\n",
    "                batch_dict['src_mat'] = src_mat\n",
    "                batch_dict['trg_mat'] = trg_mat\n",
    "\n",
    "        return batch_dict\n",
    "    \n",
    "def make_pair_mask(src, src_len):\n",
    "     encode_mask = torch.arange(src.shape[1], device=src.device).expand(src.shape[:2]) < src_len.unsqueeze(1)\n",
    "     pair_mask = encode_mask[:, None, :] * encode_mask[:, :, None]\n",
    "     assert isinstance(pair_mask, torch.BoolTensor) or isinstance(pair_mask, torch.cuda.BoolTensor)\n",
    "     return torch.bitwise_not(pair_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = df['sequence'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "PAD_INDEX = 0\n",
    "input_samples = []\n",
    "for seq in sequence:\n",
    "    length = len(seq)\n",
    "    seq_vocab = ['A', 'C', 'G', 'U', 'N']\n",
    "    seq_stoi = dict(zip(seq_vocab, range(len(seq_vocab))))\n",
    "    int_sequence = list(map(seq_stoi.get, seq))\n",
    "    input_sample = torch.LongTensor(int_sequence)\n",
    "    input_sample = {'src_seq': input_sample, 'length': torch.LongTensor([len(input_sample)])[0]}\n",
    "    input_samples.append(input_sample)\n",
    "collator = CollatorRNA(PAD_INDEX, IGNORE_INDEX)\n",
    "batch = collator(input_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'length': tensor([ 88,  71,  73,  93,  66,  62,  86,  85, 151,  76,  94,  69,  75, 160,\n",
       "          72,  69,  82,  53,  62,  71,  77,  47,  65,  76,  72,  52,  63, 137,\n",
       "          62,  90,  80,  60,  71,  90,  72,  90, 112,  71,  66,  71,  70, 166,\n",
       "         159,  77,  72,  67,  50,  61,  55,  52,  52, 114,  71,  68,  52,  56,\n",
       "          66,  84,  69, 141,  71,  79,  77,  64, 165,  56,  84,  60,  71,  93,\n",
       "          92,  67,  68,  93,  85,  75,  60,  65,  56,  72,  74,  55,  90,  92,\n",
       "          97,  74,  55,  48,  78,  68,  75,  68,  51,  56,  69,  52,  73,  92,\n",
       "          76,  57]),\n",
       " 'src_seq': tensor([[3, 2, 0,  ..., 0, 0, 0],\n",
       "         [3, 0, 2,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 2, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [2, 1, 2,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 166, 166])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_mask = make_pair_mask(batch['src_seq'], batch['length'])\n",
    "pair_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 166, 166, 256])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_latent = EmbedSequence2Matrix(CONFIG)(batch['src_seq'])\n",
    "pair_latent.masked_fill_(pair_mask[:, :, :, None], 0.0)\n",
    "pair_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 88, 88, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNAformerStack(CONFIG)(pair_latent, pair_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4216,  1.5679,  3.5777,  1.1813],\n",
      "         [-0.3246, -0.4616, -1.2953, -0.7179]],\n",
      "\n",
      "        [[ 5.0070,  6.7411,  2.1508,  0.5941],\n",
      "         [-0.8725, -3.4541,  1.5236,  0.0857]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input Data\n",
    "data = torch.tensor([\n",
    "    [1, 0, 2, 4],\n",
    "    [2, 8, 5, 6]\n",
    "], dtype=torch.float32).unsqueeze(1)  # Shape: (2, 1, 4)\n",
    "\n",
    "# Masks\n",
    "masks = torch.tensor([\n",
    "    [True, True, False, False],\n",
    "    [True, False, False, False]\n",
    "], dtype=torch.bool)  # Ensure dtype is bool\n",
    "\n",
    "# Apply the mask to zero out values\n",
    "masked_data = torch.where(masks.unsqueeze(1), torch.zeros_like(data), data)  # Shape should still be (2, 1, 4)\n",
    "\n",
    "# Define Conv1d layer\n",
    "conv1d_layer = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, padding=1)\n",
    "\n",
    "# Apply Conv1d\n",
    "output = conv1d_layer(masked_data)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 2., 4.]],\n",
       "\n",
       "        [[0., 8., 5., 6.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
