{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "from rotary_embedding_torch import RotaryEmbedding, apply_rotary_emb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import math\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.data import Data, Batch\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, TransformerWrapper\n",
    "from torch_geometric.nn import GATConv, GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "List = nn.ModuleList\n",
    "\n",
    "# normalizations\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        fn\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, *args,**kwargs)\n",
    "\n",
    "# gated residual\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def forward(self, x, res):\n",
    "        return x + res\n",
    "\n",
    "class GatedResidual(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim * 3, 1, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, res):\n",
    "        gate_input = torch.cat((x, res, x - res), dim = -1)\n",
    "        gate = self.proj(gate_input)\n",
    "        return x * gate + res * (1 - gate)\n",
    "\n",
    "# attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        pos_emb = None,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        edge_dim = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        edge_dim = default(edge_dim, dim)\n",
    "\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.pos_emb = pos_emb\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2)\n",
    "        self.edges_to_kv = nn.Linear(edge_dim, inner_dim)\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, nodes, edges, mask = None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(nodes)\n",
    "        k, v = self.to_kv(nodes).chunk(2, dim = -1)\n",
    "\n",
    "        e_kv = self.edges_to_kv(edges)\n",
    "\n",
    "        q, k, v, e_kv = map(lambda t: rearrange(t, 'b ... (h d) -> (b h) ... d', h = h), (q, k, v, e_kv))\n",
    "\n",
    "        if exists(self.pos_emb):\n",
    "            freqs = self.pos_emb(torch.arange(nodes.shape[1], device = nodes.device))\n",
    "            freqs = rearrange(freqs, 'n d -> () n d')\n",
    "            q = apply_rotary_emb(freqs, q)\n",
    "            k = apply_rotary_emb(freqs, k)\n",
    "\n",
    "        ek, ev = e_kv, e_kv\n",
    "\n",
    "        k, v = map(lambda t: rearrange(t, 'b j d -> b () j d '), (k, v))\n",
    "        k = k + ek\n",
    "        v = v + ev\n",
    "\n",
    "        sim = einsum('b i d, b i j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b i -> b i ()') & rearrange(mask, 'b j -> b () j')\n",
    "            mask = repeat(mask, 'b i j -> (b h) i j', h = h)\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b i j, b i j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# optional feedforward\n",
    "\n",
    "def FeedForward(dim, ff_mult = 4):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, dim * ff_mult),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(dim * ff_mult, dim)\n",
    "    )\n",
    "\n",
    "\n",
    "# classes\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        seq_embed_dim=4,\n",
    "        out_dim = 2,\n",
    "        dim_head = 64,\n",
    "        edge_dim = None,\n",
    "        heads = 8,\n",
    "        gated_residual = True,\n",
    "        with_feedforwards = False,\n",
    "        norm_edges = False,\n",
    "        rel_pos_emb = False,\n",
    "        accept_adjacency_matrix = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = List([])\n",
    "        edge_dim = default(edge_dim, dim)\n",
    "        self.seq_embed_dim = nn.Embedding(seq_embed_dim, dim)\n",
    "        self.norm_edges = nn.LayerNorm(edge_dim) if norm_edges else nn.Identity()\n",
    "\n",
    "        self.adj_emb = nn.Embedding(2, edge_dim) if accept_adjacency_matrix else None\n",
    "\n",
    "        pos_emb = RotaryEmbedding(dim_head) if rel_pos_emb else None\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(List([\n",
    "                List([\n",
    "                    PreNorm(dim, Attention(dim, pos_emb = pos_emb, edge_dim = edge_dim, dim_head = dim_head, heads = heads)),\n",
    "                    GatedResidual(dim)\n",
    "                ]),\n",
    "                List([\n",
    "                    PreNorm(dim, FeedForward(dim)),\n",
    "                    GatedResidual(dim)\n",
    "                ]) if with_feedforwards else None\n",
    "            ]))\n",
    "            \n",
    "        self.to_out = nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):  \n",
    "        \n",
    "        edges = None\n",
    "        mask = x['mask']\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        \n",
    "        mask = mask[:,:Lmax]\n",
    "        nodes = x['seq'][:, :Lmax]\n",
    "        adj_mat = x['adj_matrix'][:, :Lmax, :Lmax]\n",
    "        \n",
    "        \n",
    "        nodes = self.seq_embed_dim(nodes.long())\n",
    "        batch, seq, _ = nodes.shape\n",
    "\n",
    "        if exists(edges):\n",
    "            edges = self.norm_edges(edges)\n",
    "\n",
    "        if exists(adj_mat):\n",
    "            assert adj_mat.shape == (batch, seq, seq)\n",
    "            assert exists(self.adj_emb), 'accept_adjacency_matrix must be set to True'\n",
    "            adj_mat = self.adj_emb(adj_mat.long())\n",
    "\n",
    "        all_edges = default(edges, 0) + default(adj_mat, 0)\n",
    "\n",
    "        for attn_block, ff_block in self.layers:\n",
    "            attn, attn_residual = attn_block\n",
    "            nodes = attn_residual(attn(nodes, all_edges, mask = mask), nodes)\n",
    "\n",
    "            if exists(ff_block):\n",
    "                ff, ff_residual = ff_block\n",
    "                nodes = ff_residual(ff(nodes), nodes)\n",
    "        out = self.to_out(nodes)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def full_attention_conv(qs, ks, vs, kernel, output_attn=False):\n",
    "    '''\n",
    "    qs: query tensor [N, H, M]\n",
    "    ks: key tensor [L, H, M]\n",
    "    vs: value tensor [L, H, D]\n",
    "\n",
    "    return output [N, H, D]\n",
    "    '''\n",
    "    if kernel == 'simple':\n",
    "        # normalize input\n",
    "        qs = qs / torch.norm(qs, p=2) # [N, H, M]\n",
    "        ks = ks / torch.norm(ks, p=2) # [L, H, M]\n",
    "        N = qs.shape[0]\n",
    "\n",
    "        # numerator\n",
    "        kvs = torch.einsum(\"lhm,lhd->hmd\", ks, vs)\n",
    "        attention_num = torch.einsum(\"nhm,hmd->nhd\", qs, kvs) # [N, H, D]\n",
    "        all_ones = torch.ones([vs.shape[0]]).to(vs.device)\n",
    "        vs_sum = torch.einsum(\"l,lhd->hd\", all_ones, vs) # [H, D]\n",
    "        attention_num += vs_sum.unsqueeze(0).repeat(vs.shape[0], 1, 1) # [N, H, D]\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        ks_sum = torch.einsum(\"lhm,l->hm\", ks, all_ones)\n",
    "        attention_normalizer = torch.einsum(\"nhm,hm->nh\", qs, ks_sum)  # [N, H]\n",
    "\n",
    "        # attentive aggregated results\n",
    "        attention_normalizer = torch.unsqueeze(attention_normalizer, len(attention_normalizer.shape))  # [N, H, 1]\n",
    "        attention_normalizer += torch.ones_like(attention_normalizer) * N\n",
    "        attn_output = attention_num / attention_normalizer # [N, H, D]\n",
    "\n",
    "        # compute attention for visualization if needed\n",
    "        if output_attn:\n",
    "            attention = torch.einsum(\"nhm,lhm->nlh\", qs, ks) / attention_normalizer # [N, L, H]\n",
    "\n",
    "    elif kernel == 'sigmoid':\n",
    "        # numerator\n",
    "        attention_num = torch.sigmoid(torch.einsum(\"nhm,lhm->nlh\", qs, ks))  # [N, L, H]\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        attention_normalizer = torch.einsum(\"nlh,l->nh\", attention_num, all_ones)\n",
    "        attention_normalizer = attention_normalizer.unsqueeze(1).repeat(1, ks.shape[0], 1)  # [N, L, H]\n",
    "\n",
    "        # compute attention and attentive aggregated results\n",
    "        attention = attention_num / attention_normalizer\n",
    "        attn_output = torch.einsum(\"nlh,lhd->nhd\", attention, vs)  # [N, H, D]\n",
    "\n",
    "    if output_attn:\n",
    "        return attn_output, attention\n",
    "    else:\n",
    "        return attn_output\n",
    "\n",
    "def gcn_conv(x, edge_index, edge_weight):\n",
    "    N, H = x.shape[0], x.shape[1]\n",
    "    row, col = edge_index\n",
    "    d = degree(col, N).float()\n",
    "    d_norm_in = (1. / d[col]).sqrt()\n",
    "    d_norm_out = (1. / d[row]).sqrt()\n",
    "    gcn_conv_output = []\n",
    "    if edge_weight is None:\n",
    "        value = torch.ones_like(row) * d_norm_in * d_norm_out\n",
    "    else:\n",
    "        value = edge_weight * d_norm_in * d_norm_out\n",
    "    value = torch.nan_to_num(value, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    adj = SparseTensor(row=col, col=row, value=value, sparse_sizes=(N, N))\n",
    "    for i in range(x.shape[1]):\n",
    "        gcn_conv_output.append( matmul(adj, x[:, i]) )  # [N, D]\n",
    "    gcn_conv_output = torch.stack(gcn_conv_output, dim=1) # [N, H, D]\n",
    "    return gcn_conv_output\n",
    "\n",
    "class DIFFormerConv(nn.Module):\n",
    "    '''\n",
    "    one DIFFormer layer\n",
    "    '''\n",
    "    def __init__(self, in_channels,\n",
    "               out_channels,\n",
    "               num_heads,\n",
    "               kernel='simple',\n",
    "               use_graph=True,\n",
    "               use_weight=True):\n",
    "        super(DIFFormerConv, self).__init__()\n",
    "        self.Wk = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        self.Wq = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        if use_weight:\n",
    "            self.Wv = nn.Linear(in_channels, out_channels * num_heads)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.kernel = kernel\n",
    "        self.use_graph = use_graph\n",
    "        self.use_weight = use_weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.Wk.reset_parameters()\n",
    "        self.Wq.reset_parameters()\n",
    "        if self.use_weight:\n",
    "            self.Wv.reset_parameters()\n",
    "\n",
    "    def forward(self, query_input, source_input, edge_index=None, edge_weight=None, output_attn=False):\n",
    "        # feature transformation\n",
    "        query = self.Wq(query_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        key = self.Wk(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        if self.use_weight:\n",
    "            value = self.Wv(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        else:\n",
    "            value = source_input.reshape(-1, 1, self.out_channels)\n",
    "\n",
    "        # compute full attentive aggregation\n",
    "        if output_attn:\n",
    "            attention_output, attn = full_attention_conv(query, key, value, self.kernel, output_attn)  # [N, H, D]\n",
    "        else:\n",
    "            attention_output = full_attention_conv(query,key,value,self.kernel) # [N, H, D]\n",
    "\n",
    "        # use input graph for gcn conv\n",
    "        if self.use_graph:\n",
    "            final_output = attention_output + gcn_conv(value, edge_index, edge_weight)\n",
    "        else:\n",
    "            final_output = attention_output\n",
    "        final_output = final_output.mean(dim=1)\n",
    "\n",
    "        if output_attn:\n",
    "            return final_output, attn\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "class DIFFormer(nn.Module):\n",
    "    '''\n",
    "    DIFFormer model class\n",
    "    x: input node features [N, D]\n",
    "    edge_index: 2-dim indices of edges [2, E]\n",
    "    return y_hat predicted logits [N, C]\n",
    "    '''\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1, kernel='simple',\n",
    "                 alpha=0.5, dropout=0.5, use_bn=True, use_residual=True, use_weight=True, use_graph=True):\n",
    "        super(DIFFormer, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.fcs.append(nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                DIFFormerConv(hidden_channels, hidden_channels, num_heads=num_heads, kernel=kernel, use_graph=use_graph, use_weight=use_weight))\n",
    "            self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "\n",
    "        self.fcs.append(nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = F.relu\n",
    "        self.use_bn = use_bn\n",
    "        self.residual = use_residual\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        for fc in self.fcs:\n",
    "            fc.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        layer_ = []\n",
    "\n",
    "        # input MLP layer\n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # store as residual link\n",
    "        layer_.append(x)\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # graph convolution with DIFFormer layer\n",
    "            x = conv(x, x, edge_index, edge_weight)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1-self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i+1](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            layer_.append(x)\n",
    "\n",
    "        # output MLP layer\n",
    "        x_out = self.fcs[-1](x)\n",
    "        return x_out\n",
    "\n",
    "    def get_attentions(self, x):\n",
    "        layer_, attentions = [], []\n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        layer_.append(x)\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x, attn = conv(x, x, output_attn=True)\n",
    "            attentions.append(attn)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1 - self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i + 1](x)\n",
    "            layer_.append(x)\n",
    "        return torch.stack(attentions, dim=0) # [layer num, N, N]\n",
    "\n",
    "def to_graph_batch(batch):\n",
    "    res = []\n",
    "    seq = batch[\"seq\"]\n",
    "    mask = batch[\"mask\"]\n",
    "    adj_matrix = batch[\"adj_matrix\"]\n",
    "    for i in range(len(seq)):\n",
    "        res.append(Data(x=seq[i][mask[i]], edge_index=adj_matrix[i].nonzero().T))\n",
    "    return Batch.from_data_list(res)\n",
    "\n",
    "class DifformerCustomV0(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.encoder = DIFFormer(in_channels, hidden_channels, out_channels, num_layers=num_layers, num_heads=num_heads)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x  = to_graph_batch(x)\n",
    "        x = self.encoder(x.x, x.edge_index)\n",
    "        x, _ = to_dense_batch(x, x.batch)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"p={}\".format(self.drop_prob)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, scale_base=512, use_xpos=True):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        self.use_xpos = use_xpos\n",
    "        self.scale_base = scale_base\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "        self.register_buffer(\"scale\", scale)\n",
    "\n",
    "    def forward(self, seq_len, device=\"cuda\"):\n",
    "        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i , j -> i j\", t, self.inv_freq)\n",
    "        freqs = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "        if not self.use_xpos:\n",
    "            return freqs, torch.ones(1, device=device)\n",
    "\n",
    "        power = (t - (seq_len // 2)) / self.scale_base\n",
    "        scale = self.scale ** rearrange(power, \"n -> n 1\")\n",
    "        scale = torch.cat((scale, scale), dim=-1)\n",
    "\n",
    "        return freqs, scale\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(pos, t, scale=1.0):\n",
    "    return (t * pos.cos() * scale) + (rotate_half(t) * pos.sin() * scale)\n",
    "\n",
    "\n",
    "class Conv1D(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        if src_key_padding_mask is not None:\n",
    "            self.src_key_padding_mask = src_key_padding_mask\n",
    "        if self.src_key_padding_mask is not None:\n",
    "            x = torch.where(\n",
    "                self.src_key_padding_mask.unsqueeze(-1)\n",
    "                .expand(-1, -1, x.shape[-1])\n",
    "                .bool(),\n",
    "                torch.zeros_like(x),\n",
    "                x,\n",
    "            )\n",
    "        return super().forward(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Sequential):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__(\n",
    "            nn.LayerNorm(d_model), nn.GELU(), Conv1D(d_model, d_model, 3, padding=1)\n",
    "        )\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        self[-1].src_key_padding_mask = (\n",
    "            src_key_padding_mask\n",
    "            if src_key_padding_mask is not None\n",
    "            else self.src_key_padding_mask\n",
    "        )\n",
    "        return x + super().forward(x)\n",
    "\n",
    "\n",
    "class Extractor(nn.Sequential):\n",
    "    def __init__(self, d_model, in_ch=4):\n",
    "        super().__init__(\n",
    "            nn.Embedding(in_ch, d_model // 4),\n",
    "            Conv1D(d_model // 4, d_model, 7, padding=3),\n",
    "            ResBlock(d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        for i in [1, 2]:\n",
    "            self[i].src_key_padding_mask = src_key_padding_mask\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "# BEiTv2 block\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        init_values=None,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        window_size=None,\n",
    "        attn_head_dim=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            dim, num_heads, dropout=drop, batch_first=True\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(\n",
    "                init_values * torch.ones((dim)), requires_grad=True\n",
    "            )\n",
    "            self.gamma_2 = nn.Parameter(\n",
    "                init_values * torch.ones((dim)), requires_grad=True\n",
    "            )\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "        self.emb = RotaryEmbedding(dim)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        q = k = v = self.norm1(x)\n",
    "        positions, scale = self.emb(x.shape[1], x.device)\n",
    "        q = apply_rotary_pos_emb(positions, q, scale)\n",
    "        k = apply_rotary_pos_emb(positions, k, scale**-1)\n",
    "\n",
    "        if self.gamma_1 is None:\n",
    "            x = x + self.drop_path(\n",
    "                self.attn(\n",
    "                    q,\n",
    "                    k,\n",
    "                    v,\n",
    "                    attn_mask=attn_mask,\n",
    "                    key_padding_mask=key_padding_mask,\n",
    "                    need_weights=False,\n",
    "                )[0]\n",
    "            )\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = x + self.drop_path(\n",
    "                self.gamma_1\n",
    "                * self.attn(\n",
    "                    q,\n",
    "                    k,\n",
    "                    v,\n",
    "                    attn_mask=attn_mask,\n",
    "                    key_padding_mask=key_padding_mask,\n",
    "                    need_weights=False,\n",
    "                )[0]\n",
    "            )\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block_conv(Block):\n",
    "    def __init__(self, dim, mlp_ratio, *args, **kwargs):\n",
    "        super().__init__(dim, *args, **kwargs)\n",
    "        self.mlp.fc1 = Conv1D(dim, dim, 3, padding=1)\n",
    "        self.mlp.fc2 = Conv1D(dim, dim, 3, padding=1)\n",
    "\n",
    "    def forward(self, *args, key_padding_mask=None, **kwargs):\n",
    "        self.mlp.fc1.src_key_padding_mask = key_padding_mask\n",
    "        self.mlp.fc2.src_key_padding_mask = key_padding_mask\n",
    "        return super().forward(*args, **kwargs)\n",
    "\n",
    "\n",
    "class RNA_ModelV2(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        # self.emb = nn.Sequential(nn.Embedding(4,dim//4), Conv1D(dim//4,dim,7,padding=3),\n",
    "        #                        nn.LayerNorm(dim), nn.GELU(), Conv1D(dim,dim,3,padding=1))\n",
    "        self.extractor = Extractor(dim)\n",
    "        # self.pos_enc = SinusoidalPosEmb(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # self.transformer = nn.TransformerEncoder(\n",
    "        #    TransformerEncoderLayer_conv(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "        #        dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)\n",
    "        self.proj_out = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerV0(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, attb_heads=8, out=2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(4, dim)\n",
    "        self.dec = ContinuousTransformerWrapper(\n",
    "            dim_in=dim,\n",
    "            dim_out=out,\n",
    "            max_seq_len=512,\n",
    "            attn_layers=Encoder(\n",
    "                dim=dim,\n",
    "                depth=depth,\n",
    "                heads=attb_heads,\n",
    "                attn_flash=True,\n",
    "                rotary_pos_emb=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.emb(x)\n",
    "        out = self.dec(x, mask=mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CustomTransformerV1(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, attn_heads=8, head_size=32):\n",
    "        super().__init__()\n",
    "        self.dec = TransformerWrapper(\n",
    "            num_tokens=4,\n",
    "            logits_dim=2,\n",
    "            max_seq_len=512,\n",
    "            attn_layers=Encoder(\n",
    "                dim=dim,\n",
    "                depth=depth,\n",
    "                heads=attn_heads,\n",
    "                attn_flash=True,\n",
    "                rotary_pos_emb=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        out = self.dec(x, mask=mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        num_layers=2,\n",
    "        dropout=0.5,\n",
    "        use_bn=False,\n",
    "        heads=2,\n",
    "        out_heads=1,\n",
    "    ):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            GATConv(\n",
    "                in_channels, hidden_channels, dropout=dropout, heads=heads, concat=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels * heads))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GATConv(\n",
    "                    hidden_channels * heads,\n",
    "                    hidden_channels,\n",
    "                    dropout=dropout,\n",
    "                    heads=heads,\n",
    "                    concat=True,\n",
    "                )\n",
    "            )\n",
    "            self.bns.append(nn.LayerNorm(hidden_channels * heads))\n",
    "\n",
    "        self.convs.append(\n",
    "            GATConv(\n",
    "                hidden_channels * heads,\n",
    "                out_channels,\n",
    "                dropout=dropout,\n",
    "                heads=out_heads,\n",
    "                concat=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = F.elu\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        res = x\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i](x)\n",
    "            x = self.activation(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        x = res + x\n",
    "        return x\n",
    "\n",
    "\n",
    "def to_graph_batchv1(seq, mask, adj_matrix):\n",
    "    res = []\n",
    "    for i in range(len(seq)):\n",
    "        res.append(Data(x=seq[i][mask[i]], edge_index=adj_matrix[i].nonzero().T))\n",
    "    return Batch.from_data_list(res)\n",
    "\n",
    "\n",
    "class PytorchBatchWrapper(nn.Module):\n",
    "    def __init__(self, md):\n",
    "        super().__init__()\n",
    "        self.md = md\n",
    "\n",
    "    def forward(self, seq, mask, adj_matrix):\n",
    "        batch = to_graph_batchv1(seq, mask, adj_matrix)\n",
    "        out = self.md(batch.x, batch.edge_index)\n",
    "        out, _ = to_dense_batch(out, batch.batch)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RNA_ModelV3(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, graph_layers_every=4, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = Extractor(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.graph_layers_every = graph_layers_every\n",
    "        self.graph_layers = nn.ModuleList(\n",
    "            [\n",
    "                PytorchBatchWrapper(\n",
    "                    GAT(\n",
    "                        in_channels=dim,\n",
    "                        hidden_channels=dim // 2,\n",
    "                        out_channels=dim,\n",
    "                        num_layers=2,\n",
    "                        dropout=0.1,\n",
    "                        use_bn=True,\n",
    "                        heads=4,\n",
    "                        out_heads=1,\n",
    "                    )\n",
    "                )\n",
    "                for i in range(depth)\n",
    "                if i % self.graph_layers_every == 0\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.proj_out = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "\n",
    "        graph_layer_index = 0\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "            if i % self.graph_layers_every == 0:\n",
    "                x = self.graph_layers[graph_layer_index](x, mask, x0[\"adj_matrix\"])\n",
    "                graph_layer_index += 1\n",
    "\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        num_layers=2,\n",
    "        dropout=0.5,\n",
    "        save_mem=True,\n",
    "        use_bn=True,\n",
    "    ):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        # self.convs.append(\n",
    "        #     GCNConv(in_channels, hidden_channels, cached=not save_mem, normalize=not save_mem))\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels, cached=not save_mem))\n",
    "\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, cached=not save_mem)\n",
    "            )\n",
    "            self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "\n",
    "        # self.convs.append(\n",
    "        #     GCNConv(hidden_channels, out_channels, cached=not save_mem, normalize=not save_mem))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels, cached=not save_mem))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = F.gelu\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i](x)\n",
    "            x = self.activation(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "        var = torch.var(x, dim=-1, unbiased=False, keepdim=True)\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        return (x - mean) * (var + eps).rsqrt() * self.g\n",
    "\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gate)\n",
    "\n",
    "\n",
    "class FeedForwardV0(nn.Module):\n",
    "    def __init__(self, dim, out=2, mult=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        inner_dim = int(dim * mult)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim * 2, bias=False),\n",
    "            GEGLU(),\n",
    "            LayerNorm(inner_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, out, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class RNA_ModelV4(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, graph_layers_every=3, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = Extractor(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.graph_layers_every = graph_layers_every\n",
    "        self.graph_layers = PytorchBatchWrapper(\n",
    "            GCN(\n",
    "                dim * (depth // graph_layers_every),\n",
    "                dim,\n",
    "                dim,\n",
    "                num_layers=depth // graph_layers_every,\n",
    "                dropout=0.2,\n",
    "                use_bn=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.proj_out = FeedForwardV0(dim * 2, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "\n",
    "        intermediates = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "            if i % self.graph_layers_every == 0:\n",
    "                intermediates.append(x)\n",
    "        graph = self.graph_layers(\n",
    "            torch.concat(intermediates, dim=-1), mask, x0[\"adj_matrix\"]\n",
    "        )\n",
    "\n",
    "        x = torch.concat([x, graph], dim=-1)\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from rnacomp.dataset import LenMatchBatchSampler, RNA_DatasetBaselineSplitbppV0, DeviceDataLoader\n",
    "\n",
    "# class CFG:\n",
    "#     path = Path(\"../data/\")\n",
    "#     pathbb = Path(\"../data/Ribonanza_bpp_files\")\n",
    "#     split_id = Path('../eda/fold_split.csv')\n",
    "#     bs = 16\n",
    "#     num_workers = 8\n",
    "#     device = 'cpu'\n",
    "#     adjnact_prob = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# fns = list(CFG.pathbb.rglob(\"*.txt\"))\n",
    "# bpp_df = pd.DataFrame({\"bpp\": fns})\n",
    "# bpp_df['sequence_id'] = bpp_df['bpp'].apply(lambda x: x.stem)\n",
    "\n",
    "# df = pd.read_parquet(CFG.path/'train_data.parquet')\n",
    "# split = pd.read_csv(CFG.split_id)\n",
    "# df = pd.merge(df, split, on='sequence_id')\n",
    "# df = pd.merge(df, bpp_df, on='sequence_id')\n",
    "# df_train = df.query('is_train==True').reset_index(drop=True)\n",
    "# df_valid = df.query('is_train==False').reset_index(drop=True)\n",
    "\n",
    "# ds_val = RNA_DatasetBaselineSplitbppV0(df_valid, mode='eval')\n",
    "# ds_val_len = RNA_DatasetBaselineSplitbppV0(df_valid, mode='eval', mask_only=True)\n",
    "# sampler_val = torch.utils.data.SequentialSampler(ds_val_len)\n",
    "# len_sampler_val = LenMatchBatchSampler(sampler_val, batch_size=CFG.bs, \n",
    "#                drop_last=False)\n",
    "# dl_val= DeviceDataLoader(torch.utils.data.DataLoader(ds_val, \n",
    "#                batch_sampler=len_sampler_val, num_workers=CFG.num_workers), CFG.device)\n",
    "\n",
    "# batch = next(iter(dl_val))[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
