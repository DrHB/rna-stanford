{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "from rotary_embedding_torch import RotaryEmbedding, apply_rotary_emb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import math\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "List = nn.ModuleList\n",
    "\n",
    "# normalizations\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        fn\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, *args,**kwargs)\n",
    "\n",
    "# gated residual\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def forward(self, x, res):\n",
    "        return x + res\n",
    "\n",
    "class GatedResidual(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim * 3, 1, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, res):\n",
    "        gate_input = torch.cat((x, res, x - res), dim = -1)\n",
    "        gate = self.proj(gate_input)\n",
    "        return x * gate + res * (1 - gate)\n",
    "\n",
    "# attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        pos_emb = None,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        edge_dim = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        edge_dim = default(edge_dim, dim)\n",
    "\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.pos_emb = pos_emb\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2)\n",
    "        self.edges_to_kv = nn.Linear(edge_dim, inner_dim)\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, nodes, edges, mask = None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(nodes)\n",
    "        k, v = self.to_kv(nodes).chunk(2, dim = -1)\n",
    "\n",
    "        e_kv = self.edges_to_kv(edges)\n",
    "\n",
    "        q, k, v, e_kv = map(lambda t: rearrange(t, 'b ... (h d) -> (b h) ... d', h = h), (q, k, v, e_kv))\n",
    "\n",
    "        if exists(self.pos_emb):\n",
    "            freqs = self.pos_emb(torch.arange(nodes.shape[1], device = nodes.device))\n",
    "            freqs = rearrange(freqs, 'n d -> () n d')\n",
    "            q = apply_rotary_emb(freqs, q)\n",
    "            k = apply_rotary_emb(freqs, k)\n",
    "\n",
    "        ek, ev = e_kv, e_kv\n",
    "\n",
    "        k, v = map(lambda t: rearrange(t, 'b j d -> b () j d '), (k, v))\n",
    "        k = k + ek\n",
    "        v = v + ev\n",
    "\n",
    "        sim = einsum('b i d, b i j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b i -> b i ()') & rearrange(mask, 'b j -> b () j')\n",
    "            mask = repeat(mask, 'b i j -> (b h) i j', h = h)\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b i j, b i j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# optional feedforward\n",
    "\n",
    "def FeedForward(dim, ff_mult = 4):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, dim * ff_mult),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(dim * ff_mult, dim)\n",
    "    )\n",
    "\n",
    "\n",
    "# classes\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        seq_embed_dim=4,\n",
    "        out_dim = 2,\n",
    "        dim_head = 64,\n",
    "        edge_dim = None,\n",
    "        heads = 8,\n",
    "        gated_residual = True,\n",
    "        with_feedforwards = False,\n",
    "        norm_edges = False,\n",
    "        rel_pos_emb = False,\n",
    "        accept_adjacency_matrix = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = List([])\n",
    "        edge_dim = default(edge_dim, dim)\n",
    "        self.seq_embed_dim = nn.Embedding(seq_embed_dim, dim)\n",
    "        self.norm_edges = nn.LayerNorm(edge_dim) if norm_edges else nn.Identity()\n",
    "\n",
    "        self.adj_emb = nn.Embedding(2, edge_dim) if accept_adjacency_matrix else None\n",
    "\n",
    "        pos_emb = RotaryEmbedding(dim_head) if rel_pos_emb else None\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(List([\n",
    "                List([\n",
    "                    PreNorm(dim, Attention(dim, pos_emb = pos_emb, edge_dim = edge_dim, dim_head = dim_head, heads = heads)),\n",
    "                    GatedResidual(dim)\n",
    "                ]),\n",
    "                List([\n",
    "                    PreNorm(dim, FeedForward(dim)),\n",
    "                    GatedResidual(dim)\n",
    "                ]) if with_feedforwards else None\n",
    "            ]))\n",
    "            \n",
    "        self.to_out = nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):  \n",
    "        \n",
    "        edges = None\n",
    "        mask = x['mask']\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        \n",
    "        mask = mask[:,:Lmax]\n",
    "        nodes = x['seq'][:, :Lmax]\n",
    "        adj_mat = x['adj_matrix'][:, :Lmax, :Lmax]\n",
    "        \n",
    "        \n",
    "        nodes = self.seq_embed_dim(nodes.long())\n",
    "        batch, seq, _ = nodes.shape\n",
    "\n",
    "        if exists(edges):\n",
    "            edges = self.norm_edges(edges)\n",
    "\n",
    "        if exists(adj_mat):\n",
    "            assert adj_mat.shape == (batch, seq, seq)\n",
    "            assert exists(self.adj_emb), 'accept_adjacency_matrix must be set to True'\n",
    "            adj_mat = self.adj_emb(adj_mat.long())\n",
    "\n",
    "        all_edges = default(edges, 0) + default(adj_mat, 0)\n",
    "\n",
    "        for attn_block, ff_block in self.layers:\n",
    "            attn, attn_residual = attn_block\n",
    "            nodes = attn_residual(attn(nodes, all_edges, mask = mask), nodes)\n",
    "\n",
    "            if exists(ff_block):\n",
    "                ff, ff_residual = ff_block\n",
    "                nodes = ff_residual(ff(nodes), nodes)\n",
    "        out = self.to_out(nodes)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from x_transformers import TransformerWrapper, Decoder, Encoder\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self,dim=192, depth=12,  head_size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dec = TransformerWrapper(\n",
    "            num_tokens = 4,\n",
    "            max_seq_len = 512,\n",
    "            attn_layers = Decoder(\n",
    "                dim = dim,\n",
    "                depth = depth,\n",
    "                heads = dim//head_size,\n",
    "                attn_flash = True, \n",
    "                rotary_pos_emb = True\n",
    "            )\n",
    "         )\n",
    "        self.proj_out = nn.Linear(dim,2)\n",
    "        \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:,:Lmax]\n",
    "        x = x0['seq'][:,:Lmax]\n",
    "        \n",
    "        x = self.dec(x, mask = mask)\n",
    "        x = self.proj_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
