{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "from rotary_embedding_torch import RotaryEmbedding, apply_rotary_emb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import math\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import to_dense_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "List = nn.ModuleList\n",
    "\n",
    "# normalizations\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        fn\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, *args,**kwargs)\n",
    "\n",
    "# gated residual\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def forward(self, x, res):\n",
    "        return x + res\n",
    "\n",
    "class GatedResidual(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim * 3, 1, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, res):\n",
    "        gate_input = torch.cat((x, res, x - res), dim = -1)\n",
    "        gate = self.proj(gate_input)\n",
    "        return x * gate + res * (1 - gate)\n",
    "\n",
    "# attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        pos_emb = None,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        edge_dim = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        edge_dim = default(edge_dim, dim)\n",
    "\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.pos_emb = pos_emb\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2)\n",
    "        self.edges_to_kv = nn.Linear(edge_dim, inner_dim)\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, nodes, edges, mask = None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(nodes)\n",
    "        k, v = self.to_kv(nodes).chunk(2, dim = -1)\n",
    "\n",
    "        e_kv = self.edges_to_kv(edges)\n",
    "\n",
    "        q, k, v, e_kv = map(lambda t: rearrange(t, 'b ... (h d) -> (b h) ... d', h = h), (q, k, v, e_kv))\n",
    "\n",
    "        if exists(self.pos_emb):\n",
    "            freqs = self.pos_emb(torch.arange(nodes.shape[1], device = nodes.device))\n",
    "            freqs = rearrange(freqs, 'n d -> () n d')\n",
    "            q = apply_rotary_emb(freqs, q)\n",
    "            k = apply_rotary_emb(freqs, k)\n",
    "\n",
    "        ek, ev = e_kv, e_kv\n",
    "\n",
    "        k, v = map(lambda t: rearrange(t, 'b j d -> b () j d '), (k, v))\n",
    "        k = k + ek\n",
    "        v = v + ev\n",
    "\n",
    "        sim = einsum('b i d, b i j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b i -> b i ()') & rearrange(mask, 'b j -> b () j')\n",
    "            mask = repeat(mask, 'b i j -> (b h) i j', h = h)\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b i j, b i j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# optional feedforward\n",
    "\n",
    "def FeedForward(dim, ff_mult = 4):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, dim * ff_mult),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(dim * ff_mult, dim)\n",
    "    )\n",
    "\n",
    "\n",
    "# classes\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        seq_embed_dim=4,\n",
    "        out_dim = 2,\n",
    "        dim_head = 64,\n",
    "        edge_dim = None,\n",
    "        heads = 8,\n",
    "        gated_residual = True,\n",
    "        with_feedforwards = False,\n",
    "        norm_edges = False,\n",
    "        rel_pos_emb = False,\n",
    "        accept_adjacency_matrix = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = List([])\n",
    "        edge_dim = default(edge_dim, dim)\n",
    "        self.seq_embed_dim = nn.Embedding(seq_embed_dim, dim)\n",
    "        self.norm_edges = nn.LayerNorm(edge_dim) if norm_edges else nn.Identity()\n",
    "\n",
    "        self.adj_emb = nn.Embedding(2, edge_dim) if accept_adjacency_matrix else None\n",
    "\n",
    "        pos_emb = RotaryEmbedding(dim_head) if rel_pos_emb else None\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(List([\n",
    "                List([\n",
    "                    PreNorm(dim, Attention(dim, pos_emb = pos_emb, edge_dim = edge_dim, dim_head = dim_head, heads = heads)),\n",
    "                    GatedResidual(dim)\n",
    "                ]),\n",
    "                List([\n",
    "                    PreNorm(dim, FeedForward(dim)),\n",
    "                    GatedResidual(dim)\n",
    "                ]) if with_feedforwards else None\n",
    "            ]))\n",
    "            \n",
    "        self.to_out = nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):  \n",
    "        \n",
    "        edges = None\n",
    "        mask = x['mask']\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        \n",
    "        mask = mask[:,:Lmax]\n",
    "        nodes = x['seq'][:, :Lmax]\n",
    "        adj_mat = x['adj_matrix'][:, :Lmax, :Lmax]\n",
    "        \n",
    "        \n",
    "        nodes = self.seq_embed_dim(nodes.long())\n",
    "        batch, seq, _ = nodes.shape\n",
    "\n",
    "        if exists(edges):\n",
    "            edges = self.norm_edges(edges)\n",
    "\n",
    "        if exists(adj_mat):\n",
    "            assert adj_mat.shape == (batch, seq, seq)\n",
    "            assert exists(self.adj_emb), 'accept_adjacency_matrix must be set to True'\n",
    "            adj_mat = self.adj_emb(adj_mat.long())\n",
    "\n",
    "        all_edges = default(edges, 0) + default(adj_mat, 0)\n",
    "\n",
    "        for attn_block, ff_block in self.layers:\n",
    "            attn, attn_residual = attn_block\n",
    "            nodes = attn_residual(attn(nodes, all_edges, mask = mask), nodes)\n",
    "\n",
    "            if exists(ff_block):\n",
    "                ff, ff_residual = ff_block\n",
    "                nodes = ff_residual(ff(nodes), nodes)\n",
    "        out = self.to_out(nodes)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        # x = self.drop(x)\n",
    "        # commit this for the orignal BERT implement \n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "#BEiTv2 block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        if self.gamma_1 is None:\n",
    "            xn = self.norm1(x)\n",
    "            x = x + self.drop_path(self.attn(xn,xn,xn,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            xn = self.norm1(x)\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(xn,xn,xn,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class RnaModelV0(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, use_checkpoint=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(4,dim)\n",
    "        self.blocks = nn.ModuleList([ \n",
    "            Block(\n",
    "                dim=dim, num_heads=dim//32, mlp_ratio=4, drop_path=0.0*(i/(depth-1)), init_values=1,)\n",
    "            for i in range(depth)])\n",
    "        self.pos_enc = SinusoidalPosEmb(dim)\n",
    "        self.proj_out = nn.Linear(dim,2)\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "        self.apply(_init_weights)\n",
    "        \n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token'}\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        B,_ = mask.shape\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = torch.cat([torch.ones(B,1,dtype=mask.dtype, device=mask.device),mask],1)\n",
    "        attn_mask = torch.zeros(mask.shape, device=mask.device)\n",
    "        attn_mask[~mask] = -torch.inf\n",
    "        x = x0['seq'][:,:Lmax]\n",
    "        mask,attn_mask = mask[:,:Lmax], attn_mask[:,:Lmax]\n",
    "        \n",
    "        pos = torch.arange(Lmax, device=x.device).unsqueeze(0)\n",
    "        pos = self.pos_enc(pos)\n",
    "        x = self.emb(x)\n",
    "        x = x + pos\n",
    "        \n",
    "        #print(Lmax)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, None, attn_mask)\n",
    "            else: x = blk(x, None, attn_mask)\n",
    "                \n",
    "        x = self.proj_out(x) #cls token\n",
    "        return x\n",
    "        \n",
    "    def get_layer_groups(self):\n",
    "        def flatten_model(m):\n",
    "            return [m] if not hasattr(m,'children') or len(list(m.children())) == 0 else \\\n",
    "                sum(map(flatten_model, list(m.children())), [])\n",
    "        return [flatten_model(self)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def full_attention_conv(qs, ks, vs, kernel, output_attn=False):\n",
    "    '''\n",
    "    qs: query tensor [N, H, M]\n",
    "    ks: key tensor [L, H, M]\n",
    "    vs: value tensor [L, H, D]\n",
    "\n",
    "    return output [N, H, D]\n",
    "    '''\n",
    "    if kernel == 'simple':\n",
    "        # normalize input\n",
    "        qs = qs / torch.norm(qs, p=2) # [N, H, M]\n",
    "        ks = ks / torch.norm(ks, p=2) # [L, H, M]\n",
    "        N = qs.shape[0]\n",
    "\n",
    "        # numerator\n",
    "        kvs = torch.einsum(\"lhm,lhd->hmd\", ks, vs)\n",
    "        attention_num = torch.einsum(\"nhm,hmd->nhd\", qs, kvs) # [N, H, D]\n",
    "        all_ones = torch.ones([vs.shape[0]]).to(vs.device)\n",
    "        vs_sum = torch.einsum(\"l,lhd->hd\", all_ones, vs) # [H, D]\n",
    "        attention_num += vs_sum.unsqueeze(0).repeat(vs.shape[0], 1, 1) # [N, H, D]\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        ks_sum = torch.einsum(\"lhm,l->hm\", ks, all_ones)\n",
    "        attention_normalizer = torch.einsum(\"nhm,hm->nh\", qs, ks_sum)  # [N, H]\n",
    "\n",
    "        # attentive aggregated results\n",
    "        attention_normalizer = torch.unsqueeze(attention_normalizer, len(attention_normalizer.shape))  # [N, H, 1]\n",
    "        attention_normalizer += torch.ones_like(attention_normalizer) * N\n",
    "        attn_output = attention_num / attention_normalizer # [N, H, D]\n",
    "\n",
    "        # compute attention for visualization if needed\n",
    "        if output_attn:\n",
    "            attention = torch.einsum(\"nhm,lhm->nlh\", qs, ks) / attention_normalizer # [N, L, H]\n",
    "\n",
    "    elif kernel == 'sigmoid':\n",
    "        # numerator\n",
    "        attention_num = torch.sigmoid(torch.einsum(\"nhm,lhm->nlh\", qs, ks))  # [N, L, H]\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        attention_normalizer = torch.einsum(\"nlh,l->nh\", attention_num, all_ones)\n",
    "        attention_normalizer = attention_normalizer.unsqueeze(1).repeat(1, ks.shape[0], 1)  # [N, L, H]\n",
    "\n",
    "        # compute attention and attentive aggregated results\n",
    "        attention = attention_num / attention_normalizer\n",
    "        attn_output = torch.einsum(\"nlh,lhd->nhd\", attention, vs)  # [N, H, D]\n",
    "\n",
    "    if output_attn:\n",
    "        return attn_output, attention\n",
    "    else:\n",
    "        return attn_output\n",
    "\n",
    "def gcn_conv(x, edge_index, edge_weight):\n",
    "    N, H = x.shape[0], x.shape[1]\n",
    "    row, col = edge_index\n",
    "    d = degree(col, N).float()\n",
    "    d_norm_in = (1. / d[col]).sqrt()\n",
    "    d_norm_out = (1. / d[row]).sqrt()\n",
    "    gcn_conv_output = []\n",
    "    if edge_weight is None:\n",
    "        value = torch.ones_like(row) * d_norm_in * d_norm_out\n",
    "    else:\n",
    "        value = edge_weight * d_norm_in * d_norm_out\n",
    "    value = torch.nan_to_num(value, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    adj = SparseTensor(row=col, col=row, value=value, sparse_sizes=(N, N))\n",
    "    for i in range(x.shape[1]):\n",
    "        gcn_conv_output.append( matmul(adj, x[:, i]) )  # [N, D]\n",
    "    gcn_conv_output = torch.stack(gcn_conv_output, dim=1) # [N, H, D]\n",
    "    return gcn_conv_output\n",
    "\n",
    "class DIFFormerConv(nn.Module):\n",
    "    '''\n",
    "    one DIFFormer layer\n",
    "    '''\n",
    "    def __init__(self, in_channels,\n",
    "               out_channels,\n",
    "               num_heads,\n",
    "               kernel='simple',\n",
    "               use_graph=True,\n",
    "               use_weight=True):\n",
    "        super(DIFFormerConv, self).__init__()\n",
    "        self.Wk = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        self.Wq = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        if use_weight:\n",
    "            self.Wv = nn.Linear(in_channels, out_channels * num_heads)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.kernel = kernel\n",
    "        self.use_graph = use_graph\n",
    "        self.use_weight = use_weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.Wk.reset_parameters()\n",
    "        self.Wq.reset_parameters()\n",
    "        if self.use_weight:\n",
    "            self.Wv.reset_parameters()\n",
    "\n",
    "    def forward(self, query_input, source_input, edge_index=None, edge_weight=None, output_attn=False):\n",
    "        # feature transformation\n",
    "        query = self.Wq(query_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        key = self.Wk(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        if self.use_weight:\n",
    "            value = self.Wv(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        else:\n",
    "            value = source_input.reshape(-1, 1, self.out_channels)\n",
    "\n",
    "        # compute full attentive aggregation\n",
    "        if output_attn:\n",
    "            attention_output, attn = full_attention_conv(query, key, value, self.kernel, output_attn)  # [N, H, D]\n",
    "        else:\n",
    "            attention_output = full_attention_conv(query,key,value,self.kernel) # [N, H, D]\n",
    "\n",
    "        # use input graph for gcn conv\n",
    "        if self.use_graph:\n",
    "            final_output = attention_output + gcn_conv(value, edge_index, edge_weight)\n",
    "        else:\n",
    "            final_output = attention_output\n",
    "        final_output = final_output.mean(dim=1)\n",
    "\n",
    "        if output_attn:\n",
    "            return final_output, attn\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "class DIFFormer(nn.Module):\n",
    "    '''\n",
    "    DIFFormer model class\n",
    "    x: input node features [N, D]\n",
    "    edge_index: 2-dim indices of edges [2, E]\n",
    "    return y_hat predicted logits [N, C]\n",
    "    '''\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1, kernel='simple',\n",
    "                 alpha=0.5, dropout=0.5, use_bn=True, use_residual=True, use_weight=True, use_graph=True):\n",
    "        super(DIFFormer, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.fcs.append(nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                DIFFormerConv(hidden_channels, hidden_channels, num_heads=num_heads, kernel=kernel, use_graph=use_graph, use_weight=use_weight))\n",
    "            self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "\n",
    "        self.fcs.append(nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = F.relu\n",
    "        self.use_bn = use_bn\n",
    "        self.residual = use_residual\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        for fc in self.fcs:\n",
    "            fc.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        layer_ = []\n",
    "\n",
    "        # input MLP layer\n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # store as residual link\n",
    "        layer_.append(x)\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # graph convolution with DIFFormer layer\n",
    "            x = conv(x, x, edge_index, edge_weight)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1-self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i+1](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            layer_.append(x)\n",
    "\n",
    "        # output MLP layer\n",
    "        x_out = self.fcs[-1](x)\n",
    "        return x_out\n",
    "\n",
    "    def get_attentions(self, x):\n",
    "        layer_, attentions = [], []\n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        layer_.append(x)\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x, attn = conv(x, x, output_attn=True)\n",
    "            attentions.append(attn)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1 - self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i + 1](x)\n",
    "            layer_.append(x)\n",
    "        return torch.stack(attentions, dim=0) # [layer num, N, N]\n",
    "\n",
    "def to_graph_batch(batch):\n",
    "    res = []\n",
    "    seq = batch[\"seq\"]\n",
    "    mask = batch[\"mask\"]\n",
    "    adj_matrix = batch[\"adj_matrix\"]\n",
    "    for i in range(len(seq)):\n",
    "        res.append(Data(x=seq[i][mask[i]], edge_index=adj_matrix[i].nonzero().T))\n",
    "    return Batch.from_data_list(res)\n",
    "\n",
    "class DifformerCustomV0(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.encoder = DIFFormer(in_channels, hidden_channels, out_channels, num_layers=num_layers, num_heads=num_heads)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x  = to_graph_batch(x)\n",
    "        x = self.encoder(x.x, x.edge_index)\n",
    "        x, _ = to_dense_batch(x, x.batch)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RnaModelV0(\n",
       "  (emb): Embedding(4, 192)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pos_enc): SinusoidalPosEmb()\n",
       "  (proj_out): Linear(in_features=192, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RnaModelV0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from x_transformers import TransformerWrapper, Decoder, Encoder\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self,dim=192, depth=12,  head_size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dec = TransformerWrapper(\n",
    "            num_tokens = 4,\n",
    "            max_seq_len = 512,\n",
    "            attn_layers = Decoder(\n",
    "                dim = dim,\n",
    "                depth = depth,\n",
    "                heads = dim//head_size,\n",
    "                attn_flash = True, \n",
    "                rotary_pos_emb = True\n",
    "            )\n",
    "         )\n",
    "        self.proj_out = nn.Linear(dim,2)\n",
    "        \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:,:Lmax]\n",
    "        x = x0['seq'][:,:Lmax]\n",
    "        \n",
    "        x = self.dec(x, mask = mask)\n",
    "        x = self.proj_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
