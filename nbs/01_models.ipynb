{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "from rotary_embedding_torch import RotaryEmbedding, apply_rotary_emb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import math\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.data import Data, Batch\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, TransformerWrapper\n",
    "from torch_geometric.nn import GATConv, GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "List = nn.ModuleList\n",
    "\n",
    "# normalizations\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        fn\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, *args,**kwargs)\n",
    "\n",
    "# gated residual\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def forward(self, x, res):\n",
    "        return x + res\n",
    "\n",
    "class GatedResidual(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim * 3, 1, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, res):\n",
    "        gate_input = torch.cat((x, res, x - res), dim = -1)\n",
    "        gate = self.proj(gate_input)\n",
    "        return x * gate + res * (1 - gate)\n",
    "\n",
    "# attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        pos_emb = None,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        edge_dim = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        edge_dim = default(edge_dim, dim)\n",
    "\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.pos_emb = pos_emb\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2)\n",
    "        self.edges_to_kv = nn.Linear(edge_dim, inner_dim)\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, nodes, edges, mask = None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(nodes)\n",
    "        k, v = self.to_kv(nodes).chunk(2, dim = -1)\n",
    "\n",
    "        e_kv = self.edges_to_kv(edges)\n",
    "\n",
    "        q, k, v, e_kv = map(lambda t: rearrange(t, 'b ... (h d) -> (b h) ... d', h = h), (q, k, v, e_kv))\n",
    "\n",
    "        if exists(self.pos_emb):\n",
    "            freqs = self.pos_emb(torch.arange(nodes.shape[1], device = nodes.device))\n",
    "            freqs = rearrange(freqs, 'n d -> () n d')\n",
    "            q = apply_rotary_emb(freqs, q)\n",
    "            k = apply_rotary_emb(freqs, k)\n",
    "\n",
    "        ek, ev = e_kv, e_kv\n",
    "\n",
    "        k, v = map(lambda t: rearrange(t, 'b j d -> b () j d '), (k, v))\n",
    "        k = k + ek\n",
    "        v = v + ev\n",
    "\n",
    "        sim = einsum('b i d, b i j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b i -> b i ()') & rearrange(mask, 'b j -> b () j')\n",
    "            mask = repeat(mask, 'b i j -> (b h) i j', h = h)\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b i j, b i j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# optional feedforward\n",
    "\n",
    "def FeedForward(dim, ff_mult = 4):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, dim * ff_mult),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(dim * ff_mult, dim)\n",
    "    )\n",
    "\n",
    "\n",
    "# classes\n",
    "\n",
    "class GraphTransformerAdjacent(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        edge_dim = 16,\n",
    "        dim_head = 32,\n",
    "        heads = 8,\n",
    "        with_feedforwards = False,\n",
    "        rel_pos_emb = False,\n",
    "        accept_adjacency_matrix = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = List([])\n",
    "        edge_dim = default(edge_dim, dim)\n",
    "\n",
    "\n",
    "        pos_emb = RotaryEmbedding(dim_head) if rel_pos_emb else None\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(List([\n",
    "                List([\n",
    "                    PreNorm(dim, Attention(dim, pos_emb = pos_emb, edge_dim = edge_dim, dim_head = dim_head, heads = heads)),\n",
    "                    GatedResidual(dim)\n",
    "                ]),\n",
    "                List([\n",
    "                    PreNorm(dim, FeedForward(dim)),\n",
    "                    GatedResidual(dim)\n",
    "                ]) if with_feedforwards else None\n",
    "            ]))\n",
    "            \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        nodes,\n",
    "        mask, \n",
    "        adj_mat,\n",
    "    ):  \n",
    "        \n",
    "        edges = None\n",
    "        batch, seq, _ = nodes.shape\n",
    "        all_edges = default(edges, 0) + default(adj_mat, 0)\n",
    "\n",
    "        for attn_block, ff_block in self.layers:\n",
    "            attn, attn_residual = attn_block\n",
    "            nodes = attn_residual(attn(nodes, all_edges, mask = mask), nodes)\n",
    "\n",
    "            if exists(ff_block):\n",
    "                ff, ff_residual = ff_block\n",
    "                nodes = ff_residual(ff(nodes), nodes)\n",
    "        return nodes\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def full_attention_conv(qs, ks, vs, kernel, output_attn=False):\n",
    "    '''\n",
    "    qs: query tensor [N, H, M]\n",
    "    ks: key tensor [L, H, M]\n",
    "    vs: value tensor [L, H, D]\n",
    "\n",
    "    return output [N, H, D]\n",
    "    '''\n",
    "    if kernel == 'simple':\n",
    "        # normalize input\n",
    "        qs = qs / torch.norm(qs, p=2) # [N, H, M]\n",
    "        ks = ks / torch.norm(ks, p=2) # [L, H, M]\n",
    "        N = qs.shape[0]\n",
    "\n",
    "        # numerator\n",
    "        kvs = torch.einsum(\"lhm,lhd->hmd\", ks, vs)\n",
    "        attention_num = torch.einsum(\"nhm,hmd->nhd\", qs, kvs) # [N, H, D]\n",
    "        all_ones = torch.ones([vs.shape[0]]).to(vs.device)\n",
    "        vs_sum = torch.einsum(\"l,lhd->hd\", all_ones, vs) # [H, D]\n",
    "        attention_num += vs_sum.unsqueeze(0).repeat(vs.shape[0], 1, 1) # [N, H, D]\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        ks_sum = torch.einsum(\"lhm,l->hm\", ks, all_ones)\n",
    "        attention_normalizer = torch.einsum(\"nhm,hm->nh\", qs, ks_sum)  # [N, H]\n",
    "\n",
    "        # attentive aggregated results\n",
    "        attention_normalizer = torch.unsqueeze(attention_normalizer, len(attention_normalizer.shape))  # [N, H, 1]\n",
    "        attention_normalizer += torch.ones_like(attention_normalizer) * N\n",
    "        attn_output = attention_num / attention_normalizer # [N, H, D]\n",
    "\n",
    "        # compute attention for visualization if needed\n",
    "        if output_attn:\n",
    "            attention = torch.einsum(\"nhm,lhm->nlh\", qs, ks) / attention_normalizer # [N, L, H]\n",
    "\n",
    "    elif kernel == 'sigmoid':\n",
    "        # numerator\n",
    "        attention_num = torch.sigmoid(torch.einsum(\"nhm,lhm->nlh\", qs, ks))  # [N, L, H]\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        attention_normalizer = torch.einsum(\"nlh,l->nh\", attention_num, all_ones)\n",
    "        attention_normalizer = attention_normalizer.unsqueeze(1).repeat(1, ks.shape[0], 1)  # [N, L, H]\n",
    "\n",
    "        # compute attention and attentive aggregated results\n",
    "        attention = attention_num / attention_normalizer\n",
    "        attn_output = torch.einsum(\"nlh,lhd->nhd\", attention, vs)  # [N, H, D]\n",
    "\n",
    "    if output_attn:\n",
    "        return attn_output, attention\n",
    "    else:\n",
    "        return attn_output\n",
    "\n",
    "def gcn_conv(x, edge_index, edge_weight):\n",
    "    N, H = x.shape[0], x.shape[1]\n",
    "    row, col = edge_index\n",
    "    d = degree(col, N).float()\n",
    "    d_norm_in = (1. / d[col]).sqrt()\n",
    "    d_norm_out = (1. / d[row]).sqrt()\n",
    "    gcn_conv_output = []\n",
    "    if edge_weight is None:\n",
    "        value = torch.ones_like(row) * d_norm_in * d_norm_out\n",
    "    else:\n",
    "        value = edge_weight * d_norm_in * d_norm_out\n",
    "    value = torch.nan_to_num(value, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    adj = SparseTensor(row=col, col=row, value=value, sparse_sizes=(N, N))\n",
    "    for i in range(x.shape[1]):\n",
    "        gcn_conv_output.append( matmul(adj, x[:, i]) )  # [N, D]\n",
    "    gcn_conv_output = torch.stack(gcn_conv_output, dim=1) # [N, H, D]\n",
    "    return gcn_conv_output\n",
    "\n",
    "class DIFFormerConv(nn.Module):\n",
    "    '''\n",
    "    one DIFFormer layer\n",
    "    '''\n",
    "    def __init__(self, in_channels,\n",
    "               out_channels,\n",
    "               num_heads,\n",
    "               kernel='simple',\n",
    "               use_graph=True,\n",
    "               use_weight=True):\n",
    "        super(DIFFormerConv, self).__init__()\n",
    "        self.Wk = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        self.Wq = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        if use_weight:\n",
    "            self.Wv = nn.Linear(in_channels, out_channels * num_heads)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.kernel = kernel\n",
    "        self.use_graph = use_graph\n",
    "        self.use_weight = use_weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.Wk.reset_parameters()\n",
    "        self.Wq.reset_parameters()\n",
    "        if self.use_weight:\n",
    "            self.Wv.reset_parameters()\n",
    "\n",
    "    def forward(self, query_input, source_input, edge_index=None, edge_weight=None, output_attn=False):\n",
    "        # feature transformation\n",
    "        query = self.Wq(query_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        key = self.Wk(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        if self.use_weight:\n",
    "            value = self.Wv(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        else:\n",
    "            value = source_input.reshape(-1, 1, self.out_channels)\n",
    "\n",
    "        # compute full attentive aggregation\n",
    "        if output_attn:\n",
    "            attention_output, attn = full_attention_conv(query, key, value, self.kernel, output_attn)  # [N, H, D]\n",
    "        else:\n",
    "            attention_output = full_attention_conv(query,key,value,self.kernel) # [N, H, D]\n",
    "\n",
    "        # use input graph for gcn conv\n",
    "        if self.use_graph:\n",
    "            final_output = attention_output + gcn_conv(value, edge_index, edge_weight)\n",
    "        else:\n",
    "            final_output = attention_output\n",
    "        final_output = final_output.mean(dim=1)\n",
    "\n",
    "        if output_attn:\n",
    "            return final_output, attn\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "class DIFFormer(nn.Module):\n",
    "    '''\n",
    "    DIFFormer model class\n",
    "    x: input node features [N, D]\n",
    "    edge_index: 2-dim indices of edges [2, E]\n",
    "    return y_hat predicted logits [N, C]\n",
    "    '''\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers=2, num_heads=1, kernel='simple',\n",
    "                 alpha=0.5, dropout=0.5, use_bn=True, use_residual=True, use_weight=True, use_graph=True):\n",
    "        super(DIFFormer, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.fcs.append(nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                DIFFormerConv(hidden_channels, hidden_channels, num_heads=num_heads, kernel=kernel, use_graph=use_graph, use_weight=use_weight))\n",
    "            self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = F.relu\n",
    "        self.use_bn = use_bn\n",
    "        self.residual = use_residual\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        for fc in self.fcs:\n",
    "            fc.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        layer_ = []\n",
    "\n",
    "        # input MLP layer\n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # store as residual link\n",
    "        layer_.append(x)\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # graph convolution with DIFFormer layer\n",
    "            x = conv(x, x, edge_index, edge_weight)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1-self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i+1](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            layer_.append(x)\n",
    "\n",
    "        # output MLP layer\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_attentions(self, x):\n",
    "        layer_, attentions = [], []\n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        layer_.append(x)\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x, attn = conv(x, x, output_attn=True)\n",
    "            attentions.append(attn)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1 - self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i + 1](x)\n",
    "            layer_.append(x)\n",
    "        return torch.stack(attentions, dim=0) # [layer num, N, N]\n",
    "\n",
    "def to_graph_batch(batch):\n",
    "    res = []\n",
    "    seq = batch[\"seq\"]\n",
    "    mask = batch[\"mask\"]\n",
    "    adj_matrix = batch[\"adj_matrix\"]\n",
    "    for i in range(len(seq)):\n",
    "        res.append(Data(x=seq[i][mask[i]], edge_index=adj_matrix[i].nonzero().T))\n",
    "    return Batch.from_data_list(res)\n",
    "\n",
    "class DifformerCustomV0(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.encoder = DIFFormer(in_channels, hidden_channels, out_channels, num_layers=num_layers, num_heads=num_heads)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x  = to_graph_batch(x)\n",
    "        x = self.encoder(x.x, x.edge_index)\n",
    "        x, _ = to_dense_batch(x, x.batch)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"p={}\".format(self.drop_prob)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, scale_base=512, use_xpos=True):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        self.use_xpos = use_xpos\n",
    "        self.scale_base = scale_base\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "        self.register_buffer(\"scale\", scale)\n",
    "\n",
    "    def forward(self, seq_len, device=\"cuda\"):\n",
    "        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i , j -> i j\", t, self.inv_freq)\n",
    "        freqs = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "        if not self.use_xpos:\n",
    "            return freqs, torch.ones(1, device=device)\n",
    "\n",
    "        power = (t - (seq_len // 2)) / self.scale_base\n",
    "        scale = self.scale ** rearrange(power, \"n -> n 1\")\n",
    "        scale = torch.cat((scale, scale), dim=-1)\n",
    "\n",
    "        return freqs, scale\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(pos, t, scale=1.0):\n",
    "    return (t * pos.cos() * scale) + (rotate_half(t) * pos.sin() * scale)\n",
    "\n",
    "\n",
    "class Conv1D(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        if src_key_padding_mask is not None:\n",
    "            self.src_key_padding_mask = src_key_padding_mask\n",
    "        if self.src_key_padding_mask is not None:\n",
    "            x = torch.where(\n",
    "                self.src_key_padding_mask.unsqueeze(-1)\n",
    "                .expand(-1, -1, x.shape[-1])\n",
    "                .bool(),\n",
    "                torch.zeros_like(x),\n",
    "                x,\n",
    "            )\n",
    "        return super().forward(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Sequential):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__(\n",
    "            nn.LayerNorm(d_model), nn.GELU(), Conv1D(d_model, d_model, 3, padding=1)\n",
    "        )\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        self[-1].src_key_padding_mask = (\n",
    "            src_key_padding_mask\n",
    "            if src_key_padding_mask is not None\n",
    "            else self.src_key_padding_mask\n",
    "        )\n",
    "        return x + super().forward(x)\n",
    "\n",
    "\n",
    "class Extractor(nn.Sequential):\n",
    "    def __init__(self, d_model, in_ch=4):\n",
    "        super().__init__(\n",
    "            nn.Embedding(in_ch, d_model // 4),\n",
    "            Conv1D(d_model // 4, d_model, 7, padding=3),\n",
    "            ResBlock(d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        for i in [1, 2]:\n",
    "            self[i].src_key_padding_mask = src_key_padding_mask\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class ExtractorV0(nn.Sequential):\n",
    "    def __init__(self, d_model, in_ch=4):\n",
    "        super().__init__()\n",
    "        self.seq_embed = nn.Sequential(\n",
    "            nn.Embedding(in_ch, d_model // 4),\n",
    "            Conv1D(d_model // 4, d_model // 2, 7, padding=3),\n",
    "        )\n",
    "        self.ss_embed = nn.Sequential(\n",
    "            nn.Embedding(3, d_model // 4),\n",
    "            Conv1D(d_model // 4, d_model // 2, 7, padding=3),\n",
    "        )\n",
    "\n",
    "        self.out = ResBlock(d_model)\n",
    "\n",
    "    def forward(self, seq, ss, src_key_padding_mask=None):\n",
    "        for i in [1, 2]:\n",
    "            self[i].src_key_padding_mask = src_key_padding_mask\n",
    "\n",
    "        seq = self.seq_embed(seq)\n",
    "        ss = self.ss_embed(ss)\n",
    "        x = torch.concat([seq, ss], dim=-1)\n",
    "        out = self.out(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# BEiTv2 block\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        init_values=None,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        window_size=None,\n",
    "        attn_head_dim=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            dim, num_heads, dropout=drop, batch_first=True\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(\n",
    "                init_values * torch.ones((dim)), requires_grad=True\n",
    "            )\n",
    "            self.gamma_2 = nn.Parameter(\n",
    "                init_values * torch.ones((dim)), requires_grad=True\n",
    "            )\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "        self.emb = RotaryEmbedding(dim)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        q = k = v = self.norm1(x)\n",
    "        positions, scale = self.emb(x.shape[1], x.device)\n",
    "        q = apply_rotary_pos_emb(positions, q, scale)\n",
    "        k = apply_rotary_pos_emb(positions, k, scale**-1)\n",
    "\n",
    "        if self.gamma_1 is None:\n",
    "            x = x + self.drop_path(\n",
    "                self.attn(\n",
    "                    q,\n",
    "                    k,\n",
    "                    v,\n",
    "                    attn_mask=attn_mask,\n",
    "                    key_padding_mask=key_padding_mask,\n",
    "                    need_weights=False,\n",
    "                )[0]\n",
    "            )\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = x + self.drop_path(\n",
    "                self.gamma_1\n",
    "                * self.attn(\n",
    "                    q,\n",
    "                    k,\n",
    "                    v,\n",
    "                    attn_mask=attn_mask,\n",
    "                    key_padding_mask=key_padding_mask,\n",
    "                    need_weights=False,\n",
    "                )[0]\n",
    "            )\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block_conv(Block):\n",
    "    def __init__(self, dim, mlp_ratio, *args, **kwargs):\n",
    "        super().__init__(dim, *args, **kwargs)\n",
    "        self.mlp.fc1 = Conv1D(dim, dim, 3, padding=1)\n",
    "        self.mlp.fc2 = Conv1D(dim, dim, 3, padding=1)\n",
    "\n",
    "    def forward(self, *args, key_padding_mask=None, **kwargs):\n",
    "        self.mlp.fc1.src_key_padding_mask = key_padding_mask\n",
    "        self.mlp.fc2.src_key_padding_mask = key_padding_mask\n",
    "        return super().forward(*args, **kwargs)\n",
    "\n",
    "\n",
    "class RNA_ModelV2(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        # self.emb = nn.Sequential(nn.Embedding(4,dim//4), Conv1D(dim//4,dim,7,padding=3),\n",
    "        #                        nn.LayerNorm(dim), nn.GELU(), Conv1D(dim,dim,3,padding=1))\n",
    "        self.extractor = Extractor(dim)\n",
    "        # self.pos_enc = SinusoidalPosEmb(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # self.transformer = nn.TransformerEncoder(\n",
    "        #    TransformerEncoderLayer_conv(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "        #        dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)\n",
    "        self.proj_out = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNA_ModelV2SS(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        # self.emb = nn.Sequential(nn.Embedding(4,dim//4), Conv1D(dim//4,dim,7,padding=3),\n",
    "        #                        nn.LayerNorm(dim), nn.GELU(), Conv1D(dim,dim,3,padding=1))\n",
    "        self.seq_extractor = Extractor(dim // 2)\n",
    "        self.ss_extractor = Extractor(dim // 2, 3)\n",
    "        # self.pos_enc = SinusoidalPosEmb(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # self.transformer = nn.TransformerEncoder(\n",
    "        #    TransformerEncoderLayer_conv(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "        #        dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)\n",
    "        self.proj_out = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        ss = x0[\"ss_seq\"][:, :Lmax]\n",
    "        seq = self.seq_extractor(x, src_key_padding_mask=~mask)\n",
    "        ss = self.ss_extractor(ss, src_key_padding_mask=~mask)\n",
    "        x = torch.concat([seq, ss], dim=-1)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNA_ModelV2SSV1(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        # self.emb = nn.Sequential(nn.Embedding(4,dim//4), Conv1D(dim//4,dim,7,padding=3),\n",
    "        #                        nn.LayerNorm(dim), nn.GELU(), Conv1D(dim,dim,3,padding=1))\n",
    "        self.extractor = ExtractorV0(dim)\n",
    "        # self.pos_enc = SinusoidalPosEmb(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # self.transformer = nn.TransformerEncoder(\n",
    "        #    TransformerEncoderLayer_conv(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "        #        dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)\n",
    "        self.proj_out = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        ss = x0[\"ss_seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, ss, src_key_padding_mask=~mask)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerV0(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, attb_heads=8, out=2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(4, dim)\n",
    "        self.dec = ContinuousTransformerWrapper(\n",
    "            dim_in=dim,\n",
    "            dim_out=out,\n",
    "            max_seq_len=512,\n",
    "            attn_layers=Encoder(\n",
    "                dim=dim,\n",
    "                depth=depth,\n",
    "                heads=attb_heads,\n",
    "                attn_flash=True,\n",
    "                rotary_pos_emb=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.emb(x)\n",
    "        out = self.dec(x, mask=mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CustomTransformerV1(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, attn_heads=8, head_size=32):\n",
    "        super().__init__()\n",
    "        self.dec = TransformerWrapper(\n",
    "            num_tokens=4,\n",
    "            logits_dim=2,\n",
    "            max_seq_len=512,\n",
    "            attn_layers=Encoder(\n",
    "                dim=dim,\n",
    "                depth=depth,\n",
    "                heads=attn_heads,\n",
    "                attn_flash=True,\n",
    "                rotary_pos_emb=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        out = self.dec(x, mask=mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        num_layers=2,\n",
    "        dropout=0.5,\n",
    "        use_bn=False,\n",
    "        heads=2,\n",
    "        out_heads=1,\n",
    "    ):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            GATConv(\n",
    "                in_channels, hidden_channels, dropout=dropout, heads=heads, concat=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels * heads))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GATConv(\n",
    "                    hidden_channels * heads,\n",
    "                    hidden_channels,\n",
    "                    dropout=dropout,\n",
    "                    heads=heads,\n",
    "                    concat=True,\n",
    "                )\n",
    "            )\n",
    "            self.bns.append(nn.LayerNorm(hidden_channels * heads))\n",
    "\n",
    "        self.convs.append(\n",
    "            GATConv(\n",
    "                hidden_channels * heads,\n",
    "                out_channels,\n",
    "                dropout=dropout,\n",
    "                heads=out_heads,\n",
    "                concat=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = F.elu\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        res = x\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i](x)\n",
    "            x = self.activation(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        x = res + x\n",
    "        return x\n",
    "\n",
    "\n",
    "def to_graph_batchv1(seq, mask, adj_matrix):\n",
    "    res = []\n",
    "    for i in range(len(seq)):\n",
    "        res.append(Data(x=seq[i][mask[i]], edge_index=adj_matrix[i].nonzero().T))\n",
    "    return Batch.from_data_list(res)\n",
    "\n",
    "\n",
    "class PytorchBatchWrapper(nn.Module):\n",
    "    def __init__(self, md):\n",
    "        super().__init__()\n",
    "        self.md = md\n",
    "\n",
    "    def forward(self, seq, mask, adj_matrix, batch_index=False):\n",
    "        batch = to_graph_batchv1(seq, mask, adj_matrix)\n",
    "        if batch_index:\n",
    "            out = self.md(batch.x, batch.edge_index, batch=batch.batch)\n",
    "        else:\n",
    "            out = self.md(batch.x, batch.edge_index)\n",
    "        out, _ = to_dense_batch(out, batch.batch)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RNA_ModelV3(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, graph_layers_every=4, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = Extractor(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.graph_layers_every = graph_layers_every\n",
    "        self.graph_layers = nn.ModuleList(\n",
    "            [\n",
    "                PytorchBatchWrapper(\n",
    "                    GAT(\n",
    "                        in_channels=dim,\n",
    "                        hidden_channels=dim // 2,\n",
    "                        out_channels=dim,\n",
    "                        num_layers=2,\n",
    "                        dropout=0.1,\n",
    "                        use_bn=True,\n",
    "                        heads=4,\n",
    "                        out_heads=1,\n",
    "                    )\n",
    "                )\n",
    "                for i in range(depth)\n",
    "                if i % self.graph_layers_every == 0\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.proj_out = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "\n",
    "        graph_layer_index = 0\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "            if i % self.graph_layers_every == 0:\n",
    "                x = self.graph_layers[graph_layer_index](x, mask, x0[\"adj_matrix\"])\n",
    "                graph_layer_index += 1\n",
    "\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNA_ModelV3SS(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, graph_layers_every=4, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = Extractor(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.graph_layers_every = graph_layers_every\n",
    "        self.graph_layers = nn.ModuleList(\n",
    "            [\n",
    "                PytorchBatchWrapper(\n",
    "                    GAT(\n",
    "                        in_channels=dim,\n",
    "                        hidden_channels=dim // 2,\n",
    "                        out_channels=dim,\n",
    "                        num_layers=2,\n",
    "                        dropout=0.1,\n",
    "                        use_bn=True,\n",
    "                        heads=4,\n",
    "                        out_heads=1,\n",
    "                    )\n",
    "                )\n",
    "                for i in range(depth)\n",
    "                if i % self.graph_layers_every == 0\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.proj_out = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "\n",
    "        graph_layer_index = 0\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "            if i % self.graph_layers_every == 0:\n",
    "                x = self.graph_layers[graph_layer_index](x, mask, x0[\"ss_adj\"])\n",
    "                graph_layer_index += 1\n",
    "\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        num_layers=2,\n",
    "        dropout=0.5,\n",
    "        save_mem=True,\n",
    "        use_bn=True,\n",
    "    ):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        # self.convs.append(\n",
    "        #     GCNConv(in_channels, hidden_channels, cached=not save_mem, normalize=not save_mem))\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels, cached=not save_mem))\n",
    "\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, cached=not save_mem)\n",
    "            )\n",
    "            self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "\n",
    "        # self.convs.append(\n",
    "        #     GCNConv(hidden_channels, out_channels, cached=not save_mem, normalize=not save_mem))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels, cached=not save_mem))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = F.gelu\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i](x)\n",
    "            x = self.activation(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "        var = torch.var(x, dim=-1, unbiased=False, keepdim=True)\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        return (x - mean) * (var + eps).rsqrt() * self.g\n",
    "\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gate)\n",
    "\n",
    "\n",
    "class FeedForwardV0(nn.Module):\n",
    "    def __init__(self, dim, out=2, mult=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        inner_dim = int(dim * mult)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim * 2, bias=False),\n",
    "            GEGLU(),\n",
    "            LayerNorm(inner_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, out, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class RNA_ModelV4(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, graph_layers_every=3, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = Extractor(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.graph_layers_every = graph_layers_every\n",
    "        self.graph_layers = PytorchBatchWrapper(\n",
    "            GCN(\n",
    "                dim * (depth // graph_layers_every),\n",
    "                dim,\n",
    "                dim,\n",
    "                num_layers=depth // graph_layers_every,\n",
    "                dropout=0.2,\n",
    "                use_bn=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.proj_out = FeedForwardV0(dim * 2, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "\n",
    "        intermediates = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "            if i % self.graph_layers_every == 0:\n",
    "                intermediates.append(x)\n",
    "        graph = self.graph_layers(\n",
    "            torch.concat(intermediates, dim=-1), mask, x0[\"adj_matrix\"]\n",
    "        )\n",
    "\n",
    "        x = torch.concat([x, graph], dim=-1)\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNA_ModelV5(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=192,\n",
    "        depth=12,\n",
    "        head_size=32,\n",
    "        graph_layers_every=4,\n",
    "        edge_dim=8,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = Extractor(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.adj_emb = nn.Embedding(2, edge_dim)\n",
    "        self.graph_layers_every = graph_layers_every\n",
    "        self.graph_layers = nn.ModuleList(\n",
    "            [\n",
    "                GraphTransformerAdjacent(\n",
    "                    dim=dim,\n",
    "                    depth=1,\n",
    "                    heads=4,\n",
    "                    edge_dim=edge_dim,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "                if i % self.graph_layers_every == 0\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.proj_out = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        adj_matrix = x0[\"adj_matrix\"][:, :Lmax, :Lmax]\n",
    "        adj_matrix = self.adj_emb(adj_matrix.long())\n",
    "\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "\n",
    "        graph_layer_index = 0\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "            if i % self.graph_layers_every == 0:\n",
    "                x = self.graph_layers[graph_layer_index](x, mask, adj_matrix)\n",
    "                graph_layer_index += 1\n",
    "\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNA_ModelV6(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, graph_layers_every=4, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = Extractor(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.graph_layers_every = graph_layers_every\n",
    "        self.graph_layers = nn.ModuleList(\n",
    "            [\n",
    "                PytorchBatchWrapper(\n",
    "                    GCN(\n",
    "                        dim,\n",
    "                        dim // 2,\n",
    "                        dim,\n",
    "                        num_layers=depth // graph_layers_every,\n",
    "                        dropout=0.2,\n",
    "                        use_bn=True,\n",
    "                    )\n",
    "                )\n",
    "                for i in range(depth)\n",
    "                if i % self.graph_layers_every == 0\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.proj_out = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "\n",
    "        graph_layer_index = 0\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "            if i % self.graph_layers_every == 0:\n",
    "                x = self.graph_layers[graph_layer_index](x, mask, x0[\"adj_matrix\"])\n",
    "                graph_layer_index += 1\n",
    "\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNA_ModelV7(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, graph_layers=4, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = Extractor(dim // 2)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.graph_layers = PytorchBatchWrapper(\n",
    "            GAT(\n",
    "                in_channels=dim // 2,\n",
    "                hidden_channels=dim // 2,\n",
    "                out_channels=dim // 2,\n",
    "                num_layers=graph_layers,\n",
    "                dropout=0.1,\n",
    "                use_bn=True,\n",
    "                heads=4,\n",
    "                out_heads=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.proj_out = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        x = torch.concat([x, self.graph_layers(x, mask, x0[\"adj_matrix\"])], -1)\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNA_ModelV8(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, graph_layers=4, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = Extractor(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.graph_layers = PytorchBatchWrapper(\n",
    "            GAT(\n",
    "                in_channels=dim,\n",
    "                hidden_channels=dim // 2,\n",
    "                out_channels=dim,\n",
    "                num_layers=graph_layers,\n",
    "                dropout=0.4,\n",
    "                use_bn=True,\n",
    "                heads=4,\n",
    "                out_heads=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.proj_out =  PreNorm(dim, nn.Sequential(\n",
    "             nn.Linear(dim, dim // 2),\n",
    "             nn.GELU(),\n",
    "             nn.Linear(dim // 2, 2)\n",
    "        ))\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "\n",
    "        x = self.graph_layers(x, mask, x0[\"adj_matrix\"])\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "class RNA_ModelV9(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, graph_layers=4, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = Extractor(dim // 3)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=0.2 * (i / (depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=0.1,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.bpp = PytorchBatchWrapper(\n",
    "            GAT(\n",
    "                in_channels=dim // 3,\n",
    "                hidden_channels=dim // 2,\n",
    "                out_channels=dim // 3,\n",
    "                num_layers=graph_layers,\n",
    "                dropout=0.1,\n",
    "                use_bn=True,\n",
    "                heads=4,\n",
    "                out_heads=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.ss = PytorchBatchWrapper(\n",
    "            GAT(\n",
    "                in_channels=dim // 3,\n",
    "                hidden_channels=dim // 2,\n",
    "                out_channels=dim // 3,\n",
    "                num_layers=graph_layers,\n",
    "                dropout=0.1,\n",
    "                use_bn=True,\n",
    "                heads=4,\n",
    "                out_heads=1,\n",
    "            )\n",
    "        )\n",
    "        self.proj_out = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        x = torch.concat([x, self.bpp(x, mask, x0[\"adj_matrix\"]), self.ss(x, mask, x0[\"ss_adj\"])], -1)\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from rnacomp.dataset import LenMatchBatchSampler, RNA_DatasetBaselineSplitbppV0, DeviceDataLoader, RNA_DatasetBaselineSplitssV1, RNA_DatasetBaselineSplit, RNA_DatasetBaselineSplitbppV1, RNA_DatasetBaselineSplitssbppV0\n",
    "\n",
    "# class CFG:\n",
    "#     path = Path(\"../data/\")\n",
    "#     pathbb = Path(\"../data/Ribonanza_bpp_files\")\n",
    "#     split_id = Path('../eda/fold_split.csv')\n",
    "#     pathss = Path(\"../eda/train_ss_vienna_rna.parquet\")\n",
    "#     bs = 16\n",
    "#     num_workers = 8\n",
    "#     device = 'cpu'\n",
    "#     adjnact_prob = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# fns = list(CFG.pathbb.rglob(\"*.txt\"))\n",
    "# bpp_df = pd.DataFrame({\"bpp\": fns})\n",
    "# bpp_df['sequence_id'] = bpp_df['bpp'].apply(lambda x: x.stem)\n",
    "# ss = pd.read_parquet(CFG.pathss)[[\"sequence_id\", \"ss_full\"]]\n",
    "# df = pd.read_parquet(CFG.path/'train_data.parquet')\n",
    "# split = pd.read_csv(CFG.split_id)\n",
    "# df = pd.merge(df, split, on='sequence_id')\n",
    "# df = pd.merge(df, bpp_df, on='sequence_id')\n",
    "# df = pd.merge(df, ss, on='sequence_id')\n",
    "# df_train = df.query('is_train==True').reset_index(drop=True)\n",
    "# df_valid = df.query('is_train==False').reset_index(drop=True)\n",
    "\n",
    "# ds_val = RNA_DatasetBaselineSplitssbppV0(df_valid, mode='eval')\n",
    "# ds_val_len = RNA_DatasetBaselineSplitssbppV0(df_valid, mode='eval', mask_only=True)\n",
    "# sampler_val = torch.utils.data.SequentialSampler(ds_val_len)\n",
    "# len_sampler_val = LenMatchBatchSampler(sampler_val, batch_size=CFG.bs, \n",
    "#                drop_last=False)\n",
    "# dl_val= DeviceDataLoader(torch.utils.data.DataLoader(ds_val, \n",
    "#                batch_sampler=len_sampler_val, num_workers=CFG.num_workers), CFG.device)\n",
    "\n",
    "# batch = next(iter(dl_val))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     out = RNA_ModelV9(dim=768)(batch)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export \n",
    "\n",
    "# from torch.utils.checkpoint import checkpoint\n",
    "# import warnings\n",
    "# from termcolor import colored\n",
    "# import torch.nn.functional as F\n",
    "# try:\n",
    "#     from flash_attn.flash_attn_interface import (\n",
    "#         flash_attn_unpadded_qkvpacked_func,\n",
    "#         flash_attn_unpadded_kvpacked_func,\n",
    "#     )\n",
    "#     from flash_attn.bert_padding import unpad_input, pad_input\n",
    "#     from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\n",
    "# except:\n",
    "#     warnings.warn(colored(\"Could not import flash_attn\", \"magenta\"))\n",
    "#     load_flash_attn_check = False\n",
    "\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(\n",
    "#         self, model_dim, ff_dim, use_bias, glu, initializer_range, zero_init, n_layers\n",
    "#     ):\n",
    "#         super(FeedForward, self).__init__()\n",
    "\n",
    "#         self.glu = glu\n",
    "\n",
    "#         if self.glu:\n",
    "#             ff_dim_2 = np.exp2(np.ceil(np.log2(256 * 4 / 3))).astype(int)\n",
    "#             ff_dim_1 = ff_dim_2 * 2\n",
    "#         else:\n",
    "#             ff_dim_1, ff_dim_2 = ff_dim, ff_dim\n",
    "\n",
    "#         self.input_norm = nn.LayerNorm(model_dim, eps=1e-6)\n",
    "\n",
    "#         self.linear_1 = nn.Linear(model_dim, ff_dim_1, bias=use_bias)\n",
    "#         self.linear_2 = nn.Linear(ff_dim_2, model_dim, bias=use_bias)\n",
    "#         self.act = nn.SiLU()\n",
    "\n",
    "#         self.initialize(zero_init, use_bias, initializer_range, n_layers)\n",
    "\n",
    "#     def initialize(self, zero_init, use_bias, initializer_range, n_layers):\n",
    "#         nn.init.normal_(self.linear_1.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "#         if use_bias:\n",
    "#             nn.init.constant_(self.linear_1.bias, 0.0)\n",
    "#             nn.init.constant_(self.linear_2.bias, 0.0)\n",
    "\n",
    "#         if zero_init:\n",
    "#             nn.init.constant_(self.linear_2.weight, 0.0)\n",
    "#         else:\n",
    "#             nn.init.normal_(\n",
    "#                 self.linear_2.weight,\n",
    "#                 mean=0.0,\n",
    "#                 std=initializer_range / math.sqrt(2 * n_layers),\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.input_norm(x)\n",
    "\n",
    "#         if self.glu:\n",
    "#             x = self.linear_1(x)\n",
    "#             x, gate = x.chunk(2, dim=-1)\n",
    "#             x = self.act(gate) * x\n",
    "#         else:\n",
    "#             x = self.act(self.linear_1(x))\n",
    "\n",
    "#         return self.linear_2(x)\n",
    "\n",
    "\n",
    "# class ConvFeedForward(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_dim,\n",
    "#         ff_dim,\n",
    "#         use_bias,\n",
    "#         initializer_range,\n",
    "#         n_layers,\n",
    "#         kernel,\n",
    "#         zero_init=True,\n",
    "#     ):\n",
    "#         super(ConvFeedForward, self).__init__()\n",
    "\n",
    "#         self.zero_init = zero_init\n",
    "\n",
    "#         self.input_norm = nn.GroupNorm(1, model_dim)\n",
    "\n",
    "#         if kernel == 1:\n",
    "#             self.conv1 = nn.Conv2d(model_dim, ff_dim, kernel_size=1, bias=use_bias)\n",
    "#             self.conv2 = nn.Conv2d(ff_dim, model_dim, kernel_size=1, bias=use_bias)\n",
    "#         else:\n",
    "#             self.conv1 = nn.Conv2d(\n",
    "#                 model_dim,\n",
    "#                 ff_dim,\n",
    "#                 bias=use_bias,\n",
    "#                 kernel_size=kernel,\n",
    "#                 padding=(kernel - 1) // 2,\n",
    "#             )\n",
    "#             self.conv2 = nn.Conv2d(\n",
    "#                 ff_dim,\n",
    "#                 model_dim,\n",
    "#                 bias=use_bias,\n",
    "#                 kernel_size=kernel,\n",
    "#                 padding=(kernel - 1) // 2,\n",
    "#             )\n",
    "\n",
    "#         self.act = nn.SiLU()\n",
    "\n",
    "#         self.initialize(zero_init, use_bias, initializer_range, n_layers)\n",
    "\n",
    "#     def initialize(self, zero_init, use_bias, initializer_range, n_layers):\n",
    "#         nn.init.normal_(self.conv1.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "#         if use_bias:\n",
    "#             nn.init.constant_(self.conv1.bias, 0.0)\n",
    "#             nn.init.constant_(self.conv2.bias, 0.0)\n",
    "\n",
    "#         if zero_init:\n",
    "#             nn.init.constant_(self.conv2.weight, 0.0)\n",
    "#         else:\n",
    "#             nn.init.normal_(\n",
    "#                 self.conv2.weight,\n",
    "#                 mean=0.0,\n",
    "#                 std=initializer_range / math.sqrt(2 * n_layers),\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "#         x = self.input_norm(x)\n",
    "#         x = self.act(self.conv1(x))\n",
    "#         x = self.conv2(x)\n",
    "#         x = x.permute(0, 2, 3, 1)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class FlashAttention2d(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_dim,\n",
    "#         num_head,\n",
    "#         softmax_scale,\n",
    "#         zero_init,\n",
    "#         use_bias,\n",
    "#         initializer_range,\n",
    "#         n_layers,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         assert model_dim % num_head == 0\n",
    "#         assert model_dim % num_head == 0\n",
    "#         self.key_dim = model_dim // num_head\n",
    "#         self.value_dim = model_dim // num_head\n",
    "\n",
    "#         self.causal = False\n",
    "#         self.checkpointing = False\n",
    "\n",
    "#         if softmax_scale:\n",
    "#             self.softmax_scale = self.key_dim ** (-0.5)\n",
    "#         else:\n",
    "#             self.softmax_scale = None\n",
    "\n",
    "#         self.num_head = num_head\n",
    "\n",
    "#         self.Wqkv = nn.Linear(model_dim, 3 * model_dim, bias=use_bias)\n",
    "\n",
    "#         self.out_proj = nn.Linear(model_dim, model_dim, bias=use_bias)\n",
    "\n",
    "#         self.initialize(zero_init, use_bias, initializer_range, n_layers)\n",
    "\n",
    "#     def initialize(self, zero_init, use_bias, initializer_range, n_layers):\n",
    "#         nn.init.normal_(self.Wqkv.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "#         if use_bias:\n",
    "#             nn.init.constant_(self.Wqkv.bias, 0.0)\n",
    "#             nn.init.constant_(self.out_proj.bias, 0.0)\n",
    "\n",
    "#         if zero_init:\n",
    "#             nn.init.constant_(self.out_proj.weight, 0.0)\n",
    "#         else:\n",
    "#             nn.init.normal_(\n",
    "#                 self.out_proj.weight,\n",
    "#                 mean=0.0,\n",
    "#                 std=initializer_range / math.sqrt(2 * n_layers),\n",
    "#             )\n",
    "\n",
    "#     def forward(self, pair_act, attention_mask):\n",
    "#         batch_size = pair_act.shape[0]\n",
    "#         seqlen = pair_act.shape[1]\n",
    "#         extended_batch_size = batch_size * seqlen\n",
    "\n",
    "#         qkv = self.Wqkv(pair_act)\n",
    "#         not_attention_mask = torch.logical_not(attention_mask)\n",
    "\n",
    "#         x_qkv = rearrange(\n",
    "#             qkv, \"b s f ... -> (b s) f ...\", b=batch_size, f=seqlen, s=seqlen\n",
    "#         )\n",
    "#         key_padding_mask = rearrange(\n",
    "#             not_attention_mask,\n",
    "#             \"b s f ... -> (b s) f ...\",\n",
    "#             b=batch_size,\n",
    "#             f=seqlen,\n",
    "#             s=seqlen,\n",
    "#         )\n",
    "\n",
    "#         x_unpad, indices, cu_seqlens, max_s = unpad_input(x_qkv, key_padding_mask)\n",
    "#         x_unpad = rearrange(\n",
    "#             x_unpad, \"nnz (three h d) -> nnz three h d\", three=3, h=self.num_head\n",
    "#         )\n",
    "\n",
    "#         if self.training and self.checkpointing:\n",
    "#             output_unpad = torch.utils.checkpoint.checkpoint(\n",
    "#                 flash_attn_unpadded_qkvpacked_func,\n",
    "#                 x_unpad,\n",
    "#                 cu_seqlens,\n",
    "#                 max_s,\n",
    "#                 0.0,\n",
    "#                 self.softmax_scale,\n",
    "#                 self.causal,\n",
    "#                 False,\n",
    "#             )\n",
    "#         else:\n",
    "#             output_unpad = flash_attn_unpadded_qkvpacked_func(\n",
    "#                 x_unpad,\n",
    "#                 cu_seqlens,\n",
    "#                 max_s,\n",
    "#                 0.0,\n",
    "#                 softmax_scale=self.softmax_scale,\n",
    "#                 causal=self.causal,\n",
    "#             )\n",
    "\n",
    "#         pre_pad_latent = rearrange(output_unpad, \"nnz h d -> nnz (h d)\")\n",
    "#         padded_latent = pad_input(pre_pad_latent, indices, extended_batch_size, seqlen)\n",
    "#         output = rearrange(padded_latent, \"b f (h d) -> b f h d\", h=self.num_head)\n",
    "\n",
    "#         output = rearrange(\n",
    "#             output, \"(b s) f h d -> b s f (h d)\", b=batch_size, f=seqlen, s=seqlen\n",
    "#         )\n",
    "\n",
    "#         return self.out_proj(output)\n",
    "\n",
    "\n",
    "# class Attention2d(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_dim,\n",
    "#         num_head,\n",
    "#         softmax_scale,\n",
    "#         precision,\n",
    "#         zero_init,\n",
    "#         use_bias,\n",
    "#         initializer_range,\n",
    "#         n_layers,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         assert model_dim % num_head == 0\n",
    "#         assert model_dim % num_head == 0\n",
    "#         self.key_dim = model_dim // num_head\n",
    "#         self.value_dim = model_dim // num_head\n",
    "\n",
    "#         if softmax_scale:\n",
    "#             self.softmax_scale = torch.sqrt(torch.FloatTensor([self.key_dim]))\n",
    "#         else:\n",
    "#             self.softmax_scale = False\n",
    "\n",
    "#         self.num_head = num_head\n",
    "#         self.model_dim = model_dim\n",
    "\n",
    "#         if precision == \"fp32\" or precision == 32 or precision == \"bf16\":\n",
    "#             self.mask_bias = -1e9\n",
    "#         elif precision == \"fp16\" or precision == 16:\n",
    "#             self.mask_bias = -1e4\n",
    "#         else:\n",
    "#             raise UserWarning(\n",
    "#                 f\"unknown precision: {precision} . Please us fp16, fp32 or bf16\"\n",
    "#             )\n",
    "\n",
    "#         self.Wqkv = nn.Linear(model_dim, 3 * model_dim, bias=use_bias)\n",
    "#         self.out_proj = nn.Linear(model_dim, model_dim, bias=use_bias)\n",
    "\n",
    "#         self.initialize(zero_init, use_bias, initializer_range, n_layers)\n",
    "\n",
    "#     def initialize(self, zero_init, use_bias, initializer_range, n_layers):\n",
    "#         nn.init.normal_(self.Wqkv.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "#         if use_bias:\n",
    "#             nn.init.constant_(self.Wqkv.bias, 0.0)\n",
    "#             nn.init.constant_(self.out_proj.bias, 0.0)\n",
    "\n",
    "#         if zero_init:\n",
    "#             nn.init.constant_(self.out_proj.weight, 0.0)\n",
    "#         else:\n",
    "#             nn.init.normal_(\n",
    "#                 self.out_proj.weight,\n",
    "#                 mean=0.0,\n",
    "#                 std=initializer_range / math.sqrt(2 * n_layers),\n",
    "#             )\n",
    "\n",
    "#     def forward(self, pair_act, attention_mask):\n",
    "#         batch_size = pair_act.size(0)\n",
    "#         N_seq = pair_act.size(1)\n",
    "#         N_res = pair_act.size(2)\n",
    "\n",
    "#         query, key, value = self.Wqkv(pair_act).split(self.model_dim, dim=3)\n",
    "\n",
    "#         query = query.view(\n",
    "#             batch_size, N_seq, N_res, self.num_head, self.key_dim\n",
    "#         ).permute(0, 1, 3, 2, 4)\n",
    "#         key = key.view(batch_size, N_seq, N_res, self.num_head, self.value_dim).permute(\n",
    "#             0, 1, 3, 4, 2\n",
    "#         )\n",
    "#         value = value.view(\n",
    "#             batch_size, N_seq, N_res, self.num_head, self.value_dim\n",
    "#         ).permute(0, 1, 3, 2, 4)\n",
    "\n",
    "#         attn_weights = torch.matmul(query, key)\n",
    "\n",
    "#         if self.softmax_scale:\n",
    "#             attn_weights = attn_weights / self.softmax_scale.to(pair_act.device)\n",
    "\n",
    "#         if attention_mask is not None:\n",
    "#             attention_mask = attention_mask[:, :, None, None, :]\n",
    "#             attn_weights.masked_fill_(attention_mask, self.mask_bias)\n",
    "#         attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "\n",
    "#         weighted_avg = torch.matmul(attn_weights, value).permute(0, 1, 3, 2, 4)\n",
    "\n",
    "#         output = self.out_proj(\n",
    "#             weighted_avg.reshape(\n",
    "#                 batch_size, N_seq, N_res, self.num_head * self.value_dim\n",
    "#             )\n",
    "#         )\n",
    "#         return output\n",
    "\n",
    "\n",
    "# class TriangleAttention(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_dim,\n",
    "#         num_head,\n",
    "#         orientation,\n",
    "#         softmax_scale,\n",
    "#         precision,\n",
    "#         zero_init,\n",
    "#         use_bias,\n",
    "#         flash_attn,\n",
    "#         initializer_range,\n",
    "#         n_layers,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.model_dim = model_dim\n",
    "#         self.num_head = num_head\n",
    "\n",
    "#         assert orientation in [\"per_row\", \"per_column\"]\n",
    "#         self.orientation = orientation\n",
    "\n",
    "#         self.input_norm = nn.LayerNorm(model_dim, eps=1e-6)\n",
    "\n",
    "#         if flash_attn:\n",
    "#             self.attn = FlashAttention2d(\n",
    "#                 model_dim,\n",
    "#                 num_head,\n",
    "#                 softmax_scale,\n",
    "#                 zero_init,\n",
    "#                 use_bias,\n",
    "#                 initializer_range,\n",
    "#                 n_layers,\n",
    "#             )\n",
    "#         else:\n",
    "#             self.attn = Attention2d(\n",
    "#                 model_dim,\n",
    "#                 num_head,\n",
    "#                 softmax_scale,\n",
    "#                 precision,\n",
    "#                 zero_init,\n",
    "#                 use_bias,\n",
    "#                 initializer_range,\n",
    "#                 n_layers,\n",
    "#             )\n",
    "\n",
    "#     def forward(self, pair_act, pair_mask, cycle_infer=False):\n",
    "#         assert len(pair_act.shape) == 4\n",
    "\n",
    "#         if self.orientation == \"per_column\":\n",
    "#             pair_act = torch.swapaxes(pair_act, -2, -3)\n",
    "#             if pair_mask is not None:\n",
    "#                 pair_mask = torch.swapaxes(pair_mask, -1, -2)\n",
    "\n",
    "#         pair_act = self.input_norm(pair_act)\n",
    "\n",
    "#         if self.training and not cycle_infer:\n",
    "#             pair_act = checkpoint(self.attn, pair_act, pair_mask, use_reentrant=True)\n",
    "#         else:\n",
    "#             pair_act = self.attn(pair_act, pair_mask)\n",
    "\n",
    "#         if self.orientation == \"per_column\":\n",
    "#             pair_act = torch.swapaxes(pair_act, -2, -3)\n",
    "\n",
    "#         return pair_act\n",
    "\n",
    "\n",
    "# class RNAformerBlock(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "\n",
    "#         ff_dim = int(config.ff_factor * config.model_dim)\n",
    "\n",
    "#         self.attn_pair_row = TriangleAttention(\n",
    "#             config.model_dim,\n",
    "#             config.num_head,\n",
    "#             \"per_row\",\n",
    "#             config.softmax_scale,\n",
    "#             config.precision,\n",
    "#             config.zero_init,\n",
    "#             config.use_bias,\n",
    "#             config.flash_attn,\n",
    "#             config.initializer_range,\n",
    "#             config.n_layers,\n",
    "#         )\n",
    "#         self.attn_pair_col = TriangleAttention(\n",
    "#             config.model_dim,\n",
    "#             config.num_head,\n",
    "#             \"per_column\",\n",
    "#             config.softmax_scale,\n",
    "#             config.precision,\n",
    "#             config.zero_init,\n",
    "#             config.use_bias,\n",
    "#             config.flash_attn,\n",
    "#             config.initializer_range,\n",
    "#             config.n_layers,\n",
    "#         )\n",
    "\n",
    "#         self.pair_dropout_row = nn.Dropout(p=config.resi_dropout / 2)\n",
    "#         self.pair_dropout_col = nn.Dropout(p=config.resi_dropout / 2)\n",
    "\n",
    "#         if config.ff_kernel:\n",
    "#             self.pair_transition = ConvFeedForward(\n",
    "#                 config.model_dim,\n",
    "#                 ff_dim,\n",
    "#                 use_bias=config.use_bias,\n",
    "#                 kernel=config.ff_kernel,\n",
    "#                 initializer_range=config.initializer_range,\n",
    "#                 zero_init=config.zero_init,\n",
    "#                 n_layers=config.n_layers,\n",
    "#             )\n",
    "#         else:\n",
    "#             self.pair_transition = FeedForward(\n",
    "#                 config.model_dim,\n",
    "#                 ff_dim,\n",
    "#                 use_bias=config.use_bias,\n",
    "#                 glu=config.use_glu,\n",
    "#                 initializer_range=config.initializer_range,\n",
    "#                 zero_init=config.zero_init,\n",
    "#                 n_layers=config.n_layers,\n",
    "#             )\n",
    "\n",
    "#         self.res_dropout = nn.Dropout(p=config.resi_dropout)\n",
    "\n",
    "#     def forward(self, pair_act, pair_mask, cycle_infer=False):\n",
    "#         pair_act = pair_act + self.pair_dropout_row(\n",
    "#             self.attn_pair_row(pair_act, pair_mask, cycle_infer)\n",
    "#         )\n",
    "#         pair_act = pair_act + self.pair_dropout_col(\n",
    "#             self.attn_pair_col(pair_act, pair_mask, cycle_infer)\n",
    "#         )\n",
    "#         pair_act = pair_act + self.res_dropout(self.pair_transition(pair_act))\n",
    "\n",
    "#         return pair_act\n",
    "\n",
    "\n",
    "# class ConvFeedForward(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_dim,\n",
    "#         ff_dim,\n",
    "#         use_bias,\n",
    "#         initializer_range,\n",
    "#         n_layers,\n",
    "#         kernel,\n",
    "#         zero_init=True,\n",
    "#     ):\n",
    "#         super(ConvFeedForward, self).__init__()\n",
    "\n",
    "#         self.zero_init = zero_init\n",
    "\n",
    "#         self.input_norm = nn.GroupNorm(1, model_dim)\n",
    "\n",
    "#         if kernel == 1:\n",
    "#             self.conv1 = nn.Conv2d(model_dim, ff_dim, kernel_size=1, bias=use_bias)\n",
    "#             self.conv2 = nn.Conv2d(ff_dim, model_dim, kernel_size=1, bias=use_bias)\n",
    "#         else:\n",
    "#             self.conv1 = nn.Conv2d(\n",
    "#                 model_dim,\n",
    "#                 ff_dim,\n",
    "#                 bias=use_bias,\n",
    "#                 kernel_size=kernel,\n",
    "#                 padding=(kernel - 1) // 2,\n",
    "#             )\n",
    "#             self.conv2 = nn.Conv2d(\n",
    "#                 ff_dim,\n",
    "#                 model_dim,\n",
    "#                 bias=use_bias,\n",
    "#                 kernel_size=kernel,\n",
    "#                 padding=(kernel - 1) // 2,\n",
    "#             )\n",
    "\n",
    "#         self.act = nn.SiLU()\n",
    "\n",
    "#         self.initialize(zero_init, use_bias, initializer_range, n_layers)\n",
    "\n",
    "#     def initialize(self, zero_init, use_bias, initializer_range, n_layers):\n",
    "#         nn.init.normal_(self.conv1.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "#         if use_bias:\n",
    "#             nn.init.constant_(self.conv1.bias, 0.0)\n",
    "#             nn.init.constant_(self.conv2.bias, 0.0)\n",
    "\n",
    "#         if zero_init:\n",
    "#             nn.init.constant_(self.conv2.weight, 0.0)\n",
    "#         else:\n",
    "#             nn.init.normal_(\n",
    "#                 self.conv2.weight,\n",
    "#                 mean=0.0,\n",
    "#                 std=initializer_range / math.sqrt(2 * n_layers),\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "#         x = self.input_norm(x)\n",
    "#         x = self.act(self.conv1(x))\n",
    "#         x = self.conv2(x)\n",
    "#         x = x.permute(0, 2, 3, 1)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class PosEmbedding(nn.Module):\n",
    "#     def __init__(self, vocab, model_dim, max_len, rel_pos_enc, initializer_range):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.rel_pos_enc = rel_pos_enc\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#         self.embed_seq = nn.Embedding(vocab, model_dim)\n",
    "\n",
    "#         self.scale = nn.Parameter(\n",
    "#             torch.sqrt(torch.FloatTensor([model_dim // 2])), requires_grad=False\n",
    "#         )\n",
    "\n",
    "#         if rel_pos_enc:\n",
    "#             self.embed_pair_pos = nn.Linear(max_len, model_dim, bias=False)\n",
    "#         else:\n",
    "#             self.embed_pair_pos = nn.Linear(model_dim, model_dim, bias=False)\n",
    "\n",
    "#             pe = torch.zeros(max_len, model_dim)\n",
    "#             position = torch.arange(0, max_len).unsqueeze(1).type(torch.FloatTensor)\n",
    "#             div_term = torch.exp(\n",
    "#                 torch.arange(0, model_dim, 2).type(torch.FloatTensor)\n",
    "#                 * -(math.log(10000.0) / model_dim)\n",
    "#             )\n",
    "#             pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#             pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#             pe = pe.unsqueeze(0)\n",
    "#             pe = torch.nn.Parameter(pe, requires_grad=False)\n",
    "#             self.register_buffer(\"pe\", pe)\n",
    "\n",
    "#         self.initialize(initializer_range)  #\n",
    "\n",
    "#     def initialize(self, initializer_range):\n",
    "#         nn.init.normal_(self.embed_seq.weight, mean=0.0, std=initializer_range)\n",
    "#         nn.init.normal_(self.embed_pair_pos.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "#     def relative_position_encoding(self, src_seq):\n",
    "#         residue_index = torch.arange(src_seq.size()[1], device=src_seq.device).expand(\n",
    "#             src_seq.size()\n",
    "#         )\n",
    "#         rel_pos = F.one_hot(\n",
    "#             torch.clip(residue_index, min=0, max=self.max_len - 1), self.max_len\n",
    "#         )\n",
    "\n",
    "#         if isinstance(self.embed_pair_pos.weight, torch.cuda.BFloat16Tensor):\n",
    "#             rel_pos = rel_pos.type(torch.bfloat16)\n",
    "#         elif isinstance(self.embed_pair_pos.weight, torch.cuda.HalfTensor):\n",
    "#             rel_pos = rel_pos.half()\n",
    "#         else:\n",
    "#             rel_pos = rel_pos.type(torch.float32)\n",
    "\n",
    "#         pos_encoding = self.embed_pair_pos(rel_pos)\n",
    "#         return pos_encoding\n",
    "\n",
    "#     def forward(self, src_seq):\n",
    "#         seq_embed = self.embed_seq(src_seq) * self.scale\n",
    "\n",
    "#         if self.rel_pos_enc:\n",
    "#             seq_embed = seq_embed + self.relative_position_encoding(src_seq)\n",
    "#         else:\n",
    "#             seq_embed = seq_embed + self.embed_pair_pos(self.pe[:, : src_seq.size(1)])\n",
    "\n",
    "#         return seq_embed\n",
    "\n",
    "\n",
    "# class EmbedSequence2Matrix(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.src_embed_1 = PosEmbedding(\n",
    "#             config.seq_vocab_size,\n",
    "#             config.model_dim,\n",
    "#             config.max_len,\n",
    "#             config.rel_pos_enc,\n",
    "#             config.initializer_range,\n",
    "#         )\n",
    "#         self.src_embed_2 = PosEmbedding(\n",
    "#             config.seq_vocab_size,\n",
    "#             config.model_dim,\n",
    "#             config.max_len,\n",
    "#             config.rel_pos_enc,\n",
    "#             config.initializer_range,\n",
    "#         )\n",
    "\n",
    "#         self.norm = nn.LayerNorm(config.model_dim)\n",
    "\n",
    "#     def forward(self, src_seq):\n",
    "#         seq_1_embed = self.src_embed_1(src_seq)\n",
    "#         seq_2_embed = self.src_embed_2(src_seq)\n",
    "\n",
    "#         pair_latent = seq_1_embed.unsqueeze(1) + seq_2_embed.unsqueeze(2)\n",
    "\n",
    "#         pair_latent = self.norm(pair_latent)\n",
    "\n",
    "#         return pair_latent\n",
    "\n",
    "\n",
    "# class RNAformerStack(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.output_ln = nn.LayerNorm(config.model_dim)\n",
    "\n",
    "#         module_list = []\n",
    "#         for idx in range(config.n_layers):\n",
    "#             layer = RNAformerBlock(config=config)\n",
    "#             module_list.append(layer)\n",
    "#         self.layers = nn.ModuleList(module_list)\n",
    "\n",
    "#     def forward(self, pair_act, pair_mask, cycle_infer=False):\n",
    "#         for idx, layer in enumerate(self.layers):\n",
    "#             pair_act = layer(pair_act, pair_mask, cycle_infer=cycle_infer)\n",
    "\n",
    "#         pair_act = self.output_ln(pair_act)\n",
    "\n",
    "#         return pair_act\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# def load_matching_weights(model, weights_path):\n",
    "#     \"\"\"\n",
    "#     Load model weights from a given path if they match, otherwise skip.\n",
    "#     Prints the number of matched and unmatched weights.\n",
    "\n",
    "#     :param model: The PyTorch model for which weights should be loaded.\n",
    "#     :param weights_path: The path to the saved weights file (.pth or .pt).\n",
    "#     \"\"\"\n",
    "#     # Load the saved state dictionary\n",
    "#     saved_state_dict = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "\n",
    "#     # Get the model's state dictionary\n",
    "#     model_state_dict = model.state_dict()\n",
    "\n",
    "#     # Create a new state dictionary to store matching weights\n",
    "#     matching_state_dict = {}\n",
    "\n",
    "#     # Initialize counters for matched and unmatched weights\n",
    "#     matched_weights = 0\n",
    "#     unmatched_weights = 0\n",
    "\n",
    "#     # Iterate through the saved state dictionary\n",
    "#     for name, saved_weight in saved_state_dict.items():\n",
    "#         # Check if the name exists in the model's state dictionary and the shapes match\n",
    "#         if name in model_state_dict and model_state_dict[name].shape == saved_weight.shape:\n",
    "#             # If it matches, add it to the matching state dictionary\n",
    "#             matching_state_dict[name] = saved_weight\n",
    "#             matched_weights += 1\n",
    "#         else:\n",
    "#             #print(f\"Skipping weight: {name} - Shape mismatch or not found in model\")\n",
    "#             unmatched_weights += 1\n",
    "\n",
    "#     # Update the model's state dictionary with the matching state dictionary\n",
    "#     model_state_dict.update(matching_state_dict)\n",
    "\n",
    "#     # Load the updated state dictionary into the model\n",
    "#     model.load_state_dict(model_state_dict)\n",
    "\n",
    "#     print(f\"Matched weights: {matched_weights}\")\n",
    "#     print(f\"Unmatched weights: {unmatched_weights}\")\n",
    "\n",
    "# class CONFIGRNAFORMER:\n",
    "#     model_dim = 128  # hidden dimension of transformer\n",
    "#     n_layers = 6  # number of transformer layers\n",
    "#     num_head = 4  # number of heads per layer\n",
    "#     ff_factor = 4  # hidden dim * ff_factor = size of feed-forward layer\n",
    "#     ff_kernel = 3\n",
    "#     cycling = False\n",
    "#     resi_dropout = 0.1\n",
    "#     embed_dropout = 0.1\n",
    "#     rel_pos_enc = True  # relative position encoding\n",
    "#     head_bias = False\n",
    "#     ln_eps = 1e-5\n",
    "#     softmax_scale = True\n",
    "#     key_dim_scaler = True\n",
    "#     gating = False\n",
    "#     use_glu = False\n",
    "#     use_bias = True\n",
    "#     flash_attn = False\n",
    "#     initializer_range = 0.02\n",
    "#     zero_init = False\n",
    "#     precision = 16\n",
    "#     seq_vocab_size = 5\n",
    "#     max_len = 500\n",
    "    \n",
    "# class CustomRnaFormer(nn.Module):\n",
    "#     def __init__(self, config = CONFIGRNAFORMER):\n",
    "#         super().__init__()\n",
    "#         self.seq2mat_embed = EmbedSequence2Matrix(config)\n",
    "#         self.RNAformer = RNAformerStack(config)\n",
    "#         self.proj_out = nn.Sequential(\n",
    "#             nn.LayerNorm(config.model_dim),  # Layer Normalization\n",
    "#             nn.Linear(config.model_dim, config.model_dim // 2),  # Linear Layer\n",
    "#             nn.GELU(),  # GeLU activation\n",
    "#             nn.Linear(config.model_dim // 2, 2),  # Final Linear Layer with 2 outputs\n",
    "#         )\n",
    "\n",
    "#     def make_pair_mask(self, src, src_len):\n",
    "#         encode_mask = torch.arange(src.shape[1], device=src.device).expand(\n",
    "#             src.shape[:2]\n",
    "#         ) < src_len.unsqueeze(1)\n",
    "\n",
    "#         pair_mask = encode_mask[:, None, :] * encode_mask[:, :, None]\n",
    "\n",
    "#         assert isinstance(pair_mask, torch.BoolTensor) or isinstance(\n",
    "#             pair_mask, torch.cuda.BoolTensor\n",
    "#         )\n",
    "#         return torch.bitwise_not(pair_mask)\n",
    "\n",
    "#     def extract_features(self, x, mask_L):\n",
    "#         n_n_mask = self.make_pair_mask(x, mask_L)\n",
    "#         n_n_latent = self.seq2mat_embed(x)\n",
    "#         n_n_latent.masked_fill_(n_n_mask[:, :, :, None], 0.0)\n",
    "#         latent = self.RNAformer(\n",
    "#             pair_act=n_n_latent, pair_mask=n_n_mask, cycle_infer=False\n",
    "#         )\n",
    "#         latent.masked_fill_(n_n_mask.unsqueeze(-1), 0.0)\n",
    "#         return latent.max(2)[0]\n",
    "\n",
    "#     def forward(self, x0):\n",
    "#         mask = x0[\"mask\"]\n",
    "#         L0 = mask.shape[1]\n",
    "#         Lmax = mask.sum(-1).max() + 5\n",
    "#         mask = mask[:, :Lmax]\n",
    "#         x = x0[\"seq\"][:, :Lmax]\n",
    "#         mask_L = x0[\"mask\"].sum(-1)\n",
    "#         out = self.extract_features(x, mask_L)\n",
    "#         out = self.proj_out(out)\n",
    "#         out = F.pad(out, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "    \n",
    "#         return out\n",
    "\n",
    "# def CustomRnaFormerV0():\n",
    "#     md = CustomRnaFormer()\n",
    "#     load_matching_weights(md, '/opt/slh/rna/eda/rna_former.pth')\n",
    "#     return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
