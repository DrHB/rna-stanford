{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp modelsconv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "from rotary_embedding_torch import RotaryEmbedding, apply_rotary_emb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import math\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.data import Data, Batch\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, TransformerWrapper\n",
    "from torch_geometric.nn import GATConv, GCNConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def good_luck():\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, img_ch=3, output_ch=1, CH_FOLD2=1):\n",
    "        super(U_Net, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch, ch_out=int(32 * CH_FOLD2))\n",
    "        self.Conv2 = conv_block(ch_in=int(32 * CH_FOLD2), ch_out=int(64 * CH_FOLD2))\n",
    "        self.Conv3 = conv_block(ch_in=int(64 * CH_FOLD2), ch_out=int(128 * CH_FOLD2))\n",
    "        self.Conv4 = conv_block(ch_in=int(128 * CH_FOLD2), ch_out=int(256 * CH_FOLD2))\n",
    "        self.Conv5 = conv_block(ch_in=int(256 * CH_FOLD2), ch_out=int(512 * CH_FOLD2))\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=int(512 * CH_FOLD2), ch_out=int(256 * CH_FOLD2))\n",
    "        self.Up_conv5 = conv_block(\n",
    "            ch_in=int(512 * CH_FOLD2), ch_out=int(256 * CH_FOLD2)\n",
    "        )\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=int(256 * CH_FOLD2), ch_out=int(128 * CH_FOLD2))\n",
    "        self.Up_conv4 = conv_block(\n",
    "            ch_in=int(256 * CH_FOLD2), ch_out=int(128 * CH_FOLD2)\n",
    "        )\n",
    "\n",
    "        self.Up3 = up_conv(ch_in=int(128 * CH_FOLD2), ch_out=int(64 * CH_FOLD2))\n",
    "        self.Up_conv3 = conv_block(ch_in=int(128 * CH_FOLD2), ch_out=int(64 * CH_FOLD2))\n",
    "\n",
    "        self.Up2 = up_conv(ch_in=int(64 * CH_FOLD2), ch_out=int(32 * CH_FOLD2))\n",
    "        self.Up_conv2 = conv_block(ch_in=int(64 * CH_FOLD2), ch_out=int(32 * CH_FOLD2))\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(\n",
    "            int(32 * CH_FOLD2), output_ch, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        d1 = d1.squeeze(1)\n",
    "        out = torch.transpose(d1, -1, -2) * d1\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class UnetWrapper2D(nn.Module):\n",
    "    def __init__(self, md, output_chans=2):\n",
    "        super().__init__()\n",
    "        self.md = md\n",
    "        self.output_chans = output_chans\n",
    "\n",
    "    def do_forward(self, x, crop16, crop):\n",
    "        out = torch.zeros(\n",
    "            x.shape[0], self.output_chans, x.shape[2], x.shape[3], device=x.device\n",
    "        )\n",
    "        res = self.md(x[:, :, :crop16, :crop16])\n",
    "        out[:, :, :crop, :crop] = res[:, :, :crop, :crop]\n",
    "        return out\n",
    "\n",
    "    def forward(self, xs, crop_to_16, crop_original, original_order):\n",
    "        res = []\n",
    "        for x, crop16, crop in zip(xs, crop_to_16, crop_original):\n",
    "            res.append(self.do_forward(x, crop16, crop))\n",
    "        return torch.cat(res)[original_order]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_batches(tensor, nn_out):\n",
    "    # Sort the tensor\n",
    "    if tensor.unique().shape[0] == 1:\n",
    "        return [nn_out], torch.arange(len(tensor), device=tensor.device)\n",
    "    sorted_tensor, order = tensor.sort()\n",
    "    nn_out = nn_out.index_select(0, order)\n",
    "\n",
    "    # Find the change points\n",
    "    diff = torch.cat([torch.tensor([1]), torch.diff(sorted_tensor)])\n",
    "    change_indices = torch.where(diff != 0)[0]\n",
    "    change_indices = torch.cat([change_indices, torch.tensor([len(tensor)])])\n",
    "    b = [\n",
    "        nn_out[change_indices[i] : change_indices[i + 1]]\n",
    "        for i in range(len(change_indices) - 1)\n",
    "    ]\n",
    "    return b, order.argsort()\n",
    "\n",
    "\n",
    "def make_pair_mask(seq, seq_len):\n",
    "    encode_mask = torch.arange(seq.shape[1], device=seq.device).expand(\n",
    "        seq.shape[:2]\n",
    "    ) < seq_len.unsqueeze(1)\n",
    "    pair_mask = encode_mask[:, None, :] * encode_mask[:, :, None]\n",
    "    assert isinstance(pair_mask, torch.BoolTensor) or isinstance(\n",
    "        pair_mask, torch.cuda.BoolTensor\n",
    "    )\n",
    "    return torch.bitwise_not(pair_mask)\n",
    "\n",
    "\n",
    "class ScaledSinuEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(\n",
    "            torch.ones(\n",
    "                1,\n",
    "            )\n",
    "        )\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, device = x.shape[1], x.device\n",
    "        t = torch.arange(n, device=device).type_as(self.inv_freq)\n",
    "        sinu = einsum(\"i , j -> i j\", t, self.inv_freq)\n",
    "        emb = torch.cat((sinu.sin(), sinu.cos()), dim=-1)\n",
    "        return emb * self.scale\n",
    "\n",
    "\n",
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, dim, vocab=4):\n",
    "        super().__init__()\n",
    "        self.embed_seq = nn.Embedding(vocab, dim)\n",
    "        self.pos_enc = ScaledSinuEmbedding(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed_seq(x)\n",
    "        x = x + self.pos_enc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeqToImage(nn.Module):\n",
    "    def __init__(self, dim, vocab=4):\n",
    "        super().__init__()\n",
    "        self.embed_h = CustomEmbedding(dim=dim, vocab=vocab)\n",
    "        self.embed_w = CustomEmbedding(dim=dim, vocab=vocab)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, seq, mask):\n",
    "        seq_h = self.embed_h(seq)\n",
    "        seq_w = self.embed_w(seq)\n",
    "        x = seq_h.unsqueeze(1) + seq_w.unsqueeze(2)\n",
    "        x = self.norm(x)\n",
    "        x.masked_fill_(mask[:, :, :, None], 0.0)  # bs, h, w, dim\n",
    "        x = x.permute(0, 3, 1, 2)  # bs, dim, h, w\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attn_pool(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n, n, 1)\n",
    "        self.attn = nn.Conv2d(n, n, 1)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        emb = self.conv(x)\n",
    "        attn = self.attn(x)\n",
    "\n",
    "        # Apply the mask to attention scores before softmax\n",
    "        if key_padding_mask is not None:\n",
    "            attn = attn.masked_fill(key_padding_mask.unsqueeze(1), float(\"-inf\"))\n",
    "        # attn = torch.clamp(attn, min=-1e9, max=1e9)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (emb * attn).sum(-1)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class FeedForwardV5(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.2, out=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class RnaModelConvV0(nn.Module):\n",
    "    def __init__(self, embed_size, conv_out=8, vecob_size=4):\n",
    "        super().__init__()\n",
    "        self.seq_to_image = SeqToImage(embed_size, vocab=vecob_size)\n",
    "        self.md = UnetWrapper2D(U_Net(embed_size, conv_out), conv_out)\n",
    "        self.attnpool = Attn_pool(conv_out)\n",
    "        self.out = FeedForwardV5(conv_out, conv_out, out=2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        L_seq = batch[\"mask\"].sum(1)\n",
    "        L0 = batch[\"mask\"].shape[1]\n",
    "\n",
    "        crop_to_original = L_seq.unique()  # unique lengths\n",
    "        crop_to_16 = [\n",
    "            ((i // 16) + 1) * 16 for i in crop_to_original\n",
    "        ]  # for each unique length, find the nearest 16 miltip\n",
    "        seq = batch[\"seq\"][:, : crop_to_16[-1]]  # shortening to largest 16 multiple\n",
    "\n",
    "        # make a square mask [bs, crop_to_16[-1], crop_to_16[-1]]  #crop_to_16[-1] is the largest 16 multiple\n",
    "        square_mask = make_pair_mask(seq, L_seq)\n",
    "\n",
    "        x = self.seq_to_image(seq, square_mask)\n",
    "        x, idc = generate_batches(L_seq, x)\n",
    "        x = self.md(x, crop_to_16, crop_to_original, idc)\n",
    "        x, attn = self.attnpool(x, square_mask)\n",
    "        x = self.out(x.permute(0, 2, 1))\n",
    "        x = F.pad(x, (0, 0, 0, L0 - x.shape[1], 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(oup, int(inp // reduction)),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(int(inp // reduction), oup),\n",
    "            # Concater(Bilinear(int(inp // reduction), int(inp // reduction // 2), rank=0.5, bias=True)),\n",
    "            # nn.SiLU(),\n",
    "            # nn.Linear(int(inp // reduction) +  int(inp // reduction // 2), oup),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        (\n",
    "            b,\n",
    "            c,\n",
    "            _,\n",
    "        ) = x.size()\n",
    "        y = x.view(b, c, -1).mean(dim=2)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class Conv1D(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        if src_key_padding_mask is not None:\n",
    "            self.src_key_padding_mask = src_key_padding_mask\n",
    "        if self.src_key_padding_mask is not None:\n",
    "            x = torch.where(\n",
    "                self.src_key_padding_mask.unsqueeze(-1)\n",
    "                .expand(-1, -1, x.shape[-1])\n",
    "                .bool(),\n",
    "                torch.zeros_like(x),\n",
    "                x,\n",
    "            )\n",
    "\n",
    "        return super().forward(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Sequential):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__(\n",
    "            nn.LayerNorm(d_model), nn.GELU(), Conv1D(d_model, d_model, 3, padding=1)\n",
    "        )\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        self[-1].src_key_padding_mask = (\n",
    "            src_key_padding_mask\n",
    "            if src_key_padding_mask is not None\n",
    "            else self.src_key_padding_mask\n",
    "        )\n",
    "        return x + super().forward(x)\n",
    "\n",
    "\n",
    "class Extractor(nn.Sequential):\n",
    "    def __init__(self, d_model, in_ch=4):\n",
    "        super().__init__(\n",
    "            nn.Embedding(in_ch, d_model // 4),\n",
    "            Conv1D(d_model // 4, d_model, 7, padding=3),\n",
    "            ResBlock(d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        for i in [1, 2]:\n",
    "            self[i].src_key_padding_mask = src_key_padding_mask\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class LocalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, ks, activation, out_ch=None):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.ks = ks\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.in_ch,\n",
    "                out_channels=self.out_ch,\n",
    "                kernel_size=self.ks,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm1d(self.out_ch),\n",
    "            activation(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class EffBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch,\n",
    "        ks,\n",
    "        resize_factor,\n",
    "        filter_per_group,\n",
    "        activation,\n",
    "        out_ch=None,\n",
    "        se_reduction=None,\n",
    "        se_type=\"simple\",\n",
    "        inner_dim_calculation=\"out\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = resize_factor if se_reduction is None else se_reduction\n",
    "        self.ks = ks\n",
    "        self.inner_dim_calculation = inner_dim_calculation\n",
    "\n",
    "        \"\"\"\n",
    "        `in` refers to the original method of EfficientNetV2 to set the dimensionality of the EfficientNetV2-like block\n",
    "        `out` is the mode used in the original LegNet approach\n",
    "\n",
    "        This parameter slighly changes the mechanism of channel number calculation \n",
    "        which can be seen in the figure above (C, channel number is highlighted in red).\n",
    "        \"\"\"\n",
    "        if inner_dim_calculation == \"out\":\n",
    "            self.inner_dim = self.out_ch * self.resize_factor\n",
    "        elif inner_dim_calculation == \"in\":\n",
    "            self.inner_dim = self.in_ch * self.resize_factor\n",
    "        else:\n",
    "            raise Exception(f\"Wrong inner_dim_calculation: {inner_dim_calculation}\")\n",
    "\n",
    "        self.filter_per_group = filter_per_group\n",
    "\n",
    "        se_constructor = SELayer\n",
    "\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.in_ch,\n",
    "                out_channels=self.inner_dim,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm1d(self.inner_dim),\n",
    "            activation(),\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.inner_dim,\n",
    "                out_channels=self.inner_dim,\n",
    "                kernel_size=ks,\n",
    "                groups=self.inner_dim // self.filter_per_group,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm1d(self.inner_dim),\n",
    "            activation(),\n",
    "            se_constructor(\n",
    "                self.in_ch, self.inner_dim, reduction=self.se_reduction\n",
    "            ),  # self.in_ch is not good\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.inner_dim,\n",
    "                out_channels=self.in_ch,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm1d(self.in_ch),\n",
    "            activation(),\n",
    "        )\n",
    "\n",
    "        self.block = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The `activation()` in the optimized architecture simply equals `nn.Identity`\n",
    "In the original LegNet approach it was `nn.SiLU`\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MappingBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=in_ch,\n",
    "                out_channels=out_ch,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "            ),\n",
    "            activation(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ResidualConcat(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return torch.concat([self.fn(x, **kwargs), x], dim=1)\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "class LegNet(nn.Module):\n",
    "    __constants__ = \"resize_factor\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        use_single_channel: bool,\n",
    "        use_reverse_channel: bool,\n",
    "        block_sizes: list[int] = [256, 128, 128, 64, 64, 64, 64],\n",
    "        ks: int = 7,\n",
    "        resize_factor: int = 4,\n",
    "        activation: Type[nn.Module] = nn.SiLU,\n",
    "        final_activation: Type[nn.Module] = nn.Identity,\n",
    "        filter_per_group: int = 1,\n",
    "        se_reduction: int = 4,\n",
    "        res_block_type: str = \"concat\",\n",
    "        se_type: str = \"simple\",\n",
    "        inner_dim_calculation: str = \"in\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.block_sizes = block_sizes\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = se_reduction\n",
    "        self.use_single_channel = use_single_channel\n",
    "        self.use_reverse_channel = use_reverse_channel\n",
    "        self.filter_per_group = filter_per_group\n",
    "        self.final_ch = 2  # number of bins in the competition\n",
    "        self.inner_dim_calculation = inner_dim_calculation\n",
    "        self.res_block_type = res_block_type\n",
    "\n",
    "        residual = ResidualConcat\n",
    "\n",
    "        self.stem_block = LocalBlock(\n",
    "            in_ch=self.in_channels, out_ch=block_sizes[0], ks=ks, activation=activation\n",
    "        )\n",
    "\n",
    "        blocks = []\n",
    "        for ind, (prev_sz, sz) in enumerate(zip(block_sizes[:-1], block_sizes[1:])):\n",
    "            block = nn.Sequential(\n",
    "                residual(\n",
    "                    EffBlock(\n",
    "                        in_ch=prev_sz,\n",
    "                        out_ch=sz,\n",
    "                        ks=ks,\n",
    "                        resize_factor=4,\n",
    "                        activation=activation,\n",
    "                        filter_per_group=self.filter_per_group,\n",
    "                        se_type=se_type,\n",
    "                        inner_dim_calculation=inner_dim_calculation,\n",
    "                    )\n",
    "                ),\n",
    "                LocalBlock(in_ch=2 * prev_sz, out_ch=sz, ks=ks, activation=activation),\n",
    "            )\n",
    "            blocks.append(block)\n",
    "\n",
    "        self.main = nn.Sequential(*blocks)\n",
    "\n",
    "        self.mapper = MappingBlock(\n",
    "            in_ch=block_sizes[-1], out_ch=self.final_ch, activation=final_activation\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def in_channels(self) -> int:\n",
    "        return self.input_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.stem_block(x)\n",
    "        x = self.main(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RnaModelConvV1(nn.Module):\n",
    "    def __init__(self, dim=192, block_sizes=[256, 128, 128, 64, 64, 64, 64]):\n",
    "        super().__init__()\n",
    "        self.extractor = Extractor(dim)\n",
    "        self.cnn_model = LegNet(\n",
    "            input_size=dim,\n",
    "            use_single_channel=False,\n",
    "            use_reverse_channel=False,\n",
    "            block_sizes=block_sizes,\n",
    "        )\n",
    "        self.proj_out = nn.Sequential(nn.Linear(block_sizes[-1], 2))\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        x = self.cnn_model(x)\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class EffBlockV2(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch,\n",
    "        ks,\n",
    "        resize_factor,\n",
    "        filter_per_group,\n",
    "        activation,\n",
    "        out_ch=None,\n",
    "        se_reduction=None,\n",
    "        se_type=\"simple\",\n",
    "        inner_dim_calculation=\"out\",\n",
    "    ):\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = resize_factor if se_reduction is None else se_reduction\n",
    "        self.ks = ks\n",
    "        self.inner_dim_calculation = inner_dim_calculation\n",
    "\n",
    "        if inner_dim_calculation == \"out\":\n",
    "            self.inner_dim = self.out_ch * self.resize_factor\n",
    "        elif inner_dim_calculation == \"in\":\n",
    "            self.inner_dim = self.in_ch * self.resize_factor\n",
    "        else:\n",
    "            raise Exception(f\"Wrong inner_dim_calculation: {inner_dim_calculation}\")\n",
    "\n",
    "        self.filter_per_group = filter_per_group\n",
    "\n",
    "        super().__init__(\n",
    "            Conv1D(\n",
    "                in_channels=self.in_ch,\n",
    "                out_channels=self.inner_dim,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.LayerNorm(self.inner_dim),\n",
    "            activation(),\n",
    "            Conv1D(\n",
    "                in_channels=self.inner_dim,\n",
    "                out_channels=self.inner_dim,\n",
    "                kernel_size=ks,\n",
    "                groups=self.inner_dim // self.filter_per_group,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.LayerNorm(self.inner_dim),\n",
    "            activation(),\n",
    "            Conv1D(\n",
    "                in_channels=self.inner_dim,\n",
    "                out_channels=self.in_ch,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.LayerNorm(self.in_ch),\n",
    "            activation(),\n",
    "        )\n",
    "\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        for i in [0, 3, 6]:\n",
    "            self[i].src_key_padding_mask = src_key_padding_mask\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class LocalBlockV2(nn.Sequential):\n",
    "    def __init__(self, in_ch, ks, activation, out_ch=None):\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.ks = ks\n",
    "\n",
    "        super().__init__(\n",
    "            Conv1D(\n",
    "                in_channels=self.in_ch,\n",
    "                out_channels=self.out_ch,\n",
    "                kernel_size=self.ks,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.LayerNorm(self.out_ch),\n",
    "            activation(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        for i in [0]:\n",
    "            self[i].src_key_padding_mask = src_key_padding_mask\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class ConvolutionConcatBlockV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch=256,\n",
    "        ks=7,\n",
    "        resize_factor=4,\n",
    "        filter_per_group=1,\n",
    "        activation=nn.GELU,\n",
    "        out_ch=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.effblock = EffBlockV2(\n",
    "            in_ch=in_ch,\n",
    "            ks=ks,\n",
    "            resize_factor=resize_factor,\n",
    "            filter_per_group=filter_per_group,\n",
    "            activation=activation,\n",
    "            out_ch=out_ch,\n",
    "        )\n",
    "\n",
    "        self.localblock = LocalBlockV2(\n",
    "            in_ch=in_ch * 2, ks=ks, activation=activation, out_ch=out_ch\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        res = x\n",
    "        x = self.effblock(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = torch.cat([x, res], dim=-1)\n",
    "        x = self.localblock(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomConvdV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        block_sizes: list[int] = [256, 128, 128, 64, 64, 64, 64],\n",
    "        ks: int = 7,\n",
    "        resize_factor: int = 4,\n",
    "        activation: Type[nn.Module] = nn.SiLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem_block = LocalBlockV2(\n",
    "            in_ch=dim, out_ch=block_sizes[0], ks=ks, activation=activation\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for ind, (prev_sz, sz) in enumerate(zip(block_sizes[:-1], block_sizes[1:])):\n",
    "            block = ConvolutionConcatBlockV2(\n",
    "                in_ch=prev_sz,\n",
    "                out_ch=sz,\n",
    "                ks=ks,\n",
    "                resize_factor=resize_factor,\n",
    "                activation=activation,\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        x = self.stem_block(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RnaModelConvV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=192,\n",
    "        block_sizes=[256, 128, 128, 64, 64, 64, 64],\n",
    "        resize_factor=4,\n",
    "        ks=7,\n",
    "        activation=nn.GELU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.extractor = Extractor(dim)\n",
    "        self.cnn_model = CustomConvdV2(\n",
    "            dim=dim,\n",
    "            block_sizes=block_sizes,\n",
    "            resize_factor=resize_factor,\n",
    "            ks=ks,\n",
    "            activation=activation,\n",
    "        )\n",
    "        self.proj_out = nn.Sequential(nn.Linear(block_sizes[-1], 2))\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        x = self.cnn_model(x, src_key_padding_mask=~mask)\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 16\n",
    "out_chan_unet = 32\n",
    "md = RnaModelConvV2(dim=192*2).eval()\n",
    "batch = torch.load(\"batch.pt\")\n",
    "with torch.no_grad():\n",
    "    out = md(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
