{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp modelsconv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#| export\n",
    "import sys\n",
    "sys.path.append('/opt/slh/rna/')\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "from rotary_embedding_torch import RotaryEmbedding, apply_rotary_emb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import math\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.data import Data, Batch\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, TransformerWrapper\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "from rnacomp.models import CombinationTransformerEncoderV1, Block_conv, CombinationTransformerEncoderV29\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, TransformerWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def good_luck():\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, img_ch=3, output_ch=1, CH_FOLD2=1):\n",
    "        super(U_Net, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch, ch_out=int(32 * CH_FOLD2))\n",
    "        self.Conv2 = conv_block(ch_in=int(32 * CH_FOLD2), ch_out=int(64 * CH_FOLD2))\n",
    "        self.Conv3 = conv_block(ch_in=int(64 * CH_FOLD2), ch_out=int(128 * CH_FOLD2))\n",
    "        self.Conv4 = conv_block(ch_in=int(128 * CH_FOLD2), ch_out=int(256 * CH_FOLD2))\n",
    "        self.Conv5 = conv_block(ch_in=int(256 * CH_FOLD2), ch_out=int(512 * CH_FOLD2))\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=int(512 * CH_FOLD2), ch_out=int(256 * CH_FOLD2))\n",
    "        self.Up_conv5 = conv_block(\n",
    "            ch_in=int(512 * CH_FOLD2), ch_out=int(256 * CH_FOLD2)\n",
    "        )\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=int(256 * CH_FOLD2), ch_out=int(128 * CH_FOLD2))\n",
    "        self.Up_conv4 = conv_block(\n",
    "            ch_in=int(256 * CH_FOLD2), ch_out=int(128 * CH_FOLD2)\n",
    "        )\n",
    "\n",
    "        self.Up3 = up_conv(ch_in=int(128 * CH_FOLD2), ch_out=int(64 * CH_FOLD2))\n",
    "        self.Up_conv3 = conv_block(ch_in=int(128 * CH_FOLD2), ch_out=int(64 * CH_FOLD2))\n",
    "\n",
    "        self.Up2 = up_conv(ch_in=int(64 * CH_FOLD2), ch_out=int(32 * CH_FOLD2))\n",
    "        self.Up_conv2 = conv_block(ch_in=int(64 * CH_FOLD2), ch_out=int(32 * CH_FOLD2))\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(\n",
    "            int(32 * CH_FOLD2), output_ch, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        d1 = d1.squeeze(1)\n",
    "        out = torch.transpose(d1, -1, -2) * d1\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class UnetWrapper2D(nn.Module):\n",
    "    def __init__(self, md, output_chans=2):\n",
    "        super().__init__()\n",
    "        self.md = md\n",
    "        self.output_chans = output_chans\n",
    "\n",
    "    def do_forward(self, x, crop16, crop):\n",
    "        out = torch.zeros(\n",
    "            x.shape[0], self.output_chans, x.shape[2], x.shape[3], device=x.device\n",
    "        )\n",
    "        res = self.md(x[:, :, :crop16, :crop16])\n",
    "        out[:, :, :crop, :crop] = res[:, :, :crop, :crop]\n",
    "        return out\n",
    "\n",
    "    def forward(self, xs, crop_to_16, crop_original, original_order):\n",
    "        res = []\n",
    "        for x, crop16, crop in zip(xs, crop_to_16, crop_original):\n",
    "            res.append(self.do_forward(x, crop16, crop))\n",
    "        return torch.cat(res)[original_order]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_batches(tensor, nn_out):\n",
    "    # Sort the tensor\n",
    "    if tensor.unique().shape[0] == 1:\n",
    "        return [nn_out], torch.arange(len(tensor), device=tensor.device)\n",
    "    sorted_tensor, order = tensor.sort()\n",
    "    nn_out = nn_out.index_select(0, order)\n",
    "\n",
    "    # Find the change points\n",
    "    diff = torch.cat([torch.tensor([1]), torch.diff(sorted_tensor)])\n",
    "    change_indices = torch.where(diff != 0)[0]\n",
    "    change_indices = torch.cat([change_indices, torch.tensor([len(tensor)])])\n",
    "    b = [\n",
    "        nn_out[change_indices[i] : change_indices[i + 1]]\n",
    "        for i in range(len(change_indices) - 1)\n",
    "    ]\n",
    "    return b, order.argsort()\n",
    "\n",
    "\n",
    "def make_pair_mask(seq, seq_len):\n",
    "    encode_mask = torch.arange(seq.shape[1], device=seq.device).expand(\n",
    "        seq.shape[:2]\n",
    "    ) < seq_len.unsqueeze(1)\n",
    "    pair_mask = encode_mask[:, None, :] * encode_mask[:, :, None]\n",
    "    assert isinstance(pair_mask, torch.BoolTensor) or isinstance(\n",
    "        pair_mask, torch.cuda.BoolTensor\n",
    "    )\n",
    "    return torch.bitwise_not(pair_mask)\n",
    "\n",
    "\n",
    "class ScaledSinuEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(\n",
    "            torch.ones(\n",
    "                1,\n",
    "            )\n",
    "        )\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, device = x.shape[1], x.device\n",
    "        t = torch.arange(n, device=device).type_as(self.inv_freq)\n",
    "        sinu = einsum(\"i , j -> i j\", t, self.inv_freq)\n",
    "        emb = torch.cat((sinu.sin(), sinu.cos()), dim=-1)\n",
    "        return emb * self.scale\n",
    "\n",
    "\n",
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, dim, vocab=4):\n",
    "        super().__init__()\n",
    "        self.embed_seq = nn.Embedding(vocab, dim)\n",
    "        self.pos_enc = ScaledSinuEmbedding(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed_seq(x)\n",
    "        x = x + self.pos_enc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeqToImage(nn.Module):\n",
    "    def __init__(self, dim, vocab=4):\n",
    "        super().__init__()\n",
    "        self.embed_h = CustomEmbedding(dim=dim, vocab=vocab)\n",
    "        self.embed_w = CustomEmbedding(dim=dim, vocab=vocab)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, seq, mask):\n",
    "        seq_h = self.embed_h(seq)\n",
    "        seq_w = self.embed_w(seq)\n",
    "        x = seq_h.unsqueeze(1) + seq_w.unsqueeze(2)\n",
    "        x = self.norm(x)\n",
    "        x.masked_fill_(mask[:, :, :, None], 0.0)  # bs, h, w, dim\n",
    "        x = x.permute(0, 3, 1, 2)  # bs, dim, h, w\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attn_pool(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n, n, 1)\n",
    "        self.attn = nn.Conv2d(n, n, 1)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        emb = self.conv(x)\n",
    "        attn = self.attn(x)\n",
    "\n",
    "        # Apply the mask to attention scores before softmax\n",
    "        if key_padding_mask is not None:\n",
    "            attn = attn.masked_fill(key_padding_mask.unsqueeze(1), float(\"-inf\"))\n",
    "        # attn = torch.clamp(attn, min=-1e9, max=1e9)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (emb * attn).sum(-1)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class FeedForwardV5(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.2, out=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class RnaModelConvV0(nn.Module):\n",
    "    def __init__(self, embed_size, conv_out=8, vecob_size=4):\n",
    "        super().__init__()\n",
    "        self.seq_to_image = SeqToImage(embed_size, vocab=vecob_size)\n",
    "        self.md = UnetWrapper2D(U_Net(embed_size, conv_out), conv_out)\n",
    "        self.attnpool = Attn_pool(conv_out)\n",
    "        self.out = FeedForwardV5(conv_out, conv_out, out=2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        L_seq = batch[\"mask\"].sum(1)\n",
    "        L0 = batch[\"mask\"].shape[1]\n",
    "\n",
    "        crop_to_original = L_seq.unique()  # unique lengths\n",
    "        crop_to_16 = [\n",
    "            ((i // 16) + 1) * 16 for i in crop_to_original\n",
    "        ]  # for each unique length, find the nearest 16 miltip\n",
    "        seq = batch[\"seq\"][:, : crop_to_16[-1]]  # shortening to largest 16 multiple\n",
    "\n",
    "        # make a square mask [bs, crop_to_16[-1], crop_to_16[-1]]  #crop_to_16[-1] is the largest 16 multiple\n",
    "        square_mask = make_pair_mask(seq, L_seq)\n",
    "\n",
    "        x = self.seq_to_image(seq, square_mask)\n",
    "        x, idc = generate_batches(L_seq, x)\n",
    "        x = self.md(x, crop_to_16, crop_to_original, idc)\n",
    "        x, attn = self.attnpool(x, square_mask)\n",
    "        x = self.out(x.permute(0, 2, 1))\n",
    "        x = F.pad(x, (0, 0, 0, L0 - x.shape[1], 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(oup, int(inp // reduction)),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(int(inp // reduction), oup),\n",
    "            # Concater(Bilinear(int(inp // reduction), int(inp // reduction // 2), rank=0.5, bias=True)),\n",
    "            # nn.SiLU(),\n",
    "            # nn.Linear(int(inp // reduction) +  int(inp // reduction // 2), oup),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        (\n",
    "            b,\n",
    "            c,\n",
    "            _,\n",
    "        ) = x.size()\n",
    "        y = x.view(b, c, -1).mean(dim=2)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class Conv1D(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        if src_key_padding_mask is not None:\n",
    "            self.src_key_padding_mask = src_key_padding_mask\n",
    "        if self.src_key_padding_mask is not None:\n",
    "            x = torch.where(\n",
    "                self.src_key_padding_mask.unsqueeze(-1)\n",
    "                .expand(-1, -1, x.shape[-1])\n",
    "                .bool(),\n",
    "                torch.zeros_like(x),\n",
    "                x,\n",
    "            )\n",
    "\n",
    "        return super().forward(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Sequential):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__(\n",
    "            nn.LayerNorm(d_model), nn.GELU(), Conv1D(d_model, d_model, 3, padding=1)\n",
    "        )\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        self[-1].src_key_padding_mask = (\n",
    "            src_key_padding_mask\n",
    "            if src_key_padding_mask is not None\n",
    "            else self.src_key_padding_mask\n",
    "        )\n",
    "        return x + super().forward(x)\n",
    "\n",
    "\n",
    "class Extractor(nn.Sequential):\n",
    "    def __init__(self, d_model, in_ch=4):\n",
    "        super().__init__(\n",
    "            nn.Embedding(in_ch, d_model // 4),\n",
    "            Conv1D(d_model // 4, d_model, 7, padding=3),\n",
    "            ResBlock(d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        for i in [1, 2]:\n",
    "            self[i].src_key_padding_mask = src_key_padding_mask\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class LocalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, ks, activation, out_ch=None):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.ks = ks\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.in_ch,\n",
    "                out_channels=self.out_ch,\n",
    "                kernel_size=self.ks,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm1d(self.out_ch),\n",
    "            activation(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class EffBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch,\n",
    "        ks,\n",
    "        resize_factor,\n",
    "        filter_per_group,\n",
    "        activation,\n",
    "        out_ch=None,\n",
    "        se_reduction=None,\n",
    "        se_type=\"simple\",\n",
    "        inner_dim_calculation=\"out\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = resize_factor if se_reduction is None else se_reduction\n",
    "        self.ks = ks\n",
    "        self.inner_dim_calculation = inner_dim_calculation\n",
    "\n",
    "        \"\"\"\n",
    "        `in` refers to the original method of EfficientNetV2 to set the dimensionality of the EfficientNetV2-like block\n",
    "        `out` is the mode used in the original LegNet approach\n",
    "\n",
    "        This parameter slighly changes the mechanism of channel number calculation \n",
    "        which can be seen in the figure above (C, channel number is highlighted in red).\n",
    "        \"\"\"\n",
    "        if inner_dim_calculation == \"out\":\n",
    "            self.inner_dim = self.out_ch * self.resize_factor\n",
    "        elif inner_dim_calculation == \"in\":\n",
    "            self.inner_dim = self.in_ch * self.resize_factor\n",
    "        else:\n",
    "            raise Exception(f\"Wrong inner_dim_calculation: {inner_dim_calculation}\")\n",
    "\n",
    "        self.filter_per_group = filter_per_group\n",
    "\n",
    "        se_constructor = SELayer\n",
    "\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.in_ch,\n",
    "                out_channels=self.inner_dim,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm1d(self.inner_dim),\n",
    "            activation(),\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.inner_dim,\n",
    "                out_channels=self.inner_dim,\n",
    "                kernel_size=ks,\n",
    "                groups=self.inner_dim // self.filter_per_group,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm1d(self.inner_dim),\n",
    "            activation(),\n",
    "            se_constructor(\n",
    "                self.in_ch, self.inner_dim, reduction=self.se_reduction\n",
    "            ),  # self.in_ch is not good\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.inner_dim,\n",
    "                out_channels=self.in_ch,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm1d(self.in_ch),\n",
    "            activation(),\n",
    "        )\n",
    "\n",
    "        self.block = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The `activation()` in the optimized architecture simply equals `nn.Identity`\n",
    "In the original LegNet approach it was `nn.SiLU`\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MappingBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=in_ch,\n",
    "                out_channels=out_ch,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "            ),\n",
    "            activation(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ResidualConcat(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return torch.concat([self.fn(x, **kwargs), x], dim=1)\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "class LegNet(nn.Module):\n",
    "    __constants__ = \"resize_factor\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        use_single_channel: bool,\n",
    "        use_reverse_channel: bool,\n",
    "        block_sizes: list[int] = [256, 128, 128, 64, 64, 64, 64],\n",
    "        ks: int = 7,\n",
    "        resize_factor: int = 4,\n",
    "        activation: Type[nn.Module] = nn.SiLU,\n",
    "        final_activation: Type[nn.Module] = nn.Identity,\n",
    "        filter_per_group: int = 1,\n",
    "        se_reduction: int = 4,\n",
    "        res_block_type: str = \"concat\",\n",
    "        se_type: str = \"simple\",\n",
    "        inner_dim_calculation: str = \"in\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.block_sizes = block_sizes\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = se_reduction\n",
    "        self.use_single_channel = use_single_channel\n",
    "        self.use_reverse_channel = use_reverse_channel\n",
    "        self.filter_per_group = filter_per_group\n",
    "        self.final_ch = 2  # number of bins in the competition\n",
    "        self.inner_dim_calculation = inner_dim_calculation\n",
    "        self.res_block_type = res_block_type\n",
    "\n",
    "        residual = ResidualConcat\n",
    "\n",
    "        self.stem_block = LocalBlock(\n",
    "            in_ch=self.in_channels, out_ch=block_sizes[0], ks=ks, activation=activation\n",
    "        )\n",
    "\n",
    "        blocks = []\n",
    "        for ind, (prev_sz, sz) in enumerate(zip(block_sizes[:-1], block_sizes[1:])):\n",
    "            block = nn.Sequential(\n",
    "                residual(\n",
    "                    EffBlock(\n",
    "                        in_ch=prev_sz,\n",
    "                        out_ch=sz,\n",
    "                        ks=ks,\n",
    "                        resize_factor=4,\n",
    "                        activation=activation,\n",
    "                        filter_per_group=self.filter_per_group,\n",
    "                        se_type=se_type,\n",
    "                        inner_dim_calculation=inner_dim_calculation,\n",
    "                    )\n",
    "                ),\n",
    "                LocalBlock(in_ch=2 * prev_sz, out_ch=sz, ks=ks, activation=activation),\n",
    "            )\n",
    "            blocks.append(block)\n",
    "\n",
    "        self.main = nn.Sequential(*blocks)\n",
    "\n",
    "        self.mapper = MappingBlock(\n",
    "            in_ch=block_sizes[-1], out_ch=self.final_ch, activation=final_activation\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def in_channels(self) -> int:\n",
    "        return self.input_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.stem_block(x)\n",
    "        x = self.main(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RnaModelConvV1(nn.Module):\n",
    "    def __init__(self, dim=192, block_sizes=[256, 128, 128, 64, 64, 64, 64]):\n",
    "        super().__init__()\n",
    "        self.extractor = Extractor(dim)\n",
    "        self.cnn_model = LegNet(\n",
    "            input_size=dim,\n",
    "            use_single_channel=False,\n",
    "            use_reverse_channel=False,\n",
    "            block_sizes=block_sizes,\n",
    "        )\n",
    "        self.proj_out = nn.Sequential(nn.Linear(block_sizes[-1], 2))\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        x = self.cnn_model(x)\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class EffBlockV2(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch,\n",
    "        ks,\n",
    "        resize_factor,\n",
    "        filter_per_group,\n",
    "        activation,\n",
    "        out_ch=None,\n",
    "        se_reduction=None,\n",
    "        se_type=\"simple\",\n",
    "        inner_dim_calculation=\"out\",\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = resize_factor if se_reduction is None else se_reduction\n",
    "        self.ks = ks\n",
    "        self.inner_dim_calculation = inner_dim_calculation\n",
    "\n",
    "        if inner_dim_calculation == \"out\":\n",
    "            self.inner_dim = self.out_ch * self.resize_factor\n",
    "        elif inner_dim_calculation == \"in\":\n",
    "            self.inner_dim = self.in_ch * self.resize_factor\n",
    "        else:\n",
    "            raise Exception(f\"Wrong inner_dim_calculation: {inner_dim_calculation}\")\n",
    "\n",
    "        self.filter_per_group = filter_per_group\n",
    "\n",
    "        super().__init__(\n",
    "            Conv1D(\n",
    "                in_channels=self.in_ch,\n",
    "                out_channels=self.inner_dim,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.LayerNorm(self.inner_dim),\n",
    "            activation(),\n",
    "            nn.Dropout(dropout),\n",
    "            Conv1D(\n",
    "                in_channels=self.inner_dim,\n",
    "                out_channels=self.inner_dim,\n",
    "                kernel_size=ks,\n",
    "                groups=self.inner_dim // self.filter_per_group,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.LayerNorm(self.inner_dim),\n",
    "            activation(),\n",
    "            nn.Dropout(dropout),\n",
    "            Conv1D(\n",
    "                in_channels=self.inner_dim,\n",
    "                out_channels=self.in_ch,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.LayerNorm(self.in_ch),\n",
    "            activation(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        for i in [0, 3, 6]:\n",
    "            self[i].src_key_padding_mask = src_key_padding_mask\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class LocalBlockV2(nn.Sequential):\n",
    "    def __init__(self, in_ch, ks, activation, dropout=0.2, out_ch=None):\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.ks = ks\n",
    "\n",
    "        super().__init__(\n",
    "            Conv1D(\n",
    "                in_channels=self.in_ch,\n",
    "                out_channels=self.out_ch,\n",
    "                kernel_size=self.ks,\n",
    "                padding=\"same\",\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.LayerNorm(self.out_ch),\n",
    "            activation(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        for i in [0]:\n",
    "            self[i].src_key_padding_mask = src_key_padding_mask\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class ConvolutionConcatBlockV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch=256,\n",
    "        ks=7,\n",
    "        resize_factor=4,\n",
    "        filter_per_group=1,\n",
    "        activation=nn.GELU,\n",
    "        out_ch=None,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.effblock = EffBlockV2(\n",
    "            in_ch=in_ch,\n",
    "            ks=ks,\n",
    "            resize_factor=resize_factor,\n",
    "            filter_per_group=filter_per_group,\n",
    "            activation=activation,\n",
    "            out_ch=out_ch,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.localblock = LocalBlockV2(\n",
    "            in_ch=in_ch * 2,\n",
    "            ks=ks,\n",
    "            activation=activation,\n",
    "            out_ch=out_ch,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        res = x\n",
    "        x = self.effblock(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = torch.cat([x, res], dim=-1)\n",
    "        x = self.localblock(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomConvdV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        block_sizes: list[int] = [256, 128, 128, 64, 64, 64, 64],\n",
    "        ks: int = 7,\n",
    "        resize_factor: int = 4,\n",
    "        activation: Type[nn.Module] = nn.SiLU,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem_block = LocalBlockV2(\n",
    "            in_ch=dim, out_ch=block_sizes[0], ks=ks, activation=activation\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for ind, (prev_sz, sz) in enumerate(zip(block_sizes[:-1], block_sizes[1:])):\n",
    "            block = ConvolutionConcatBlockV2(\n",
    "                in_ch=prev_sz,\n",
    "                out_ch=sz,\n",
    "                ks=ks,\n",
    "                resize_factor=resize_factor,\n",
    "                activation=activation,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        x = self.stem_block(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RnaModelConvV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=192,\n",
    "        resize_factor=4,\n",
    "        ks=7,\n",
    "        activation=nn.SiLU,\n",
    "        head_size=32,\n",
    "        drop_pat_dropout=0.2,\n",
    "        dropout=0.2,\n",
    "        bpp_transfomer_depth=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        block_sizes = [dim, dim, dim, dim, dim]\n",
    "        self.extractor = Extractor(dim)\n",
    "        self.cnn_model = CustomConvdV2(\n",
    "            dim=dim,\n",
    "            block_sizes=block_sizes,\n",
    "            resize_factor=resize_factor,\n",
    "            ks=ks,\n",
    "            activation=activation,\n",
    "            dropout=0.0,\n",
    "        )\n",
    "\n",
    "        self.bb_comb_blocks = nn.ModuleList(\n",
    "            [\n",
    "                CombinationTransformerEncoderV1(\n",
    "                    dim // 2,\n",
    "                    head_size=head_size,\n",
    "                    dropout=dropout,\n",
    "                    drop_path=drop_pat_dropout * (i / (bpp_transfomer_depth - 1)),\n",
    "                )\n",
    "                for i in range(bpp_transfomer_depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.proj_out = nn.Linear(dim // 2, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "\n",
    "        bpp = x0[\"bb_matrix_full_prob\"][:, :Lmax, :Lmax]\n",
    "        bpp_extra = x0[\"bb_matrix_full_prob_extra\"][:, :Lmax, :Lmax].float()\n",
    "        ss = x0[\"ss_adj\"][:, :Lmax, :Lmax].float()\n",
    "\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        x = self.cnn_model(x, src_key_padding_mask=~mask)\n",
    "\n",
    "        for i, blk in enumerate(self.bb_comb_blocks):\n",
    "            x = blk(x, bpp, bpp_extra, ss, mask)\n",
    "\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvolutionConcatBlockV3(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch=256,\n",
    "        ks=7,\n",
    "        resize_factor=4,\n",
    "        filter_per_group=1,\n",
    "        activation=nn.GELU,\n",
    "        out_ch=None,\n",
    "        dropout=0.2,\n",
    "        head_size=32,\n",
    "        dropout_trasnfomer=0.2,\n",
    "        drop_path=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.effblock = EffBlock(\n",
    "            in_ch=in_ch,\n",
    "            out_ch=out_ch,\n",
    "            ks=ks,\n",
    "            resize_factor=4,\n",
    "            activation=activation,\n",
    "            filter_per_group=filter_per_group,\n",
    "            se_type=\"simple\",\n",
    "            inner_dim_calculation=\"out\",\n",
    "        )\n",
    "        self.t_block = CombinationTransformerEncoderV1(\n",
    "            in_ch,\n",
    "            head_size=head_size,\n",
    "            dropout=dropout_trasnfomer,\n",
    "            drop_path=drop_path,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, bpp, bpp_extra, ss, mask):\n",
    "        res = x\n",
    "        x = self.effblock(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.t_block(x, bpp, bpp_extra, ss, mask)\n",
    "        return x + res\n",
    "\n",
    "\n",
    "# class RnaModelConvV3(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         dim=192,\n",
    "#         resize_factor=4,\n",
    "#         ks=7,\n",
    "#         activation=nn.SiLU,\n",
    "#         head_size=32,\n",
    "#         drop_pat_dropout=0.2,\n",
    "#         dropout=0.2,\n",
    "#         transformer_depth=6,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         block_sizes = [dim, dim, dim, dim, dim]\n",
    "#         self.extractor = Extractor(dim)\n",
    "\n",
    "#         self.blocks = nn.ModuleList()\n",
    "#         for ind, (prev_sz, sz) in enumerate(zip(block_sizes[:-1], block_sizes[1:])):\n",
    "#             block = ConvolutionConcatBlockV3(\n",
    "#                 in_ch=prev_sz,\n",
    "#                 out_ch=sz,\n",
    "#                 ks=ks,\n",
    "#                 resize_factor=resize_factor,\n",
    "#                 activation=activation,\n",
    "#                 dropout=dropout,\n",
    "#                 head_size=32,\n",
    "#                 dropout_trasnfomer=0.2,\n",
    "#                 drop_path=drop_pat_dropout * (ind / (len(block_sizes) - 1)),\n",
    "#             )\n",
    "#             self.blocks.append(block)\n",
    "\n",
    "#         self.enc = ContinuousTransformerWrapper(\n",
    "#             dim_in=dim * 2,\n",
    "#             dim_out=2,\n",
    "#             max_seq_len=512,\n",
    "#             attn_layers=Encoder(\n",
    "#                 dim=dim,\n",
    "#                 depth=transformer_depth,\n",
    "#                 attn_flash=True,\n",
    "#                 rotary_pos_emb=True,\n",
    "#                 attn_gate_values=True,\n",
    "#                 attn_head_scale=True,\n",
    "#                 ff_post_act_ln=True,\n",
    "#                 attn_qk_norm=True,\n",
    "#                 attn_qk_norm_dim_scale=True,\n",
    "#                 layer_dropout=drop_pat_dropout,  # stochastic depth - dropout entire layer\n",
    "#                 attn_dropout=dropout,  # dropout post-attention\n",
    "#             ),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x0):\n",
    "#         mask = x0[\"mask\"]\n",
    "#         L0 = mask.shape[1]\n",
    "#         Lmax = mask.sum(-1).max()\n",
    "#         mask = mask[:, :Lmax]\n",
    "#         x = x0[\"seq\"][:, :Lmax]\n",
    "\n",
    "#         bpp = x0[\"bb_matrix_full_prob\"][:, :Lmax, :Lmax]\n",
    "#         bpp_extra = x0[\"bb_matrix_full_prob_extra\"][:, :Lmax, :Lmax].float()\n",
    "#         ss = x0[\"ss_adj\"][:, :Lmax, :Lmax].float()\n",
    "\n",
    "#         x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "#         res = x\n",
    "#         for i, blk in enumerate(self.blocks):\n",
    "#             x = blk(x, bpp, bpp_extra, ss, mask)\n",
    "#         x = torch.concat([x, res], -1)\n",
    "#         x = self.enc(x, mask=mask)\n",
    "#         x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "#         return x\n",
    "\n",
    "class RnaModelConvV3(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=192,\n",
    "        resize_factor=4,\n",
    "        ks=7,\n",
    "        activation=nn.SiLU,\n",
    "        head_size=32,\n",
    "        drop_pat_dropout=0.2,\n",
    "        dropout=0.2,\n",
    "        transformer_depth=6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        block_sizes = [dim, dim, dim, dim, dim]\n",
    "        self.extractor = Extractor(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for ind, (prev_sz, sz) in enumerate(zip(block_sizes[:-1], block_sizes[1:])):\n",
    "            block = ConvolutionConcatBlockV3(\n",
    "                in_ch=prev_sz,\n",
    "                out_ch=sz,\n",
    "                ks=ks,\n",
    "                resize_factor=resize_factor,\n",
    "                activation=activation,\n",
    "                dropout=dropout,\n",
    "                head_size=32,\n",
    "                dropout_trasnfomer=0.2,\n",
    "                drop_path=drop_pat_dropout * (ind / (len(block_sizes) - 1)),\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.enc = ContinuousTransformerWrapper(\n",
    "            dim_in=dim,\n",
    "            dim_out=2,\n",
    "            max_seq_len=512,\n",
    "            attn_layers=Encoder(\n",
    "                dim=dim,\n",
    "                depth=transformer_depth,\n",
    "                attn_flash=True,\n",
    "                rotary_pos_emb=True,\n",
    "                attn_gate_values=True,\n",
    "                attn_head_scale=True,\n",
    "                ff_post_act_ln=True,\n",
    "                attn_qk_norm=True,\n",
    "                attn_qk_norm_dim_scale=True,\n",
    "                layer_dropout=drop_pat_dropout,  # stochastic depth - dropout entire layer\n",
    "                attn_dropout=dropout,  # dropout post-attention\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "\n",
    "        bpp = x0[\"bb_matrix_full_prob\"][:, :Lmax, :Lmax]\n",
    "        bpp_extra = x0[\"bb_matrix_full_prob_extra\"][:, :Lmax, :Lmax].float()\n",
    "        ss = x0[\"ss_adj\"][:, :Lmax, :Lmax].float()\n",
    "\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        res = x\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, bpp, bpp_extra, ss, mask)\n",
    "        x = self.enc(x + res, mask=mask)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class ConvolutionConcatBlockV4(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch=256,\n",
    "        ks=7,\n",
    "        resize_factor=4,\n",
    "        filter_per_group=1,\n",
    "        activation=nn.GELU,\n",
    "        out_ch=None,\n",
    "        dropout=0.2,\n",
    "        head_size=32,\n",
    "        dropout_trasnfomer=0.2,\n",
    "        drop_path=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.effblock = EffBlock(\n",
    "            in_ch=in_ch,\n",
    "            out_ch=out_ch,\n",
    "            ks=ks,\n",
    "            resize_factor=4,\n",
    "            activation=activation,\n",
    "            filter_per_group=filter_per_group,\n",
    "            se_type=\"simple\",\n",
    "            inner_dim_calculation=\"out\",\n",
    "        )\n",
    "        self.t_block = CombinationTransformerEncoderV29(\n",
    "            in_ch,\n",
    "            head_size=head_size,\n",
    "            dropout=dropout_trasnfomer,\n",
    "            drop_path=drop_path,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, bpp, mask):\n",
    "        res = x\n",
    "        x = self.t_block(x, bpp, mask)\n",
    "        x = self.effblock(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        return x + res\n",
    "\n",
    "\n",
    "class RnaModelConvV4(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=192,\n",
    "        resize_factor=4,\n",
    "        ks=7,\n",
    "        activation=nn.SiLU,\n",
    "        head_size=32,\n",
    "        drop_pat_dropout=0.2,\n",
    "        dropout=0.2,\n",
    "        transformer_depth=12,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        block_sizes = [dim, dim, dim, dim, dim, dim]\n",
    "        self.extractor = Extractor(dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for ind, (prev_sz, sz) in enumerate(zip(block_sizes[:-1], block_sizes[1:])):\n",
    "            block = ConvolutionConcatBlockV4(\n",
    "                in_ch=prev_sz,\n",
    "                out_ch=sz,\n",
    "                ks=ks,\n",
    "                resize_factor=resize_factor,\n",
    "                activation=activation,\n",
    "                dropout=dropout,\n",
    "                head_size=32,\n",
    "                dropout_trasnfomer=0.2,\n",
    "                drop_path=drop_pat_dropout * (ind / (len(block_sizes) - 1)),\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "            \n",
    "        self.t_blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block_conv(\n",
    "                    dim=dim,\n",
    "                    num_heads=dim // head_size,\n",
    "                    mlp_ratio=4,\n",
    "                    drop_path=drop_pat_dropout * (i / (transformer_depth - 1)),\n",
    "                    init_values=1,\n",
    "                    drop=dropout,\n",
    "                )\n",
    "                for i in range(transformer_depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.proj_out = nn.Sequential(nn.Linear(dim, 2))\n",
    "\n",
    "\n",
    "    def forward(self, x0):\n",
    "        mask = x0[\"mask\"]\n",
    "        L0 = mask.shape[1]\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:, :Lmax]\n",
    "        x = x0[\"seq\"][:, :Lmax]\n",
    "\n",
    "\n",
    "        bpp_extra = x0[\"bb_matrix_full_prob_extra\"][:, :Lmax, :Lmax].float()\n",
    "\n",
    "\n",
    "        x = self.extractor(x, src_key_padding_mask=~mask)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, bpp_extra,  mask)\n",
    "\n",
    "        for i, blk in enumerate(self.t_blocks):\n",
    "            x = blk(x, key_padding_mask=~mask)\n",
    "\n",
    "        x = self.proj_out(x)\n",
    "        x = F.pad(x, (0, 0, 0, L0 - Lmax, 0, 0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 16\n",
    "out_chan_unet = 32\n",
    "md = RnaModelConvV4(dim=192*2).eval()\n",
    "batch = torch.load(\"batch.pt\")\n",
    "with torch.no_grad():\n",
    "    out = md(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func, flash_attn_varlen_kvpacked_func\n",
    "from flash_attn.bert_padding import unpad_input, pad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashAttention2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dim,\n",
    "        num_head,\n",
    "        softmax_scale,\n",
    "        zero_init,\n",
    "        use_bias,\n",
    "        initializer_range,\n",
    "        n_layers,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert model_dim % num_head == 0\n",
    "        assert model_dim % num_head == 0\n",
    "        self.key_dim = model_dim // num_head\n",
    "        self.value_dim = model_dim // num_head\n",
    "\n",
    "        self.causal = False\n",
    "        self.checkpointing = False\n",
    "\n",
    "        if softmax_scale:\n",
    "            self.softmax_scale = self.key_dim ** (-0.5)\n",
    "        else:\n",
    "            self.softmax_scale = None\n",
    "\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.Wqkv = nn.Linear(model_dim, 3 * model_dim, bias=use_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(model_dim, model_dim, bias=use_bias)\n",
    "\n",
    "        self.initialize(zero_init, use_bias, initializer_range, n_layers)\n",
    "\n",
    "    def initialize(self, zero_init, use_bias, initializer_range, n_layers):\n",
    "        nn.init.normal_(self.Wqkv.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "        if use_bias:\n",
    "            nn.init.constant_(self.Wqkv.bias, 0.0)\n",
    "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
    "\n",
    "        if zero_init:\n",
    "            nn.init.constant_(self.out_proj.weight, 0.0)\n",
    "        else:\n",
    "            nn.init.normal_(\n",
    "                self.out_proj.weight,\n",
    "                mean=0.0,\n",
    "                std=initializer_range / math.sqrt(2 * n_layers),\n",
    "            )\n",
    "\n",
    "    def forward(self, pair_act, attention_mask):\n",
    "        batch_size = pair_act.shape[0]\n",
    "        seqlen = pair_act.shape[1]\n",
    "        extended_batch_size = batch_size * seqlen\n",
    "\n",
    "        qkv = self.Wqkv(pair_act)\n",
    "        not_attention_mask = torch.logical_not(attention_mask)\n",
    "\n",
    "        x_qkv = rearrange(\n",
    "            qkv, \"b s f ... -> (b s) f ...\", b=batch_size, f=seqlen, s=seqlen\n",
    "        )\n",
    "        key_padding_mask = rearrange(\n",
    "            not_attention_mask,\n",
    "            \"b s f ... -> (b s) f ...\",\n",
    "            b=batch_size,\n",
    "            f=seqlen,\n",
    "            s=seqlen,\n",
    "        )\n",
    "\n",
    "        x_unpad, indices, cu_seqlens, max_s = unpad_input(x_qkv, key_padding_mask)\n",
    "        x_unpad = rearrange(\n",
    "            x_unpad, \"nnz (three h d) -> nnz three h d\", three=3, h=self.num_head\n",
    "        )\n",
    "\n",
    "        output_unpad = flash_attn_varlen_qkvpacked_func(\n",
    "            x_unpad,\n",
    "            cu_seqlens,\n",
    "            max_s,\n",
    "            0.0,\n",
    "            softmax_scale=self.softmax_scale,\n",
    "            causal=self.causal,\n",
    "        )\n",
    "\n",
    "        pre_pad_latent = rearrange(output_unpad, \"nnz h d -> nnz (h d)\")\n",
    "        padded_latent = pad_input(pre_pad_latent, indices, extended_batch_size, seqlen)\n",
    "        output = rearrange(padded_latent, \"b f (h d) -> b f h d\", h=self.num_head)\n",
    "\n",
    "        output = rearrange(\n",
    "            output, \"(b s) f h d -> b s f (h d)\", b=batch_size, f=seqlen, s=seqlen\n",
    "        )\n",
    "\n",
    "        return self.out_proj(output)\n",
    "\n",
    "\n",
    "class TriangleAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dim,\n",
    "        num_head,\n",
    "        orientation,\n",
    "        softmax_scale,\n",
    "        precision,\n",
    "        zero_init,\n",
    "        use_bias,\n",
    "        initializer_range,\n",
    "        n_layers,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.num_head = num_head\n",
    "\n",
    "        assert orientation in [\"per_row\", \"per_column\"]\n",
    "        self.orientation = orientation\n",
    "\n",
    "        self.input_norm = nn.LayerNorm(model_dim, eps=1e-6)\n",
    "\n",
    "        self.attn = FlashAttention2d(\n",
    "            model_dim,\n",
    "            num_head,\n",
    "            softmax_scale,\n",
    "            zero_init,\n",
    "            use_bias,\n",
    "            initializer_range,\n",
    "            n_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, pair_act, pair_mask, cycle_infer=False):\n",
    "        assert len(pair_act.shape) == 4\n",
    "\n",
    "        if self.orientation == \"per_column\":\n",
    "            pair_act = torch.swapaxes(pair_act, -2, -3)\n",
    "            if pair_mask is not None:\n",
    "                pair_mask = torch.swapaxes(pair_mask, -1, -2)\n",
    "\n",
    "        pair_act = self.input_norm(pair_act)\n",
    "\n",
    "        if self.training and not cycle_infer:\n",
    "            pair_act = checkpoint(self.attn, pair_act, pair_mask, use_reentrant=True)\n",
    "        else:\n",
    "            pair_act = self.attn(pair_act, pair_mask)\n",
    "\n",
    "        if self.orientation == \"per_column\":\n",
    "            pair_act = torch.swapaxes(pair_act, -2, -3)\n",
    "\n",
    "        return pair_act\n",
    "\n",
    "\n",
    "class ConvFeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dim,\n",
    "        ff_dim,\n",
    "        use_bias,\n",
    "        initializer_range,\n",
    "        n_layers,\n",
    "        kernel,\n",
    "        zero_init=True,\n",
    "    ):\n",
    "        super(ConvFeedForward, self).__init__()\n",
    "\n",
    "        self.zero_init = zero_init\n",
    "\n",
    "        self.input_norm = nn.GroupNorm(1, model_dim)\n",
    "\n",
    "        if kernel == 1:\n",
    "            self.conv1 = nn.Conv2d(model_dim, ff_dim, kernel_size=1, bias=use_bias)\n",
    "            self.conv2 = nn.Conv2d(ff_dim, model_dim, kernel_size=1, bias=use_bias)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                model_dim,\n",
    "                ff_dim,\n",
    "                bias=use_bias,\n",
    "                kernel_size=kernel,\n",
    "                padding=(kernel - 1) // 2,\n",
    "            )\n",
    "            self.conv2 = nn.Conv2d(\n",
    "                ff_dim,\n",
    "                model_dim,\n",
    "                bias=use_bias,\n",
    "                kernel_size=kernel,\n",
    "                padding=(kernel - 1) // 2,\n",
    "            )\n",
    "\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.initialize(zero_init, use_bias, initializer_range, n_layers)\n",
    "\n",
    "    def initialize(self, zero_init, use_bias, initializer_range, n_layers):\n",
    "        nn.init.normal_(self.conv1.weight, mean=0.0, std=initializer_range)\n",
    "\n",
    "        if use_bias:\n",
    "            nn.init.constant_(self.conv1.bias, 0.0)\n",
    "            nn.init.constant_(self.conv2.bias, 0.0)\n",
    "\n",
    "        if zero_init:\n",
    "            nn.init.constant_(self.conv2.weight, 0.0)\n",
    "        else:\n",
    "            nn.init.normal_(\n",
    "                self.conv2.weight,\n",
    "                mean=0.0,\n",
    "                std=initializer_range / math.sqrt(2 * n_layers),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.input_norm(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BlockTrinagle(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        ff_dim = int(config.ff_factor * config.model_dim)\n",
    "\n",
    "        self.attn_pair_row = TriangleAttention(\n",
    "            config.model_dim,\n",
    "            config.num_head,\n",
    "            \"per_row\",\n",
    "            config.softmax_scale,\n",
    "            config.precision,\n",
    "            config.zero_init,\n",
    "            config.use_bias,\n",
    "            config.initializer_range,\n",
    "            config.n_layers,\n",
    "        )\n",
    "        self.attn_pair_col = TriangleAttention(\n",
    "            config.model_dim,\n",
    "            config.num_head,\n",
    "            \"per_column\",\n",
    "            config.softmax_scale,\n",
    "            config.precision,\n",
    "            config.zero_init,\n",
    "            config.use_bias,\n",
    "            config.initializer_range,\n",
    "            config.n_layers,\n",
    "        )\n",
    "\n",
    "        self.pair_dropout_row = nn.Dropout(p=config.resi_dropout / 2)\n",
    "        self.pair_dropout_col = nn.Dropout(p=config.resi_dropout / 2)\n",
    "\n",
    "        self.pair_transition = ConvFeedForward(\n",
    "            config.model_dim,\n",
    "            ff_dim,\n",
    "            use_bias=config.use_bias,\n",
    "            kernel=config.ff_kernel,\n",
    "            initializer_range=config.initializer_range,\n",
    "            zero_init=config.zero_init,\n",
    "            n_layers=config.n_layers,\n",
    "        )\n",
    "\n",
    "        self.res_dropout = nn.Dropout(p=config.resi_dropout)\n",
    "\n",
    "    def forward(self, pair_act, pair_mask, cycle_infer=False):\n",
    "        pair_act = pair_act + self.pair_dropout_row(\n",
    "            self.attn_pair_row(pair_act, pair_mask, cycle_infer)\n",
    "        )\n",
    "        pair_act = pair_act + self.pair_dropout_col(\n",
    "            self.attn_pair_col(pair_act, pair_mask, cycle_infer)\n",
    "        )\n",
    "        pair_act = pair_act + self.res_dropout(self.pair_transition(pair_act))\n",
    "\n",
    "        return pair_act\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 170, 170, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class config:\n",
    "  cycling = False\n",
    "  ff_factor = 4\n",
    "  ff_kernel = 3\n",
    "  initializer_range = 0.02\n",
    "  model_dim = 32\n",
    "  n_layers= 1\n",
    "  num_head= 2\n",
    "  resi_dropout = 0.1\n",
    "  softmax_scale = True\n",
    "  use_bias =  True\n",
    "  use_glu =  False\n",
    "  zero_init = False\n",
    "  precision = 16\n",
    "  \n",
    "def make_pair_mask(src, src_len):\n",
    "    encode_mask = torch.arange(src.shape[1], device=src.device).expand(\n",
    "        src.shape[:2]\n",
    "    ) < src_len.unsqueeze(1)\n",
    "    pair_mask = encode_mask[:, None, :] * encode_mask[:, :, None]\n",
    "    assert isinstance(pair_mask, torch.BoolTensor) or isinstance(\n",
    "        pair_mask, torch.cuda.BoolTensor\n",
    "    )\n",
    "    return torch.bitwise_not(pair_mask)\n",
    "\n",
    "\n",
    "batch = torch.load(\"batch.pt\")\n",
    "mask = batch[\"mask\"]\n",
    "L0 = mask.shape[1]\n",
    "Lmax = mask.sum(-1).max()\n",
    "mask = mask[:, :Lmax]\n",
    "x = batch[\"seq\"][:, :Lmax]\n",
    "embed = nn.Embedding(4, 32).eval()\n",
    "with torch.no_grad():\n",
    "    embeddings = embed(x)\n",
    "\n",
    "model = BlockTrinagle(config).eval()\n",
    "pair_latent = embeddings.unsqueeze(1) + embeddings.unsqueeze(2)\n",
    "pair_mask = make_pair_mask(mask, mask.sum(-1))\n",
    "pair_latent.masked_fill_(pair_mask[:, :, :, None], 0.0)\n",
    "pair_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  with torch.no_grad(),torch.cuda.amp.autocast():\n",
    "#     latent = model(pair_act=pair_latent, pair_mask=pair_mask, cycle_infer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 170, 170])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCuGraphSAGEConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0min_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maggr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mroot_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mproject\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "The GraphSAGE operator from the `\"Inductive Representation Learning on\n",
      "Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper.\n",
      "\n",
      ":class:`CuGraphSAGEConv` is an optimized version of\n",
      ":class:`~torch_geometric.nn.conv.SAGEConv` based on the :obj:`cugraph-ops`\n",
      "package that fuses message passing computation for accelerated execution\n",
      "and lower memory footprint.\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/cugraph/sage_conv.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "torch_geometric.nn.conv.CuGraphSAGEConv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
