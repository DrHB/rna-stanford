{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch\n",
    "import seaborn as sbn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def good_luck():\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export \n",
    "class LenMatchBatchSampler(torch.utils.data.BatchSampler):\n",
    "    def __iter__(self):\n",
    "        buckets = [[]] * 100\n",
    "        yielded = 0\n",
    "\n",
    "        for idx in self.sampler:\n",
    "            s = self.sampler.data_source[idx]\n",
    "            if isinstance(s,tuple): L = s[0][\"mask\"].sum()\n",
    "            else: L = s[\"mask\"].sum()\n",
    "            L = max(1,L // 16) \n",
    "            if len(buckets[L]) == 0:  buckets[L] = []\n",
    "            buckets[L].append(idx)\n",
    "            \n",
    "            if len(buckets[L]) == self.batch_size:\n",
    "                batch = list(buckets[L])\n",
    "                yield batch\n",
    "                yielded += 1\n",
    "                buckets[L] = []\n",
    "                \n",
    "        batch = []\n",
    "        leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "        for idx in leftover:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yielded += 1\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yielded += 1\n",
    "            yield batch\n",
    "            \n",
    "def dict_to(x, device='cuda'):\n",
    "    return {k:x[k].to(device) for k in x}\n",
    "\n",
    "def to_device(x, device='cuda'):\n",
    "    return tuple(dict_to(e,device) for e in x)\n",
    "\n",
    "class DeviceDataLoader:\n",
    "    def __init__(self, dataloader, device='cuda'):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            yield tuple(dict_to(x, self.device) for x in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# |export \n",
    "def encode_rna_sequence(seq):\n",
    "    L = len(seq)\n",
    "\n",
    "    # Initialize the tensor with zeros\n",
    "    tensor = np.zeros((L, L, 8))\n",
    "\n",
    "    # Define valid base pairs\n",
    "    valid_pairs = [\n",
    "        (\"A\", \"U\"),\n",
    "        (\"U\", \"A\"),\n",
    "        (\"U\", \"G\"),\n",
    "        (\"G\", \"U\"),\n",
    "        (\"G\", \"C\"),\n",
    "        (\"C\", \"G\"),\n",
    "    ]\n",
    "\n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            # Check for valid base pairs\n",
    "            if (seq[i], seq[j]) in valid_pairs:\n",
    "                channel = valid_pairs.index((seq[i], seq[j]))\n",
    "                tensor[i, j, channel] = 1\n",
    "            # Check for diagonal\n",
    "            elif i == j:\n",
    "                tensor[i, j, 6] = 1\n",
    "            # If not a valid pair and not on the diagonal, set the last channel\n",
    "            else:\n",
    "                tensor[i, j, 7] = 1\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def generate_adj_matrix(file_path, max_len, prob=0.5):\n",
    "    \"\"\"\n",
    "    Reads a TXT file of base pair probabilities and generates an n x n matrix.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the TXT file.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: An n x n matrix of base pair probabilities.\n",
    "    \"\"\"\n",
    "    # Read the data using pandas\n",
    "    data = pd.read_csv(file_path, sep=\" \", header=None, names=[\"pos1\", \"pos2\", \"prob\"])\n",
    "    data = data.query('prob>@prob')\n",
    "\n",
    "    # Initialize the matrix\n",
    "    matrix = np.zeros((max_len, max_len))\n",
    "\n",
    "    # Populate the matrix\n",
    "    for _, row in data.iterrows():\n",
    "        pos1, pos2, prob = int(row[\"pos1\"]), int(row[\"pos2\"]), row[\"prob\"]\n",
    "        matrix[pos1 - 1, pos2 - 1] = prob\n",
    "        matrix[pos2 - 1, pos1 - 1] = prob\n",
    "\n",
    "    return torch.tensor(matrix, dtype=torch.int)\n",
    "\n",
    "def generate_edge_data(file_path):\n",
    "    # Read the file into a DataFrame\n",
    "    data = pd.read_csv(file_path, sep=\" \", header=None, names=[\"pos1\", \"pos2\", \"prob\"])\n",
    "    \n",
    "    # Convert the pos1 and pos2 columns to 0-based indices and then to a tensor for edge index\n",
    "    edge_index = torch.tensor([data[\"pos1\"].values - 1, data[\"pos2\"].values - 1], dtype=torch.long)\n",
    "    \n",
    "    # Convert the prob column to a tensor for edge features\n",
    "    edge_features = torch.tensor(data[\"prob\"].values, dtype=torch.float).unsqueeze(1)  # Adding an extra dimension\n",
    "    \n",
    "    return edge_index, edge_features\n",
    "\n",
    "\n",
    "\n",
    "class RNA_DatasetV0(Dataset):\n",
    "    def __init__(self, df, mask_only=False,prob_for_adj = 0.5, **kwargs):\n",
    "\n",
    "        self.seq_map = {'A':0,'C':1,'G':2,'U':3}\n",
    "        self.Lmax = 206\n",
    "        self.prob_for_adj = prob_for_adj\n",
    "        df['L'] = df.sequence.apply(len)\n",
    "        df_2A3 = df.loc[df.experiment_type=='2A3_MaP'].reset_index(drop=True)\n",
    "        df_DMS = df.loc[df.experiment_type=='DMS_MaP'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "        self.seq = df_2A3['sequence'].values\n",
    "        self.L = df_2A3['L'].values\n",
    "        \n",
    "        self.react_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        self.react_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        self.react_err_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "                                 'reactivity_error_0' in c]].values\n",
    "        self.react_err_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "                                'reactivity_error_0' in c]].values\n",
    "        self.sn_2A3 = df_2A3['signal_to_noise'].values\n",
    "        self.sn_DMS = df_DMS['signal_to_noise'].values\n",
    "        self.bpp = df_2A3['bpp'].values\n",
    "        self.mask_only = mask_only\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.seq)  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seq[idx]\n",
    "        if self.mask_only:\n",
    "            mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "            mask[:len(seq)] = True\n",
    "            return {'mask':mask},{'mask':mask}\n",
    "        adj_matrix = generate_adj_matrix(self.bpp[idx], self.Lmax, self.prob_for_adj)\n",
    "        seq = [self.seq_map[s] for s in seq]\n",
    "        seq = np.array(seq)\n",
    "        mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "        mask[:len(seq)] = True\n",
    "        seq = np.pad(seq,(0,self.Lmax-len(seq)))\n",
    "        \n",
    "        react = torch.from_numpy(np.stack([self.react_2A3[idx],\n",
    "                                           self.react_DMS[idx]],-1))\n",
    "        react_err = torch.from_numpy(np.stack([self.react_err_2A3[idx],\n",
    "                                               self.react_err_DMS[idx]],-1))\n",
    "        return {\"seq\": torch.from_numpy(seq), \"mask\": mask, \"adj_matrix\": adj_matrix}, {\n",
    "            \"react\": react,\n",
    "            \"react_err\": react_err,\n",
    "            \"mask\": mask,\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class RNA_DatasetV0G(Dataset):\n",
    "    def __init__(self, df, path_to_bpp_folder, mask_only=False, **kwargs):\n",
    "\n",
    "        self.seq_map = {'A':0,'C':1,'G':2,'U':3}\n",
    "        self.Lmax = 206\n",
    "        df['L'] = df.sequence.apply(len)\n",
    "        df_2A3 = df.loc[df.experiment_type=='2A3_MaP'].reset_index(drop=True)\n",
    "        df_DMS = df.loc[df.experiment_type=='DMS_MaP'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "        self.seq = df_2A3['sequence'].values\n",
    "        self.L = df_2A3['L'].values\n",
    "        \n",
    "        self.react_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        self.react_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        self.react_err_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "                                 'reactivity_error_0' in c]].values\n",
    "        self.react_err_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "                                'reactivity_error_0' in c]].values\n",
    "        self.sn_2A3 = df_2A3['signal_to_noise'].values\n",
    "        self.sn_DMS = df_DMS['signal_to_noise'].values\n",
    "        self.bpp = df_2A3['bpp'].values\n",
    "        self.mask_only = mask_only\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.seq)  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seq[idx]\n",
    "        if self.mask_only:\n",
    "            mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "            mask[:len(seq)] = True\n",
    "            return {'mask':mask},{'mask':mask}\n",
    "        edge_index, edge_features = generate_edge_data(self.bpp[idx])\n",
    "        seq = [self.seq_map[s] for s in seq]\n",
    "        seq = np.array(seq)\n",
    "\n",
    "        \n",
    "        react = torch.from_numpy(np.stack([self.react_2A3[idx],\n",
    "                                           self.react_DMS[idx]],-1))\n",
    "        react_err = torch.from_numpy(np.stack([self.react_err_2A3[idx],\n",
    "                                               self.react_err_DMS[idx]],-1))\n",
    "        return Data(x=torch.from_numpy(seq), edge_index=edge_index, edge_features= edge_features, y=react, y_err=react_err)\n",
    "    \n",
    "    \n",
    "\n",
    "class LenMatchBatchSampler(torch.utils.data.BatchSampler):\n",
    "    def __iter__(self):\n",
    "        buckets = [[]] * 100\n",
    "        yielded = 0\n",
    "\n",
    "        for idx in self.sampler:\n",
    "            s = self.sampler.data_source[idx]\n",
    "            if isinstance(s, tuple):\n",
    "                L = s[0][\"mask\"].sum()\n",
    "            else:\n",
    "                L = s[\"mask\"].sum()\n",
    "            L = max(1, L // 16)\n",
    "            if len(buckets[L]) == 0:\n",
    "                buckets[L] = []\n",
    "            buckets[L].append(idx)\n",
    "\n",
    "            if len(buckets[L]) == self.batch_size:\n",
    "                batch = list(buckets[L])\n",
    "                yield batch\n",
    "                yielded += 1\n",
    "                buckets[L] = []\n",
    "\n",
    "        batch = []\n",
    "        leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "        for idx in leftover:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yielded += 1\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yielded += 1\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    path = Path(\"../data/split\")\n",
    "    pathbb = Path(\"../data/Ribonanza_bpp_files\")\n",
    "    split_id = 'v0'\n",
    "    bs = 16\n",
    "    num_workers = 8\n",
    "    device = 'cpu'\n",
    "    adjnact_prob = 0.5\n",
    "\n",
    "\n",
    "\n",
    "fns = list(CFG.pathbb.rglob(\"*.txt\"))\n",
    "bpp_df = pd.DataFrame({\"bpp\": fns})\n",
    "bpp_df['sequence_id'] = bpp_df['bpp'].apply(lambda x: x.stem)\n",
    "df_train = pd.read_parquet(CFG.path / f\"train_data_{CFG.split_id}.parquet\")\n",
    "df_valid = pd.read_parquet(CFG.path / f\"valid_data_{CFG.split_id}.parquet\")\n",
    "df_train = df_train.merge(bpp_df, on=\"sequence_id\", how=\"left\").reset_index(drop=True)\n",
    "df_valid = df_valid.merge(bpp_df, on=\"sequence_id\", how=\"left\").reset_index(drop=True)\n",
    "ds_train = RNA_DatasetV0(df_train, path_to_bpp_folder=CFG.pathbb)\n",
    "ds_train_len = RNA_DatasetV0(df_train, path_to_bpp_folder=CFG.pathbb, mask_only=True)\n",
    "sampler_train = torch.utils.data.RandomSampler(ds_train_len)\n",
    "len_sampler_train = LenMatchBatchSampler(sampler_train, batch_size=CFG.bs,\n",
    "            drop_last=True)\n",
    "dl_train = DeviceDataLoader(torch.utils.data.DataLoader(ds_train, \n",
    "            batch_sampler=len_sampler_train, num_workers=CFG.num_workers,\n",
    "            persistent_workers=True), CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
