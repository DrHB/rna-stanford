{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_rnafm(filename, seq_len, L_max):\n",
    "    \"\"\"\n",
    "    Load data from a .npy file and convert it to an N x N matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: Path to the .npy file.\n",
    "    - N: Dimension of the square matrix.\n",
    "\n",
    "    Returns:\n",
    "    - bpp_matrix: N x N matrix reconstructed from the input file.\n",
    "    \"\"\"\n",
    "    # Load the structured array from the .npy file\n",
    "    data = np.load(filename)\n",
    "\n",
    "    # Create an empty N x N matrix\n",
    "    bpp_matrix = np.zeros((seq_len, seq_len))\n",
    "\n",
    "    # Fill the matrix with the probabilities from the loaded data\n",
    "    bpp_matrix[data[\"pos_1\"], data[\"pos_2\"]] = data[\"probabilities\"]\n",
    "\n",
    "    bpp_matrix = bpp_matrix + bpp_matrix.T - np.diag(np.diag(bpp_matrix))\n",
    "    full = np.zeros((L_max, L_max))\n",
    "    full[:seq_len, :seq_len] = bpp_matrix\n",
    "    return torch.tensor(full)\n",
    "\n",
    "\n",
    "def generate_base_pair_matrixv1(file_path, L):\n",
    "    \"\"\"\n",
    "    Reads a TXT file of base pair probabilities and generates an n x n matrix.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the TXT file.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: An n x n matrix of base pair probabilities.\n",
    "    \"\"\"\n",
    "    # Read the data using pandas\n",
    "    data = pd.read_csv(file_path, sep=\" \", header=None, names=[\"pos1\", \"pos2\", \"prob\"])\n",
    "\n",
    "    # Find the largest position in the 'pos1' column\n",
    "    largest_position = data[\"pos1\"].max()\n",
    "\n",
    "    ids = torch.from_numpy(data[[\"pos1\", \"pos2\"]].values.astype(int))\n",
    "    matrix = torch.zeros((L, L))\n",
    "    matrix[ids[:, 0] - 1, ids[:, 1] - 1] = torch.from_numpy(data[\"prob\"].values).float()\n",
    "    matrix[ids[:, 1] - 1, ids[:, 0] - 1] = torch.from_numpy(data[\"prob\"].values).float()\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def extra_bpp_from_numpy(filename, N, seq_len=None):\n",
    "    \"\"\"\n",
    "    Load data from a .npy file and convert it to an N x N matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: Path to the .npy file.\n",
    "    - N: Dimension of the square matrix.\n",
    "\n",
    "    Returns:\n",
    "    - bpp_matrix: N x N matrix reconstructed from the input file.\n",
    "    \"\"\"\n",
    "    # Load the structured array from the .npy file\n",
    "    if filename.parent.stem in [\"rnafm\", \"rnaformerv1\"]:\n",
    "        full = load_rnafm(filename, seq_len, N)\n",
    "    else:\n",
    "        data = np.load(filename)\n",
    "        # Create an empty N x N matrix\n",
    "        bpp_matrix = np.zeros((N, N))\n",
    "        # Fill the matrix with the probabilities from the loaded data\n",
    "        bpp_matrix[data[\"pos_1\"], data[\"pos_2\"]] = data[\"probabilities\"]\n",
    "        full = torch.tensor(bpp_matrix)\n",
    "\n",
    "    return full\n",
    "\n",
    "def dot_to_adjacencyv0(dot_notation, n):\n",
    "    adjacency_matrix = np.zeros((n, n), dtype=int)\n",
    "    stack = []\n",
    "    for i, char in enumerate(dot_notation):\n",
    "        if char == \"(\":\n",
    "            stack.append(i)\n",
    "        elif char == \")\":\n",
    "            j = stack.pop()\n",
    "            adjacency_matrix[i][j] = adjacency_matrix[j][i] = 1\n",
    "\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    path = Path(\"../data/\")\n",
    "    pathbb = Path(\"../data/Ribonanza_bpp_files\")\n",
    "    pathss = Path(\"../eda/train_ss_vienna_rna.parquet\")\n",
    "    split_id = Path('../eda/fold_split.csv')\n",
    "    \n",
    "fns = list(CFG.pathbb.rglob(\"*.txt\"))\n",
    "bpp_df = pd.DataFrame({\"bpp\": fns})\n",
    "bpp_df['sequence_id'] = bpp_df['bpp'].apply(lambda x: x.stem)\n",
    "bpp_df.drop_duplicates(subset = 'sequence_id', inplace=True)\n",
    "df = pd.concat([pd.read_parquet('../eda/train_ss_vienna_rna.parquet'),\n",
    "                pd.read_parquet('../eda/test_ss_vienna_rna.parquet')])\n",
    "df = df.drop_duplicates(subset=['sequence_id']).reset_index(drop=True)\n",
    "df = pd.merge(df, bpp_df, on='sequence_id')\n",
    "df['L'] = df['sequence'].apply(len)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUT = Path('bpp/comb')\n",
    "# os.makedirs(OUT, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert(row):\n",
    "#     L = row.L\n",
    "#     bpp_fn = row.bpp  # You might need to convert this to Path object if it's not already.\n",
    "#     ss_full = row.ss_full\n",
    "\n",
    "#     extra_bpp = [\"vienna_2\", \"contrafold_2\", \"rnaformerv1\", \"rnaformer\"]\n",
    "#     extra_bpp_path = Path(\"bpp\")\n",
    "#     names = ['vienna_2', 'contrafold_2', 'rnaformerv1', \"rnaformer\", 'bpp_org', 'ss_vienna']\n",
    "\n",
    "#     bpp_extra = [\n",
    "#         extra_bpp_from_numpy(extra_bpp_path / f\"{i}/{bpp_fn.stem}.npy\", L, seq_len=L).numpy()\n",
    "#         for i in extra_bpp\n",
    "#     ] + [generate_base_pair_matrixv1(bpp_fn, L).numpy().astype(np.float16)] + [dot_to_adjacencyv0(ss_full, L)]\n",
    "\n",
    "#     bpp_extra_d = {s : np.array(d).astype(np.float16) for s, d in zip(names, bpp_extra)}\n",
    "\n",
    "#     # Assuming OUT is a predefined path\n",
    "#     OUT = Path('bpp/comb')  # replace with your actual output path\n",
    "#     np.savez_compressed(OUT / f\"{row.sequence_id}.npz\", **bpp_extra_d)\n",
    "\n",
    "# # This function will be used by Pool.map which expects a single-argument function\n",
    "# def worker(index):\n",
    "#     row = df.iloc[index]\n",
    "#     convert(row)\n",
    "    \n",
    "# with Pool(processes=cpu_count()) as pool:\n",
    "#     max_ = df.shape[0]\n",
    "#     with tqdm(total=max_) as pbar:\n",
    "#         for _ in pool.imap_unordered(worker, range(max_)):\n",
    "#             pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_dict(row):\n",
    "    L = row.L\n",
    "    bpp_fn = row.bpp  # You might need to convert this to Path object if it's not already.\n",
    "    ss_full = row.ss_full\n",
    "\n",
    "    extra_bpp = [\"vienna_2\", \"contrafold_2\", \"rnaformerv1\", \"rnaformer\"]\n",
    "    extra_bpp_path = Path(\"bpp\")\n",
    "    names = ['vienna_2', 'contrafold_2', 'rnaformerv1', \"rnaformer\", 'bpp_org', 'ss_vienna']\n",
    "\n",
    "    bpp_extra = [\n",
    "        extra_bpp_from_numpy(extra_bpp_path / f\"{i}/{bpp_fn.stem}.npy\", L, seq_len=L).numpy()\n",
    "        for i in extra_bpp\n",
    "    ] + [generate_base_pair_matrixv1(bpp_fn, L).numpy().astype(np.float16)] + [dot_to_adjacencyv0(ss_full, L)]\n",
    "\n",
    "    bpp_extra_d = {s : np.array(d).astype(np.float16) for s, d in zip(names, bpp_extra)}\n",
    "    return bpp_extra_d\n",
    "\n",
    "def get_saved(row):\n",
    "    L = row.L\n",
    "    bpp_fn = row.bpp  # You might need to convert this to Path object if it's not already.\n",
    "    ss_full = row.ss_full\n",
    "\n",
    "    extra_bpp = [\"vienna_2\", \"contrafold_2\", \"rnaformerv1\", \"rnaformer\"]\n",
    "    OUT = Path('bpp/comb')  # replace with your actual output path\n",
    "    data = np.load(OUT / f\"{row.sequence_id}.npz\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 23\n",
    "row = df.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_saved(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
