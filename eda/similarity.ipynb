{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import torch \n",
    "import random\n",
    "import os\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(2023)\n",
    "\n",
    "\n",
    "def parquet_to_fasta(parquet_file_path, fasta_file_path):\n",
    "    # Read the parquet file using Polars\n",
    "    df = pl.read_parquet(parquet_file_path)\n",
    "    \n",
    "    # Check if the required columns are present in the DataFrame\n",
    "\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    df_pandas = df.to_pandas()\n",
    "    \n",
    "    # Open a file to write the FASTA format\n",
    "    with open(fasta_file_path, 'w') as fasta_file:\n",
    "        # Iterate through each row in the Pandas DataFrame\n",
    "        for _, row in tqdm(df_pandas.iterrows()):\n",
    "            sequence_id = row['sequence_id']\n",
    "            sequence = row['sequence']\n",
    "            sequence = sequence[26:-21]\n",
    "            fasta_file.write(f\">{sequence_id}\\n{sequence}\\n\")\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "#code to convert output to pandas dataframe\n",
    "# Initialize an empty list to collect rows\n",
    "def read_clstr(fn = \"similar_to_test.clstr\"):\n",
    "    rows = []\n",
    "\n",
    "    # Read the cluster file\n",
    "    with open(fn, \"r\") as f:\n",
    "        current_cluster = None\n",
    "        for line in f:\n",
    "            if line.startswith(\">Cluster\"):\n",
    "                current_cluster = int(line.split()[-1])\n",
    "            else:\n",
    "                # Parse the sequence line\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                index = int(parts[0])\n",
    "                length = int(parts[1].split(\"nt,\")[0])\n",
    "                seq_id = parts[1].split(\">\")[-1].split(\"...\")[0]\n",
    "                is_rep = \"*\" in line\n",
    "                identity = float(parts[-1].split(\"/\")[-1].rstrip(\"%\")) if \"at\" in line else 100.0\n",
    "\n",
    "                # Append this as a row\n",
    "                rows.append({\n",
    "                    \"Cluster\": current_cluster,\n",
    "                    \"Index\": index,\n",
    "                    \"Length\": length,\n",
    "                    \"Sequence_ID\": seq_id,\n",
    "                    \"Is_Representative\": is_rep,\n",
    "                    \"Identity_To_Rep\": identity\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "\n",
    "def read_fasta_ids(filename):\n",
    "    seq_ids = set()\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                # Extract the sequence ID from the line; adjust the parsing based on your specific ID format\n",
    "                seq_id = line.split()[0].lstrip('>')\n",
    "                seq_ids.add(seq_id)\n",
    "    return seq_ids\n",
    "\n",
    "def filter_clusters(group):\n",
    "    return ('Train' in group['Source'].values) and ('Test' in group['Source'].values) and len(group) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a file in `fasta` format. I'll use `train_ss` for this, as it already contains unique `sequence_id` and `sequence` columns. If you prefer to use the original file, make sure to adjust the `code` to eliminate duplicate, I also removed adapter sequnces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parquet_file_path = \"train_ss_vienna_rna.parquet\"\n",
    "fasta_file_path = \"train_seq.fasta\"\n",
    "parquet_to_fasta(parquet_file_path, fasta_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parquet_file_path = \"test_ss_vienna_rna.parquet\"\n",
    "fasta_file_path = \"test_seq.fasta\"\n",
    "parquet_to_fasta(parquet_file_path, fasta_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `train_seq.fasta` and `test_seq.fasta` files. Let's run the clustering command next. The output will include two files named `similar_to_test`, but we are mainly interested in the `.clstr` file. This file clusters the `train` and `test` sequences that are, in our case, `85%` similar. Some clusters may be empty, while others will contain `sequence_ids`. Each cluster will also have a representative sequence. Additionally, there will be a column called `Identity_To_Re`p, which indicates how identical the sequences within the cluster are to the `representative` sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-M memoery -T threads\n",
    "#sudo cd-hit-est-2d -i test_seq.fasta -i2 train_seq.fasta -c 0.88 -o similar_to_test -T 32 -M 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "df = read_clstr()\n",
    "train_ids = read_fasta_ids('train_seq.fasta')\n",
    "test_ids = read_fasta_ids('test_seq.fasta')\n",
    "#lets add column source which will iclude if sequnce belong to train or to test\n",
    "df['Source'] = df['Sequence_ID'].apply(lambda x: 'Train' if x in train_ids else ('Test' if x in test_ids else 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lets eliminate all single clusters .. and only keep clusters that has atelast one train and test sequences in them \n",
    "filtered_df = df.groupby('Cluster').filter(filter_clusters)\n",
    "#lest count how many sequnces are from train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df[\"Source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLIT GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets try to develop validatiaon dataset, we gonna cluster train with cut off 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!sudo cd-hit-est -i train_seq.fasta -c 0.80 -o clustered_train -T 44 -M 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = read_clstr('clustered_train.clstr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93028,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Cluster\"].value_counts()[df[\"Cluster\"].value_counts() > 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([130, 159, 123, 108,  68])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Length\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to read a fasta file into a DataFrame\n",
    "def read_fasta_to_df(fasta_file):\n",
    "    records = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        records.append({\"Sequence_ID\": record.id, \"Sequence\": str(record.seq)})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Function to write a DataFrame into a fasta file\n",
    "def write_to_fasta(df, fasta_file):\n",
    "    with open(fasta_file, 'w') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            f.write(f\">{row['Sequence_ID']}\\n\")\n",
    "            f.write(f\"{row['Sequence']}\\n\")\n",
    "\n",
    "            \n",
    "full_fasta_df = read_fasta_to_df(\"train_seq.fasta\")\n",
    "            \n",
    "# Get unique Cluster IDs and shuffle them\n",
    "unique_clusters = df['Cluster'].unique()\n",
    "np.random.shuffle(unique_clusters)\n",
    "\n",
    "# Split cluster IDs into training and validation sets (80/20)\n",
    "train_clusters = unique_clusters[:int(0.9 * len(unique_clusters))]\n",
    "valid_clusters = unique_clusters[int(0.9 * len(unique_clusters)):]\n",
    "\n",
    "# Get the corresponding rows for training and validation sets\n",
    "train_df = df[df['Cluster'].isin(train_clusters)]\n",
    "valid_df = df[df['Cluster'].isin(valid_clusters)]\n",
    "train_df_merged = pd.merge(train_df, full_fasta_df, on=\"Sequence_ID\", how=\"left\")\n",
    "valid_df_merged = pd.merge(valid_df, full_fasta_df, on=\"Sequence_ID\", how=\"left\")\n",
    "write_to_fasta(train_df_merged, \"train_split_fold_0.fasta\")\n",
    "write_to_fasta(valid_df_merged, \"valid_split_fold_0.fasta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((728826, 7), (77747, 7))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_merged.shape, valid_df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([130, 159, 123, 108,  68]), array([123, 130, 159, 108,  68]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df_merged[\"Length\"].unique(), train_df_merged[\"Length\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets run train and vlid sequnce comparison to find if we have idividual ids that are closer to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sudo cd-hit-est-2d -i train_split_fold_0.fasta -i2 valid_split_fold_0.fasta -c 0.85 -o similar_sequences_split -T 32 -M 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_split = read_clstr('similar_sequences_split.clstr')\n",
    "train_ids = read_fasta_ids('train_split_fold_0.fasta')\n",
    "valid_ids = read_fasta_ids('valid_split_fold_0.fasta')\n",
    "#lets add column source which will iclude if sequnce belong to train or to test\n",
    "df_split['Source'] = df_split['Sequence_ID'].apply(lambda x: 'Train' if x in train_ids else ('Test' if x in valid_ids else 'Unknown'))\n",
    "filtered_df_split = df_split.groupby('Cluster').filter(filter_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Test     18080\n",
       "Train    10595\n",
       "Name: Source, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df_split[\"Source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems like we haver some sequences that are similar .. we will take the training ids and add them to valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ids = read_fasta_ids('train_split_fold_0.fasta')\n",
    "valid_ids = read_fasta_ids('valid_split_fold_0.fasta')\n",
    "valid_ids = list(valid_ids) + filtered_df_split.query(\"Source=='Train'\")[\"Sequence_ID\"].to_list()\n",
    "train_ids = pd.DataFrame({\"sequence_id\":list(train_ids)}).set_index(\"sequence_id\")\n",
    "train_ids = train_ids.drop(filtered_df_split.query(\"Source=='Train'\")[\"Sequence_ID\"].to_list()).reset_index()\n",
    "train_ids[\"is_train\"] = True\n",
    "valid_ids = pd.DataFrame({\"sequence_id\":valid_ids})\n",
    "valid_ids[\"is_train\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.concat([train_ids, valid_ids], ignore_index=True).to_csv(\"fold_split.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = pd.read_csv(\"fold_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(806573,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split[\"sequence_id\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"train_ss_vienna_rna.parquet\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df, split, on=\"sequence_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"L\"] = df[\"sequence\"].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177    86048\n",
       "170      903\n",
       "206      838\n",
       "115      319\n",
       "155      234\n",
       "Name: L, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"is_train==False\")[\"L\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177    698124\n",
       "170     14097\n",
       "115      2410\n",
       "155      1939\n",
       "206      1661\n",
       "Name: L, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"is_train==True\")[\"L\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
